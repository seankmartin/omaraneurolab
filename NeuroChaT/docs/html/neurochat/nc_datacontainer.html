<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.6.3" />
<title>neurochat.nc_datacontainer API documentation</title>
<meta name="description" content="This module implements a container for the Ndata class to simplify multi experiment analyses â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>neurochat.nc_datacontainer</code></h1>
</header>
<section id="section-intro">
<p>This module implements a container for the Ndata class to simplify multi experiment analyses.</p>
<p>@author: Sean Martin; martins7 at tcd dot ie</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
&#34;&#34;&#34;
This module implements a container for the Ndata class to simplify multi experiment analyses.

@author: Sean Martin; martins7 at tcd dot ie
&#34;&#34;&#34;

from enum import Enum
import copy
import logging
import os
import pprint
import re

import pandas as pd
import numpy as np

from neurochat.nc_data import NData
from neurochat.nc_utils import get_all_files_in_dir, make_dir_if_not_exists
from neurochat.nc_utils import has_ext, log_exception, remove_extension


class NDataContainerIterator():
    def __init__(self, container):
        self._index = 0
        self._container = container

    def __next__(self):
        if self._index &lt; len(self._container):
            self._index += 1
            return self._container[self._index - 1]
        raise StopIteration


class NDataContainer():
    &#34;&#34;&#34;
    Class for storing multiple file locations for ndata objects.

    Additionally the ndata objects themselves can be stored.
    &#34;&#34;&#34;

    def __init__(self, share_positions=False, load_on_fly=False):
        &#34;&#34;&#34;
        Initialise the class.

        Parameters
        ----------
        share_positions : bool
            Share the same position file between the data objects
        load_on_fly : bool
            Don&#39;t store all the data in memory,
            instead load it as needed, on the fly

        Attributes
        ----------
        _container : List
        _file_names_dict : Dict
        _units : List
        _unit_count : List
        _share_positions : bool
        _load_on_fly : bool
        _smoothed_speed : bool
        _last_data_pt : tuple (int, NData)

        &#34;&#34;&#34;
        self._file_names_dict = {}
        self._units = []
        self._container = []
        self._unit_count = []
        self._share_positions = share_positions
        self._load_on_fly = load_on_fly
        self._last_data_pt = (1, None)
        self._smoothed_speed = False

    class EFileType(Enum):
        &#34;&#34;&#34;The different filetypes that can be added to an object.&#34;&#34;&#34;

        Spike = 1
        Position = 2
        LFP = 3

    def get_num_data(self):
        &#34;&#34;&#34;Return the number of Ndata objects in the container.&#34;&#34;&#34;
        if self._load_on_fly:
            for _, vals in self.get_file_dict().items():
                return len(vals)
        return len(self._container)

    def get_file_dict(self, key=None):
        &#34;&#34;&#34;Return the key value filename dictionary for this collection.&#34;&#34;&#34;
        if key:
            return self._file_names_dict.get(key, None)
        return self._file_names_dict

    def get_units(self, index=None):
        &#34;&#34;&#34;
        Return the units in this collection, optionally at a given index.

        Parameters
        ----------
        index : int
            Optional collection data index to get the units for

        Returns
        -------
        list
            Either a list containing lists of all units in the collection
            or the list of units for the given data index

        &#34;&#34;&#34;
        if index is None:
            return self._units
        if index &gt;= self.get_num_data() and (not self._load_on_fly):
            logging.error(&#34;Input index to get_data out of range&#34;)
            return
        return self._units[index]

    def get_data(self, index=None):
        &#34;&#34;&#34;
        Return the NData objects in this collection, or a specific object.

        Do not call this with no index if loading data on the fly

        Parameters
        ----------
        index : int
            Optional index to get data at

        Returns
        -------
        NData or list of NData objects

        &#34;&#34;&#34;
        if self._load_on_fly:
            if index is None:
                logging.error(&#34;Can&#39;t load all data when loading on the fly&#34;)
            result = NData()
            for key, vals in self.get_file_dict().items():
                descriptor = vals[index]
                self._load(key, descriptor, ndata=result)
            return result
        if index is None:
            return self._container
        if index &gt;= self.get_num_data():
            logging.error(&#34;Input index to get_data out of range&#34;)
            return
        return self._container[index]

    def add_data(self, data):
        &#34;&#34;&#34;Add an NData object to this container.&#34;&#34;&#34;
        if isinstance(data, NData):
            self._container.append(data)
        else:
            logging.error(&#34;Adding incorrect object to data container&#34;)
            return

    def list_all_units(self):
        &#34;&#34;&#34;Print all the units in the container.&#34;&#34;&#34;
        if self._load_on_fly:
            for key, vals in self.get_file_dict().items():
                if key == &#34;Spike&#34;:
                    for descriptor in vals:
                        result = NData()
                        self._load(key, descriptor, ndata=result)
                        print(&#34;units are {}&#34;.format(result.get_unit_list()))
        else:
            for data in self._container:
                print(&#34;units are {}&#34;.format(data.get_unit_list()))

    def add_files(self, f_type, descriptors):
        &#34;&#34;&#34;
        Add a list of filenames of the given type to the container.

        Parameters
        ----------
        f_type : EFileType:
            The type of file being added (Spike, LFP, Position)
        descriptors : list
            Either a list of filenames, or a list of tuples in the order
            (filenames, obj_names, data_sytem). Filenames should be absolute

        Returns
        -------
        None

        &#34;&#34;&#34;
        if isinstance(descriptors, list):
            descriptors = (descriptors, None, None)
        filenames, _, _ = descriptors
        if not isinstance(f_type, self.EFileType):
            logging.error(
                &#34;Parameter f_type in add files must be of EFileType\n&#34; +
                &#34;given {}&#34;.format(f_type))
            return

        if f_type.name == &#34;Position&#34; and self._share_positions and len(filenames) == 1:
            for _ in range(len(self.get_file_dict()[&#34;Spike&#34;]) - 1):
                filenames.append(filenames[0])

        # Ensure lists are empty or of equal size
        for l in descriptors:
            if l is not None:
                if len(l) != len(filenames):
                    logging.error(
                        &#34;add_files called with differing number of filenames and other data&#34;
                    )
                    return

        for idx in range(len(filenames)):
            description = []
            for el in descriptors:
                if el is not None:
                    description.append(el[idx])
                else:
                    description.append(None)
            self._file_names_dict.setdefault(
                f_type.name, []).append(description)

    def add_all_files(self, spats, spikes, lfps):
        &#34;&#34;&#34;
        Quickly add a list of positions, spikes and lfps.

        Parameters
        ----------
        spats : list
            The list of spatial files
        spikes : list
            The list of spike files
        lfps : list
            The list of lfp files

        Returns
        -------
        None

        &#34;&#34;&#34;
        self.add_files(self.EFileType.Position, spats)
        self.add_files(self.EFileType.Spike, spikes)
        self.add_files(self.EFileType.LFP, lfps)

    def set_units(self, units=&#39;all&#39;):
        &#34;&#34;&#34;Set the list of units for the collection.&#34;&#34;&#34;
        self._units = []
        if self.get_file_dict() == {}:
            print(&#34;Error: Can&#39;t set units for empty collection&#34;)
            return
        if units == &#39;all&#39;:
            if self._load_on_fly:
                vals = self.get_file_dict()[&#34;Spike&#34;]
                for descriptor in vals:
                    result = NData()
                    self._load(&#34;Spike&#34;, descriptor, ndata=result)
                    self._units.append(result.get_unit_list())
            else:
                for data in self.get_data():
                    self._units.append(data.get_unit_list())

        elif isinstance(units, list):
            for idx, unit in enumerate(units):
                if unit == &#39;all&#39;:
                    if self._load_on_fly:
                        vals = self.get_file_dict()[&#34;Spike&#34;]
                        descriptor = vals[idx]
                        result = NData()
                        self._load(&#34;Spike&#34;, descriptor, ndata=result)
                        all_units = result.get_unit_list()
                    else:
                        all_units = self.get_data(idx).get_unit_list()
                    self._units.append(all_units)
                elif isinstance(unit, int):
                    self._units.append([unit])
                elif isinstance(unit, list):
                    self._units.append(unit)
                else:
                    logging.error(
                        &#34;Unrecognised type {} passed to set units&#34;.format(type(unit)))

        else:
            logging.error(
                &#34;Unrecognised type {} passed to set units&#34;.format(type(units)))
        self._unit_count = self._count_num_units()

    def setup(self):
        &#34;&#34;&#34;Perform data initialisation based on the input filenames.&#34;&#34;&#34;
        if self._load_on_fly:
            self._last_data_pt = (1, None)
        else:
            self._load_all_data()

    def add_files_from_excel(self, file_loc, unit_sep=&#34; &#34;):
        &#34;&#34;&#34;
        Add filepaths from an excel file.

        These should be setup to be in the order:
        directory | position file | spike file | unit numbers | eeg extension

        Parameters
        ----------
        file_loc : str
            Name of the excel file that contains the data specifications
        unit_sep : str
            Optional separator character for unit numbers, default &#34; &#34;

        Returns
        -------
        excel_info :
            The raw info parsed from the excel file for further use

        &#34;&#34;&#34;
        pos_files = []
        spike_files = []
        units = []
        lfp_files = []
        to_merge = []

        if os.path.exists(file_loc):
            excel_info = pd.read_excel(file_loc, index_col=None)
            if excel_info.shape[1] % 5 != 0:
                logging.error(
                    &#34;Incorrect excel file format, it should be:\n&#34; +
                    &#34;directory | position file | spike file&#34; +
                    &#34;| unit numbers | eeg extension&#34;)
                return

            # excel_info = excel_info.iloc[:, 1:] # Can be used to remove index
            count = 0
            for full_row in excel_info.itertuples():
                split = [full_row[i:i + 5]
                         for i in range(1, len(full_row), 5)
                         if not pd.isna(full_row[i])]
                merge = True if len(split) &gt; 1 else False
                merge_list = []
                for row in split:
                    base_dir = row[0]
                    pos_name = row[1]
                    tetrode_name = row[2]

                    if pos_name[-4:] == &#39;.txt&#39;:
                        spat_file = base_dir + os.sep + pos_name
                    else:
                        spat_file = base_dir + os.sep + pos_name + &#39;.txt&#39;

                    spike_file = base_dir + os.sep + tetrode_name

                    # Load the unit numbers
                    unit_info = row[3]
                    if unit_info == &#34;all&#34;:
                        unit_list = &#34;all&#34;
                    elif isinstance(unit_info, int):
                        unit_list = unit_info
                    elif isinstance(unit_info, float):
                        unit_list = int(unit_info)
                    else:
                        unit_list = [
                            int(x) for x in unit_info.split(&#34; &#34;) if x is not &#34;&#34;]

                    # Load the lfp
                    lfp_ext = row[4]
                    if lfp_ext[0] != &#34;.&#34;:
                        lfp_ext = &#34;.&#34; + lfp_ext
                    spike_name = remove_extension(spike_file, keep_dot=False)
                    lfp_file = spike_name + lfp_ext

                    pos_files.append(spat_file)
                    spike_files.append(spike_file)
                    lfp_files.append(lfp_file)
                    units.append(unit_list)
                    merge_list.append(count)
                    count += 1
                if merge:
                    to_merge.append(merge_list)

            # Complete the file setup based on parsing from the excel file
            self.add_all_files(pos_files, spike_files, lfp_files)
            self.setup()
            self.set_units(units)

            for idx, merge_list in enumerate(to_merge):
                self.merge(merge_list)
                for j in range(idx + 1, len(to_merge)):
                    to_merge[j] = [
                        k - len(merge_list) + 1 for k in to_merge[j]]
            return excel_info
        else:
            logging.error(&#39;Excel file does not exist!&#39;)
            return None

    # Created by Sean Martin with help from Matheus Cafalchio
    def add_axona_files_from_dir(
            self, directory, recursive=False, verbose=False, **kwargs):
        &#34;&#34;&#34;
        Go through a directory, extracting files from it.

        Parameters
        ----------
        directory : str
            The directory to parse through
        recursive : bool, optional. Defaults to False.
            Whether to recurse through dirs
        verbose: bool, optional. Defaults to False.
            Whether to print the files being added.

        **kwargs: keyword arguments
            tetrode_list : list
                list of tetrodes to consider
            data_extension : str default .set
            cluster_extension : str default .cut
            pos_extension : str default .txt
            lfp_extension : str default .eeg
            re_filter : str default None 
                regex string for matching filenames
            save_result : bool default True
                should save the resulting collection to a file
            unit_cutoff : tuple of ints
                don&#39;t consider any recordings with units outside this range

        Returns
        -------
        None

        &#34;&#34;&#34;
        default_tetrode_list = [
            1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
        tetrode_list = kwargs.get(&#34;tetrode_list&#34;, default_tetrode_list)
        data_extension = kwargs.get(&#34;data_extension&#34;, &#34;.set&#34;)
        cluster_extension = kwargs.get(&#34;cluster_extension&#34;, &#34;.cut&#34;)
        clu_extension = kwargs.get(&#34;clu_extension&#34;, &#34;.clu.X&#34;)
        pos_extension = kwargs.get(&#34;pos_extension&#34;, &#34;.txt&#34;)
        lfp_extension = kwargs.get(&#34;lfp_extension&#34;, &#34;.eeg&#34;)
        re_filter = kwargs.get(&#34;re_filter&#34;, None)
        save_result = kwargs.get(&#34;save_result&#34;, True)
        unit_cutoff = kwargs.get(&#34;unit_cutoff&#34;, None)

        files = get_all_files_in_dir(
            directory, data_extension,
            recursive=recursive, verbose=verbose,
            re_filter=re_filter, return_absolute=True)
        txt_files = get_all_files_in_dir(
            directory, pos_extension,
            recursive=recursive, verbose=verbose,
            re_filter=re_filter, return_absolute=True)

        for filename in files:
            filename = filename[:-len(data_extension)]
            for tetrode in tetrode_list:
                spike_name = filename + &#39;.&#39; + str(tetrode)
                cut_name = filename + &#39;_&#39; + str(tetrode) + cluster_extension
                clu_name = filename + clu_extension[:-1] + str(tetrode)
                lfp_name = filename + lfp_extension

                if not os.path.isfile(os.path.join(directory, spike_name)):
                    continue
                # Don&#39;t consider files that have not been clustered
                if not (
                        os.path.isfile(os.path.join(directory, cut_name)) or
                        os.path.isfile(os.path.join(directory, clu_name))):
                    logging.info(
                        &#34;Skipping tetrode {} - no cluster file named {} or {}&#34;.format(tetrode, cut_name, os.path.basename(clu_name)))
                    continue

                for fname in txt_files:
                    if fname[:len(filename)] == filename:
                        pos_name = fname
                        break

                else:
                    logging.info(
                        &#34;Skipping tetrode {} - no position file for {}&#34;.format(tetrode, filename))
                    continue

                self.add_files(NDataContainer.EFileType.Spike, [spike_name])
                self.add_files(NDataContainer.EFileType.Position, [pos_name])
                self.add_files(NDataContainer.EFileType.LFP, [lfp_name])
        self.set_units()

        if unit_cutoff:
            self.remove_recordings_units(
                unit_cutoff[0], unit_cutoff[1], verbose=verbose)
        if save_result:
            friendly_re = &#34;&#34;
            if re_filter:
                friendly_re = &#34;_&#34; + \
                    &#34; &#34;.join(re.findall(&#34;[a-zA-Z]+&#34;, re_filter))
            name = (
                &#34;file_list_&#34; + os.path.basename(directory) +
                friendly_re + &#34;.txt&#34;)
            out_loc = os.path.join(directory, &#34;nc_results&#34;, name)
            make_dir_if_not_exists(out_loc)
            with open(out_loc, &#39;w&#39;) as f:
                f.write(str(self))
            print(&#34;Wrote list of files considered to {}&#34;.format(out_loc))
            return out_loc
        return None

    def merge(self, indices, force_equal_units=True):
        &#34;&#34;&#34;
        Merge the data from multiple indices together into the first index.

        ONLY FUNCTIONS FOR POSITIONS AND SPIKES CURRENTLY - DOES NOT MERGE LFP.
        Only call this after loading the data, and not while loading on the fly

        Parameters
        ----------
        indices: list
            The list of indices in the data to merge together
        force_equal_units:
            The merged indexes must have the same unit numbers available

        Returns
        -------
        The merged data point

        &#34;&#34;&#34;
        if self._load_on_fly:
            logging.error(&#34;Don&#39;t call merge when loading on the fly&#34;)
            return

        target_index = indices[0]

        data_to_merge = []
        target_data = self.get_data(target_index)
        for idx in indices[1:]:
            data = self.get_data(idx)
            data_to_merge.append(data)

        units1 = self.get_units(target_index)
        for idx, data in zip(indices[1:], data_to_merge):
            units2 = self.get_units(idx)
            if force_equal_units and (not units1 == units2):
                logging.error(
                    &#34;Can&#39;t merge files with unequal units\n&#34; +
                    &#34;Units are {} , {}&#34;.format(units1, units2))
                return

            # Merge the spikes based on times (waveforms not done yet)
            new_spike_times = (
                data.spike.get_timestamp() +
                target_data.spike.get_duration())
            new_duration = (
                target_data.spike.get_duration() +
                data.spike.get_duration())
            new_tags = data.spike.get_unit_tags()

            # Merge the spatial information based on times
            new_spat_times = (
                data.spatial._time +
                target_data.spike.get_duration())
            new_pos_x = data.spatial._pos_x
            new_pos_y = data.spatial._pos_y
            new_direction = data.spatial._direction
            new_speed = data.spatial._speed

            target_data.spike._timestamp = np.append(
                target_data.spike._timestamp, new_spike_times)
            target_data.spike._unit_Tags = np.append(
                target_data.spike._unit_Tags, new_tags)
            target_data.spike._set_duration(new_duration)

            target_data.spatial._time = np.append(
                target_data.spatial._time, new_spat_times)
            # NB this may not work properly due to different borders
            target_data.spatial._pos_x = np.append(
                target_data.spatial._pos_x, new_pos_x)
            target_data.spatial._pos_y = np.append(
                target_data.spatial._pos_y, new_pos_y)
            target_data.spatial._direction = np.append(
                target_data.spatial._direction, new_direction)
            target_data.spatial._speed = np.append(
                target_data.spatial._speed, new_speed)

        self._container[target_index] = target_data

        for idx in indices[1:]:
            self._container.pop(idx)
            self._units.pop(idx)
            indices[1:] = [a - 1 for a in indices[1:]]
        self._unit_count = self._count_num_units()

        self._container[target_index].set_unit_no(
            self.get_units(target_index)[0])
        return self.get_data(target_index)

    def subsample(self, key):
        &#34;&#34;&#34;
        Return a subsample of the original data collection.

        This subsample is not a reference, but a deep copy

        Parameters
        ----------
        key : Slice or int
            How to sample the original collection

        Returns
        -------
        NDataContainer
            The deep copied subsample

        &#34;&#34;&#34;
        result = copy.deepcopy(self)

        for k in result._file_names_dict:
            result._file_names_dict[k] = result._file_names_dict[k][key]
            if isinstance(key, int):
                result._file_names_dict[k] = [result._file_names_dict[k]]

        if len(result._units) &gt; 0:
            result._units = result._units[key]
            if isinstance(key, int):
                result._units = [result._units]

        if len(result._container) &gt; 0:
            result._container = result._container[key]
            if isinstance(key, int):
                result._container = [result._container[key]]

        result._unit_count = result._count_num_units()
        return result

    def sort_units_spatially(self, should_sort_list=None, mode=&#34;vertical&#34;):
        &#34;&#34;&#34;
        Sort the units in the collection based on the place field centroid.

        Parameters
        ----------
        should_sort_list: list
            Optional list of boolean values indicating what objects
        mode: str
            &#34;horizontal&#34; or &#34;vertical&#34;, indicating what axis to sort on.

        Returns
        -------
        None

        &#34;&#34;&#34;
        if mode == &#34;vertical&#34;:
            h = 1
        elif mode == &#34;horizontal&#34;:
            h = 0
        else:
            logging.error(
                &#34;NDataContainer: &#34;
                + &#34;Only modes horizontal and vertical are supported&#34;)

        if should_sort_list is None:
            should_sort_list = [True for _ in range(self.get_num_data())]

        for idx, bool_val in enumerate(should_sort_list):
            if bool_val:
                centroids = []
                data = self.get_data(idx)
                for unit in self.get_units()[idx]:
                    data.set_unit_no(unit)
                    place_info = data.place()
                    centroid = place_info[&#34;centroid&#34;]
                    centroids.append(centroid)
                self._units[idx] = [unit for _, unit in sorted(
                    zip(centroids, self.get_units()[idx]),
                    key=lambda pair: pair[0][h])]

    def get_index_info(self, idx, absolute=False):
        &#34;&#34;&#34;Return the Spike, LFP, Position and Unit info at idx.&#34;&#34;&#34;
        str_info = {}
        dirnames = []
        if absolute:
            idx, u_idx = self._index_to_data_pos(idx)

        for key in [&#34;Spike&#34;, &#34;LFP&#34;, &#34;Position&#34;]:
            name = self.get_file_dict(key)[idx][0]
            str_info[key] = (os.path.basename(name))
            dirnames.append(os.path.dirname(name))

        if absolute:
            str_info[&#34;Units&#34;] = (self.get_units(idx)[u_idx])
        else:
            str_info[&#34;Units&#34;] = (self.get_units(idx))

        if len(set(dirnames)) == 1:
            str_info[&#34;Root&#34;] = dirnames[0]
        else:
            print(&#34;Not all files are in the same directory {} {}&#34;.format(
                &#34;:Spike, LFP, Position: &#34;, dirnames))
            str_info[&#34;Root&#34;] = dirnames
        return str_info

    def string_repr(self, pretty=True):
        &#34;&#34;&#34;
        Return a string representation of this class.
        Parameters
        ----------
        pretty : str, Default True
            Should return a pretty version or all the info.
        &#34;&#34;&#34;
        if pretty:
            return self._pretty_string()
        else:
            return self._full_string()

    def remove_recordings_units(
            self, unit_lb=0, unit_ub=10000, verbose=False):
        start_size = self.get_num_data()
        start_total = len(self)
        for i in range(self.get_num_data() - 1, -1, -1):
            unit_count = len(self.get_units(i))
            if (unit_count &gt; unit_ub) or (unit_count &lt; unit_lb):
                for key in (&#34;Spike&#34;, &#34;LFP&#34;, &#34;Position&#34;):
                    name = self._file_names_dict[key].pop(i)
                    if (key is &#34;Spike&#34;) and verbose:
                        print(&#34;Removed {} with {} units&#34;.format(
                            os.path.basename(name[0]), unit_count))
                if self._unit_count.pop(i) != unit_count:
                    print(&#34;Error in remove recording {}&#34;.format(name))
                self._last_data_pt = (1, None)
                self._units.pop(i)
                if not self._load_on_fly:
                    self._container.pop(i)
        end_size = self.get_num_data()
        self._count_num_units()
        end_total = len(self)

        print((&#34;{} tetrodes with {} units reduced to &#34;
               + &#34;{} tetrodes with {} units&#34;).format(
            start_size, start_total, end_size, end_total))

    def get_data_at(self, data_index, unit_index):
        if self._load_on_fly:
            try:
                if data_index == self._last_data_pt[0]:
                    result = self._last_data_pt[1]
                else:
                    result = NData()
                    for key, vals in self.get_file_dict().items():
                        descriptor = vals[data_index]
                        self._load(key, descriptor,
                                   idx=data_index, ndata=result)
                    self._last_data_pt = (data_index, result)
            except Exception as e:
                log_exception(e, &#34;During loading data&#34;)
        else:
            result = self.get_data(data_index)
        if len(self.get_units()) &gt; 0:
            result.set_unit_no(self.get_units(data_index)[unit_index])
        return result

    # Methods from here on should be for private class use
    def _pretty_string(self):
        &#34;&#34;&#34;Alternative string representation should be prettier.&#34;&#34;&#34;
        all_str_info = []
        for i in range(self.get_num_data()):
            str_info = self.get_index_info(i)
            b_str = &#34;{}: \n\tSpk {}\n\tUnt {}: {}\n\tLfp {}\n\tPos {}\n\tDir {}&#34;.format(
                i, str_info[&#34;Spike&#34;], len(str_info[&#34;Units&#34;]),
                str_info[&#34;Units&#34;], str_info[&#34;LFP&#34;],
                str_info[&#34;Position&#34;], str_info[&#34;Root&#34;])
            all_str_info.append(b_str)
        return &#34;\n&#34;.join(all_str_info)

    def _full_string(self):
        &#34;&#34;&#34;Full string representation of the container.&#34;&#34;&#34;
        string = (
            &#34;NData Container Object with {} objects:\n&#34; +
            &#34;Set to Load on Fly? {}\n&#34; +
            &#34;Files are:\n{}\n&#34; +
            &#34;Units are:\n{}&#34;).format(
                self.get_num_data(),
                self._load_on_fly,
                pprint.pformat(self.get_file_dict()),
                pprint.pformat(self.get_units()))
        return string

    def _load_all_data(self):
        &#34;&#34;&#34;Intended private function which loads all the data.&#34;&#34;&#34;
        if self._load_on_fly:
            logging.error(
                &#34;Don&#39;t load all the data in container if loading on the fly&#34;)
        for key, vals in self.get_file_dict().items():
            for idx, _ in enumerate(vals):
                if idx &gt;= self.get_num_data():
                    self.add_data(NData())

            for idx, descriptor in enumerate(vals):
                self._load(key, descriptor, idx=idx)

    def _load(self, key, descriptor, idx=None, ndata=None):
        &#34;&#34;&#34;
        Intended private function which loads data for a specific filetype.

        The NData object loaded into is either passed in, or found by idx.

        Parameters
        ----------
        key : str
            &#34;Spike&#34;, &#34;Position&#34;, or &#34;LFP&#34;, which filetype to load
        descriptor : tuple
            (filename, objectname, system) tuple
        idx : int
            Optional parameter to get corresponding data from _collection
        ndata : NData
            Optional parameter to allow passing in an ndata object to load to

        Returns
        -------
        None

        &#34;&#34;&#34;
        if ndata is None:
            ndata = self.get_data(idx)
        key_fn_pairs = {
            &#34;Spike&#34;: [
                getattr(ndata, &#34;set_spike_file&#34;),
                getattr(ndata, &#34;set_spike_name&#34;),
                getattr(ndata, &#34;load_spike&#34;)],
            &#34;Position&#34;: [
                getattr(ndata, &#34;set_spatial_file&#34;),
                getattr(ndata, &#34;set_spatial_name&#34;),
                getattr(ndata, &#34;load_spatial&#34;)],
            &#34;LFP&#34;: [
                getattr(ndata, &#34;set_lfp_file&#34;),
                getattr(ndata, &#34;set_lfp_name&#34;),
                getattr(ndata, &#34;load_lfp&#34;)],
        }

        filename, objectname, system = descriptor

        if objectname is not None:
            key_fn_pairs[key][1](objectname)

        if system is not None:
            ndata.set_system(system)

        if key == &#34;Position&#34; and self._share_positions and idx != 0:
            if self._load_on_fly:
                ndata.spatial = self._last_data_pt[1].spatial
            else:
                ndata.spatial = self.get_data(0).spatial
            return

        if filename is not None:
            key_fn_pairs[key][0](filename)
            key_fn_pairs[key][2]()

    def __repr__(self):
        &#34;&#34;&#34;Return a string representation of the collection.&#34;&#34;&#34;
        return self.string_repr(pretty=True)

    def __getitem__(self, index):
        &#34;&#34;&#34;Return the data object with corresponding unit at index.&#34;&#34;&#34;
        data_index, unit_index = self._index_to_data_pos(index)
        return self.get_data_at(data_index, unit_index)

    def __len__(self):
        &#34;&#34;&#34;Return the number of units in the collection.&#34;&#34;&#34;
        counts = self._unit_count
        if len(counts) == 0:
            print(&#34;Recounting units&#34;)
            self._unit_count = self._count_num_units()
            counts = self._unit_count
        return sum(counts)

    def _count_num_units(self):
        &#34;&#34;&#34;Intended private function to count units in the collection.&#34;&#34;&#34;
        counts = []
        for unit_list in self.get_units():
            counts.append(len(unit_list))
        return counts

    def _index_to_data_pos(self, index):
        &#34;&#34;&#34;
        Intended private function to turn an index into a tuple indices.

        Parameters
        ----------
        index : int
            The unit index to convert to a data index and unit index for that

        Returns
        -------
        tuple
            (data collection index, unit index for this data object)

        &#34;&#34;&#34;
        counts = self._unit_count
        if len(counts) == 0:
            print(&#34;Recounting units&#34;)
            self._unit_count = self._count_num_units()
            counts = self._unit_count
        if index &gt;= len(self):
            print(&#34;Error, index {} is out of range {} for {}&#34;.format(
                index, len(self) - 1, self))
            raise IndexError
        else:
            running_sum, running_idx = 0, 0
            for count in counts:
                if index &lt; (running_sum + count):
                    return running_idx, (index - running_sum)
                else:
                    running_sum += count
                    running_idx += 1

    def __iter__(self):
        return NDataContainerIterator(self)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="neurochat.nc_datacontainer.NDataContainer"><code class="flex name class">
<span>class <span class="ident">NDataContainer</span></span>
<span>(</span><span>share_positions=False, load_on_fly=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Class for storing multiple file locations for ndata objects.</p>
<p>Additionally the ndata objects themselves can be stored.</p>
<p>Initialise the class.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>share_positions</code></strong> :&ensp;<code>bool</code></dt>
<dd>Share the same position file between the data objects</dd>
<dt><strong><code>load_on_fly</code></strong> :&ensp;<code>bool</code></dt>
<dd>Don't store all the data in memory,
instead load it as needed, on the fly</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>_container</code></strong> :&ensp;<code>List</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>_file_names_dict</code></strong> :&ensp;<code>Dict</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>_units</code></strong> :&ensp;<code>List</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>_unit_count</code></strong> :&ensp;<code>List</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>_share_positions</code></strong> :&ensp;<code>bool</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>_load_on_fly</code></strong> :&ensp;<code>bool</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>_smoothed_speed</code></strong> :&ensp;<code>bool</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>_last_data_pt</code></strong> :&ensp;<code>tuple</code> (<code>int</code>, <code>NData</code>)</dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class NDataContainer():
    &#34;&#34;&#34;
    Class for storing multiple file locations for ndata objects.

    Additionally the ndata objects themselves can be stored.
    &#34;&#34;&#34;

    def __init__(self, share_positions=False, load_on_fly=False):
        &#34;&#34;&#34;
        Initialise the class.

        Parameters
        ----------
        share_positions : bool
            Share the same position file between the data objects
        load_on_fly : bool
            Don&#39;t store all the data in memory,
            instead load it as needed, on the fly

        Attributes
        ----------
        _container : List
        _file_names_dict : Dict
        _units : List
        _unit_count : List
        _share_positions : bool
        _load_on_fly : bool
        _smoothed_speed : bool
        _last_data_pt : tuple (int, NData)

        &#34;&#34;&#34;
        self._file_names_dict = {}
        self._units = []
        self._container = []
        self._unit_count = []
        self._share_positions = share_positions
        self._load_on_fly = load_on_fly
        self._last_data_pt = (1, None)
        self._smoothed_speed = False

    class EFileType(Enum):
        &#34;&#34;&#34;The different filetypes that can be added to an object.&#34;&#34;&#34;

        Spike = 1
        Position = 2
        LFP = 3

    def get_num_data(self):
        &#34;&#34;&#34;Return the number of Ndata objects in the container.&#34;&#34;&#34;
        if self._load_on_fly:
            for _, vals in self.get_file_dict().items():
                return len(vals)
        return len(self._container)

    def get_file_dict(self, key=None):
        &#34;&#34;&#34;Return the key value filename dictionary for this collection.&#34;&#34;&#34;
        if key:
            return self._file_names_dict.get(key, None)
        return self._file_names_dict

    def get_units(self, index=None):
        &#34;&#34;&#34;
        Return the units in this collection, optionally at a given index.

        Parameters
        ----------
        index : int
            Optional collection data index to get the units for

        Returns
        -------
        list
            Either a list containing lists of all units in the collection
            or the list of units for the given data index

        &#34;&#34;&#34;
        if index is None:
            return self._units
        if index &gt;= self.get_num_data() and (not self._load_on_fly):
            logging.error(&#34;Input index to get_data out of range&#34;)
            return
        return self._units[index]

    def get_data(self, index=None):
        &#34;&#34;&#34;
        Return the NData objects in this collection, or a specific object.

        Do not call this with no index if loading data on the fly

        Parameters
        ----------
        index : int
            Optional index to get data at

        Returns
        -------
        NData or list of NData objects

        &#34;&#34;&#34;
        if self._load_on_fly:
            if index is None:
                logging.error(&#34;Can&#39;t load all data when loading on the fly&#34;)
            result = NData()
            for key, vals in self.get_file_dict().items():
                descriptor = vals[index]
                self._load(key, descriptor, ndata=result)
            return result
        if index is None:
            return self._container
        if index &gt;= self.get_num_data():
            logging.error(&#34;Input index to get_data out of range&#34;)
            return
        return self._container[index]

    def add_data(self, data):
        &#34;&#34;&#34;Add an NData object to this container.&#34;&#34;&#34;
        if isinstance(data, NData):
            self._container.append(data)
        else:
            logging.error(&#34;Adding incorrect object to data container&#34;)
            return

    def list_all_units(self):
        &#34;&#34;&#34;Print all the units in the container.&#34;&#34;&#34;
        if self._load_on_fly:
            for key, vals in self.get_file_dict().items():
                if key == &#34;Spike&#34;:
                    for descriptor in vals:
                        result = NData()
                        self._load(key, descriptor, ndata=result)
                        print(&#34;units are {}&#34;.format(result.get_unit_list()))
        else:
            for data in self._container:
                print(&#34;units are {}&#34;.format(data.get_unit_list()))

    def add_files(self, f_type, descriptors):
        &#34;&#34;&#34;
        Add a list of filenames of the given type to the container.

        Parameters
        ----------
        f_type : EFileType:
            The type of file being added (Spike, LFP, Position)
        descriptors : list
            Either a list of filenames, or a list of tuples in the order
            (filenames, obj_names, data_sytem). Filenames should be absolute

        Returns
        -------
        None

        &#34;&#34;&#34;
        if isinstance(descriptors, list):
            descriptors = (descriptors, None, None)
        filenames, _, _ = descriptors
        if not isinstance(f_type, self.EFileType):
            logging.error(
                &#34;Parameter f_type in add files must be of EFileType\n&#34; +
                &#34;given {}&#34;.format(f_type))
            return

        if f_type.name == &#34;Position&#34; and self._share_positions and len(filenames) == 1:
            for _ in range(len(self.get_file_dict()[&#34;Spike&#34;]) - 1):
                filenames.append(filenames[0])

        # Ensure lists are empty or of equal size
        for l in descriptors:
            if l is not None:
                if len(l) != len(filenames):
                    logging.error(
                        &#34;add_files called with differing number of filenames and other data&#34;
                    )
                    return

        for idx in range(len(filenames)):
            description = []
            for el in descriptors:
                if el is not None:
                    description.append(el[idx])
                else:
                    description.append(None)
            self._file_names_dict.setdefault(
                f_type.name, []).append(description)

    def add_all_files(self, spats, spikes, lfps):
        &#34;&#34;&#34;
        Quickly add a list of positions, spikes and lfps.

        Parameters
        ----------
        spats : list
            The list of spatial files
        spikes : list
            The list of spike files
        lfps : list
            The list of lfp files

        Returns
        -------
        None

        &#34;&#34;&#34;
        self.add_files(self.EFileType.Position, spats)
        self.add_files(self.EFileType.Spike, spikes)
        self.add_files(self.EFileType.LFP, lfps)

    def set_units(self, units=&#39;all&#39;):
        &#34;&#34;&#34;Set the list of units for the collection.&#34;&#34;&#34;
        self._units = []
        if self.get_file_dict() == {}:
            print(&#34;Error: Can&#39;t set units for empty collection&#34;)
            return
        if units == &#39;all&#39;:
            if self._load_on_fly:
                vals = self.get_file_dict()[&#34;Spike&#34;]
                for descriptor in vals:
                    result = NData()
                    self._load(&#34;Spike&#34;, descriptor, ndata=result)
                    self._units.append(result.get_unit_list())
            else:
                for data in self.get_data():
                    self._units.append(data.get_unit_list())

        elif isinstance(units, list):
            for idx, unit in enumerate(units):
                if unit == &#39;all&#39;:
                    if self._load_on_fly:
                        vals = self.get_file_dict()[&#34;Spike&#34;]
                        descriptor = vals[idx]
                        result = NData()
                        self._load(&#34;Spike&#34;, descriptor, ndata=result)
                        all_units = result.get_unit_list()
                    else:
                        all_units = self.get_data(idx).get_unit_list()
                    self._units.append(all_units)
                elif isinstance(unit, int):
                    self._units.append([unit])
                elif isinstance(unit, list):
                    self._units.append(unit)
                else:
                    logging.error(
                        &#34;Unrecognised type {} passed to set units&#34;.format(type(unit)))

        else:
            logging.error(
                &#34;Unrecognised type {} passed to set units&#34;.format(type(units)))
        self._unit_count = self._count_num_units()

    def setup(self):
        &#34;&#34;&#34;Perform data initialisation based on the input filenames.&#34;&#34;&#34;
        if self._load_on_fly:
            self._last_data_pt = (1, None)
        else:
            self._load_all_data()

    def add_files_from_excel(self, file_loc, unit_sep=&#34; &#34;):
        &#34;&#34;&#34;
        Add filepaths from an excel file.

        These should be setup to be in the order:
        directory | position file | spike file | unit numbers | eeg extension

        Parameters
        ----------
        file_loc : str
            Name of the excel file that contains the data specifications
        unit_sep : str
            Optional separator character for unit numbers, default &#34; &#34;

        Returns
        -------
        excel_info :
            The raw info parsed from the excel file for further use

        &#34;&#34;&#34;
        pos_files = []
        spike_files = []
        units = []
        lfp_files = []
        to_merge = []

        if os.path.exists(file_loc):
            excel_info = pd.read_excel(file_loc, index_col=None)
            if excel_info.shape[1] % 5 != 0:
                logging.error(
                    &#34;Incorrect excel file format, it should be:\n&#34; +
                    &#34;directory | position file | spike file&#34; +
                    &#34;| unit numbers | eeg extension&#34;)
                return

            # excel_info = excel_info.iloc[:, 1:] # Can be used to remove index
            count = 0
            for full_row in excel_info.itertuples():
                split = [full_row[i:i + 5]
                         for i in range(1, len(full_row), 5)
                         if not pd.isna(full_row[i])]
                merge = True if len(split) &gt; 1 else False
                merge_list = []
                for row in split:
                    base_dir = row[0]
                    pos_name = row[1]
                    tetrode_name = row[2]

                    if pos_name[-4:] == &#39;.txt&#39;:
                        spat_file = base_dir + os.sep + pos_name
                    else:
                        spat_file = base_dir + os.sep + pos_name + &#39;.txt&#39;

                    spike_file = base_dir + os.sep + tetrode_name

                    # Load the unit numbers
                    unit_info = row[3]
                    if unit_info == &#34;all&#34;:
                        unit_list = &#34;all&#34;
                    elif isinstance(unit_info, int):
                        unit_list = unit_info
                    elif isinstance(unit_info, float):
                        unit_list = int(unit_info)
                    else:
                        unit_list = [
                            int(x) for x in unit_info.split(&#34; &#34;) if x is not &#34;&#34;]

                    # Load the lfp
                    lfp_ext = row[4]
                    if lfp_ext[0] != &#34;.&#34;:
                        lfp_ext = &#34;.&#34; + lfp_ext
                    spike_name = remove_extension(spike_file, keep_dot=False)
                    lfp_file = spike_name + lfp_ext

                    pos_files.append(spat_file)
                    spike_files.append(spike_file)
                    lfp_files.append(lfp_file)
                    units.append(unit_list)
                    merge_list.append(count)
                    count += 1
                if merge:
                    to_merge.append(merge_list)

            # Complete the file setup based on parsing from the excel file
            self.add_all_files(pos_files, spike_files, lfp_files)
            self.setup()
            self.set_units(units)

            for idx, merge_list in enumerate(to_merge):
                self.merge(merge_list)
                for j in range(idx + 1, len(to_merge)):
                    to_merge[j] = [
                        k - len(merge_list) + 1 for k in to_merge[j]]
            return excel_info
        else:
            logging.error(&#39;Excel file does not exist!&#39;)
            return None

    # Created by Sean Martin with help from Matheus Cafalchio
    def add_axona_files_from_dir(
            self, directory, recursive=False, verbose=False, **kwargs):
        &#34;&#34;&#34;
        Go through a directory, extracting files from it.

        Parameters
        ----------
        directory : str
            The directory to parse through
        recursive : bool, optional. Defaults to False.
            Whether to recurse through dirs
        verbose: bool, optional. Defaults to False.
            Whether to print the files being added.

        **kwargs: keyword arguments
            tetrode_list : list
                list of tetrodes to consider
            data_extension : str default .set
            cluster_extension : str default .cut
            pos_extension : str default .txt
            lfp_extension : str default .eeg
            re_filter : str default None 
                regex string for matching filenames
            save_result : bool default True
                should save the resulting collection to a file
            unit_cutoff : tuple of ints
                don&#39;t consider any recordings with units outside this range

        Returns
        -------
        None

        &#34;&#34;&#34;
        default_tetrode_list = [
            1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
        tetrode_list = kwargs.get(&#34;tetrode_list&#34;, default_tetrode_list)
        data_extension = kwargs.get(&#34;data_extension&#34;, &#34;.set&#34;)
        cluster_extension = kwargs.get(&#34;cluster_extension&#34;, &#34;.cut&#34;)
        clu_extension = kwargs.get(&#34;clu_extension&#34;, &#34;.clu.X&#34;)
        pos_extension = kwargs.get(&#34;pos_extension&#34;, &#34;.txt&#34;)
        lfp_extension = kwargs.get(&#34;lfp_extension&#34;, &#34;.eeg&#34;)
        re_filter = kwargs.get(&#34;re_filter&#34;, None)
        save_result = kwargs.get(&#34;save_result&#34;, True)
        unit_cutoff = kwargs.get(&#34;unit_cutoff&#34;, None)

        files = get_all_files_in_dir(
            directory, data_extension,
            recursive=recursive, verbose=verbose,
            re_filter=re_filter, return_absolute=True)
        txt_files = get_all_files_in_dir(
            directory, pos_extension,
            recursive=recursive, verbose=verbose,
            re_filter=re_filter, return_absolute=True)

        for filename in files:
            filename = filename[:-len(data_extension)]
            for tetrode in tetrode_list:
                spike_name = filename + &#39;.&#39; + str(tetrode)
                cut_name = filename + &#39;_&#39; + str(tetrode) + cluster_extension
                clu_name = filename + clu_extension[:-1] + str(tetrode)
                lfp_name = filename + lfp_extension

                if not os.path.isfile(os.path.join(directory, spike_name)):
                    continue
                # Don&#39;t consider files that have not been clustered
                if not (
                        os.path.isfile(os.path.join(directory, cut_name)) or
                        os.path.isfile(os.path.join(directory, clu_name))):
                    logging.info(
                        &#34;Skipping tetrode {} - no cluster file named {} or {}&#34;.format(tetrode, cut_name, os.path.basename(clu_name)))
                    continue

                for fname in txt_files:
                    if fname[:len(filename)] == filename:
                        pos_name = fname
                        break

                else:
                    logging.info(
                        &#34;Skipping tetrode {} - no position file for {}&#34;.format(tetrode, filename))
                    continue

                self.add_files(NDataContainer.EFileType.Spike, [spike_name])
                self.add_files(NDataContainer.EFileType.Position, [pos_name])
                self.add_files(NDataContainer.EFileType.LFP, [lfp_name])
        self.set_units()

        if unit_cutoff:
            self.remove_recordings_units(
                unit_cutoff[0], unit_cutoff[1], verbose=verbose)
        if save_result:
            friendly_re = &#34;&#34;
            if re_filter:
                friendly_re = &#34;_&#34; + \
                    &#34; &#34;.join(re.findall(&#34;[a-zA-Z]+&#34;, re_filter))
            name = (
                &#34;file_list_&#34; + os.path.basename(directory) +
                friendly_re + &#34;.txt&#34;)
            out_loc = os.path.join(directory, &#34;nc_results&#34;, name)
            make_dir_if_not_exists(out_loc)
            with open(out_loc, &#39;w&#39;) as f:
                f.write(str(self))
            print(&#34;Wrote list of files considered to {}&#34;.format(out_loc))
            return out_loc
        return None

    def merge(self, indices, force_equal_units=True):
        &#34;&#34;&#34;
        Merge the data from multiple indices together into the first index.

        ONLY FUNCTIONS FOR POSITIONS AND SPIKES CURRENTLY - DOES NOT MERGE LFP.
        Only call this after loading the data, and not while loading on the fly

        Parameters
        ----------
        indices: list
            The list of indices in the data to merge together
        force_equal_units:
            The merged indexes must have the same unit numbers available

        Returns
        -------
        The merged data point

        &#34;&#34;&#34;
        if self._load_on_fly:
            logging.error(&#34;Don&#39;t call merge when loading on the fly&#34;)
            return

        target_index = indices[0]

        data_to_merge = []
        target_data = self.get_data(target_index)
        for idx in indices[1:]:
            data = self.get_data(idx)
            data_to_merge.append(data)

        units1 = self.get_units(target_index)
        for idx, data in zip(indices[1:], data_to_merge):
            units2 = self.get_units(idx)
            if force_equal_units and (not units1 == units2):
                logging.error(
                    &#34;Can&#39;t merge files with unequal units\n&#34; +
                    &#34;Units are {} , {}&#34;.format(units1, units2))
                return

            # Merge the spikes based on times (waveforms not done yet)
            new_spike_times = (
                data.spike.get_timestamp() +
                target_data.spike.get_duration())
            new_duration = (
                target_data.spike.get_duration() +
                data.spike.get_duration())
            new_tags = data.spike.get_unit_tags()

            # Merge the spatial information based on times
            new_spat_times = (
                data.spatial._time +
                target_data.spike.get_duration())
            new_pos_x = data.spatial._pos_x
            new_pos_y = data.spatial._pos_y
            new_direction = data.spatial._direction
            new_speed = data.spatial._speed

            target_data.spike._timestamp = np.append(
                target_data.spike._timestamp, new_spike_times)
            target_data.spike._unit_Tags = np.append(
                target_data.spike._unit_Tags, new_tags)
            target_data.spike._set_duration(new_duration)

            target_data.spatial._time = np.append(
                target_data.spatial._time, new_spat_times)
            # NB this may not work properly due to different borders
            target_data.spatial._pos_x = np.append(
                target_data.spatial._pos_x, new_pos_x)
            target_data.spatial._pos_y = np.append(
                target_data.spatial._pos_y, new_pos_y)
            target_data.spatial._direction = np.append(
                target_data.spatial._direction, new_direction)
            target_data.spatial._speed = np.append(
                target_data.spatial._speed, new_speed)

        self._container[target_index] = target_data

        for idx in indices[1:]:
            self._container.pop(idx)
            self._units.pop(idx)
            indices[1:] = [a - 1 for a in indices[1:]]
        self._unit_count = self._count_num_units()

        self._container[target_index].set_unit_no(
            self.get_units(target_index)[0])
        return self.get_data(target_index)

    def subsample(self, key):
        &#34;&#34;&#34;
        Return a subsample of the original data collection.

        This subsample is not a reference, but a deep copy

        Parameters
        ----------
        key : Slice or int
            How to sample the original collection

        Returns
        -------
        NDataContainer
            The deep copied subsample

        &#34;&#34;&#34;
        result = copy.deepcopy(self)

        for k in result._file_names_dict:
            result._file_names_dict[k] = result._file_names_dict[k][key]
            if isinstance(key, int):
                result._file_names_dict[k] = [result._file_names_dict[k]]

        if len(result._units) &gt; 0:
            result._units = result._units[key]
            if isinstance(key, int):
                result._units = [result._units]

        if len(result._container) &gt; 0:
            result._container = result._container[key]
            if isinstance(key, int):
                result._container = [result._container[key]]

        result._unit_count = result._count_num_units()
        return result

    def sort_units_spatially(self, should_sort_list=None, mode=&#34;vertical&#34;):
        &#34;&#34;&#34;
        Sort the units in the collection based on the place field centroid.

        Parameters
        ----------
        should_sort_list: list
            Optional list of boolean values indicating what objects
        mode: str
            &#34;horizontal&#34; or &#34;vertical&#34;, indicating what axis to sort on.

        Returns
        -------
        None

        &#34;&#34;&#34;
        if mode == &#34;vertical&#34;:
            h = 1
        elif mode == &#34;horizontal&#34;:
            h = 0
        else:
            logging.error(
                &#34;NDataContainer: &#34;
                + &#34;Only modes horizontal and vertical are supported&#34;)

        if should_sort_list is None:
            should_sort_list = [True for _ in range(self.get_num_data())]

        for idx, bool_val in enumerate(should_sort_list):
            if bool_val:
                centroids = []
                data = self.get_data(idx)
                for unit in self.get_units()[idx]:
                    data.set_unit_no(unit)
                    place_info = data.place()
                    centroid = place_info[&#34;centroid&#34;]
                    centroids.append(centroid)
                self._units[idx] = [unit for _, unit in sorted(
                    zip(centroids, self.get_units()[idx]),
                    key=lambda pair: pair[0][h])]

    def get_index_info(self, idx, absolute=False):
        &#34;&#34;&#34;Return the Spike, LFP, Position and Unit info at idx.&#34;&#34;&#34;
        str_info = {}
        dirnames = []
        if absolute:
            idx, u_idx = self._index_to_data_pos(idx)

        for key in [&#34;Spike&#34;, &#34;LFP&#34;, &#34;Position&#34;]:
            name = self.get_file_dict(key)[idx][0]
            str_info[key] = (os.path.basename(name))
            dirnames.append(os.path.dirname(name))

        if absolute:
            str_info[&#34;Units&#34;] = (self.get_units(idx)[u_idx])
        else:
            str_info[&#34;Units&#34;] = (self.get_units(idx))

        if len(set(dirnames)) == 1:
            str_info[&#34;Root&#34;] = dirnames[0]
        else:
            print(&#34;Not all files are in the same directory {} {}&#34;.format(
                &#34;:Spike, LFP, Position: &#34;, dirnames))
            str_info[&#34;Root&#34;] = dirnames
        return str_info

    def string_repr(self, pretty=True):
        &#34;&#34;&#34;
        Return a string representation of this class.
        Parameters
        ----------
        pretty : str, Default True
            Should return a pretty version or all the info.
        &#34;&#34;&#34;
        if pretty:
            return self._pretty_string()
        else:
            return self._full_string()

    def remove_recordings_units(
            self, unit_lb=0, unit_ub=10000, verbose=False):
        start_size = self.get_num_data()
        start_total = len(self)
        for i in range(self.get_num_data() - 1, -1, -1):
            unit_count = len(self.get_units(i))
            if (unit_count &gt; unit_ub) or (unit_count &lt; unit_lb):
                for key in (&#34;Spike&#34;, &#34;LFP&#34;, &#34;Position&#34;):
                    name = self._file_names_dict[key].pop(i)
                    if (key is &#34;Spike&#34;) and verbose:
                        print(&#34;Removed {} with {} units&#34;.format(
                            os.path.basename(name[0]), unit_count))
                if self._unit_count.pop(i) != unit_count:
                    print(&#34;Error in remove recording {}&#34;.format(name))
                self._last_data_pt = (1, None)
                self._units.pop(i)
                if not self._load_on_fly:
                    self._container.pop(i)
        end_size = self.get_num_data()
        self._count_num_units()
        end_total = len(self)

        print((&#34;{} tetrodes with {} units reduced to &#34;
               + &#34;{} tetrodes with {} units&#34;).format(
            start_size, start_total, end_size, end_total))

    def get_data_at(self, data_index, unit_index):
        if self._load_on_fly:
            try:
                if data_index == self._last_data_pt[0]:
                    result = self._last_data_pt[1]
                else:
                    result = NData()
                    for key, vals in self.get_file_dict().items():
                        descriptor = vals[data_index]
                        self._load(key, descriptor,
                                   idx=data_index, ndata=result)
                    self._last_data_pt = (data_index, result)
            except Exception as e:
                log_exception(e, &#34;During loading data&#34;)
        else:
            result = self.get_data(data_index)
        if len(self.get_units()) &gt; 0:
            result.set_unit_no(self.get_units(data_index)[unit_index])
        return result

    # Methods from here on should be for private class use
    def _pretty_string(self):
        &#34;&#34;&#34;Alternative string representation should be prettier.&#34;&#34;&#34;
        all_str_info = []
        for i in range(self.get_num_data()):
            str_info = self.get_index_info(i)
            b_str = &#34;{}: \n\tSpk {}\n\tUnt {}: {}\n\tLfp {}\n\tPos {}\n\tDir {}&#34;.format(
                i, str_info[&#34;Spike&#34;], len(str_info[&#34;Units&#34;]),
                str_info[&#34;Units&#34;], str_info[&#34;LFP&#34;],
                str_info[&#34;Position&#34;], str_info[&#34;Root&#34;])
            all_str_info.append(b_str)
        return &#34;\n&#34;.join(all_str_info)

    def _full_string(self):
        &#34;&#34;&#34;Full string representation of the container.&#34;&#34;&#34;
        string = (
            &#34;NData Container Object with {} objects:\n&#34; +
            &#34;Set to Load on Fly? {}\n&#34; +
            &#34;Files are:\n{}\n&#34; +
            &#34;Units are:\n{}&#34;).format(
                self.get_num_data(),
                self._load_on_fly,
                pprint.pformat(self.get_file_dict()),
                pprint.pformat(self.get_units()))
        return string

    def _load_all_data(self):
        &#34;&#34;&#34;Intended private function which loads all the data.&#34;&#34;&#34;
        if self._load_on_fly:
            logging.error(
                &#34;Don&#39;t load all the data in container if loading on the fly&#34;)
        for key, vals in self.get_file_dict().items():
            for idx, _ in enumerate(vals):
                if idx &gt;= self.get_num_data():
                    self.add_data(NData())

            for idx, descriptor in enumerate(vals):
                self._load(key, descriptor, idx=idx)

    def _load(self, key, descriptor, idx=None, ndata=None):
        &#34;&#34;&#34;
        Intended private function which loads data for a specific filetype.

        The NData object loaded into is either passed in, or found by idx.

        Parameters
        ----------
        key : str
            &#34;Spike&#34;, &#34;Position&#34;, or &#34;LFP&#34;, which filetype to load
        descriptor : tuple
            (filename, objectname, system) tuple
        idx : int
            Optional parameter to get corresponding data from _collection
        ndata : NData
            Optional parameter to allow passing in an ndata object to load to

        Returns
        -------
        None

        &#34;&#34;&#34;
        if ndata is None:
            ndata = self.get_data(idx)
        key_fn_pairs = {
            &#34;Spike&#34;: [
                getattr(ndata, &#34;set_spike_file&#34;),
                getattr(ndata, &#34;set_spike_name&#34;),
                getattr(ndata, &#34;load_spike&#34;)],
            &#34;Position&#34;: [
                getattr(ndata, &#34;set_spatial_file&#34;),
                getattr(ndata, &#34;set_spatial_name&#34;),
                getattr(ndata, &#34;load_spatial&#34;)],
            &#34;LFP&#34;: [
                getattr(ndata, &#34;set_lfp_file&#34;),
                getattr(ndata, &#34;set_lfp_name&#34;),
                getattr(ndata, &#34;load_lfp&#34;)],
        }

        filename, objectname, system = descriptor

        if objectname is not None:
            key_fn_pairs[key][1](objectname)

        if system is not None:
            ndata.set_system(system)

        if key == &#34;Position&#34; and self._share_positions and idx != 0:
            if self._load_on_fly:
                ndata.spatial = self._last_data_pt[1].spatial
            else:
                ndata.spatial = self.get_data(0).spatial
            return

        if filename is not None:
            key_fn_pairs[key][0](filename)
            key_fn_pairs[key][2]()

    def __repr__(self):
        &#34;&#34;&#34;Return a string representation of the collection.&#34;&#34;&#34;
        return self.string_repr(pretty=True)

    def __getitem__(self, index):
        &#34;&#34;&#34;Return the data object with corresponding unit at index.&#34;&#34;&#34;
        data_index, unit_index = self._index_to_data_pos(index)
        return self.get_data_at(data_index, unit_index)

    def __len__(self):
        &#34;&#34;&#34;Return the number of units in the collection.&#34;&#34;&#34;
        counts = self._unit_count
        if len(counts) == 0:
            print(&#34;Recounting units&#34;)
            self._unit_count = self._count_num_units()
            counts = self._unit_count
        return sum(counts)

    def _count_num_units(self):
        &#34;&#34;&#34;Intended private function to count units in the collection.&#34;&#34;&#34;
        counts = []
        for unit_list in self.get_units():
            counts.append(len(unit_list))
        return counts

    def _index_to_data_pos(self, index):
        &#34;&#34;&#34;
        Intended private function to turn an index into a tuple indices.

        Parameters
        ----------
        index : int
            The unit index to convert to a data index and unit index for that

        Returns
        -------
        tuple
            (data collection index, unit index for this data object)

        &#34;&#34;&#34;
        counts = self._unit_count
        if len(counts) == 0:
            print(&#34;Recounting units&#34;)
            self._unit_count = self._count_num_units()
            counts = self._unit_count
        if index &gt;= len(self):
            print(&#34;Error, index {} is out of range {} for {}&#34;.format(
                index, len(self) - 1, self))
            raise IndexError
        else:
            running_sum, running_idx = 0, 0
            for count in counts:
                if index &lt; (running_sum + count):
                    return running_idx, (index - running_sum)
                else:
                    running_sum += count
                    running_idx += 1

    def __iter__(self):
        return NDataContainerIterator(self)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="neurochat.nc_datacontainer.NDataContainer.EFileType"><code class="name">var <span class="ident">EFileType</span></code></dt>
<dd>
<section class="desc"><p>The different filetypes that can be added to an object.</p></section>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="neurochat.nc_datacontainer.NDataContainer.add_all_files"><code class="name flex">
<span>def <span class="ident">add_all_files</span></span>(<span>self, spats, spikes, lfps)</span>
</code></dt>
<dd>
<section class="desc"><p>Quickly add a list of positions, spikes and lfps.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>spats</code></strong> :&ensp;<code>list</code></dt>
<dd>The list of spatial files</dd>
<dt><strong><code>spikes</code></strong> :&ensp;<code>list</code></dt>
<dd>The list of spike files</dd>
<dt><strong><code>lfps</code></strong> :&ensp;<code>list</code></dt>
<dd>The list of lfp files</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def add_all_files(self, spats, spikes, lfps):
    &#34;&#34;&#34;
    Quickly add a list of positions, spikes and lfps.

    Parameters
    ----------
    spats : list
        The list of spatial files
    spikes : list
        The list of spike files
    lfps : list
        The list of lfp files

    Returns
    -------
    None

    &#34;&#34;&#34;
    self.add_files(self.EFileType.Position, spats)
    self.add_files(self.EFileType.Spike, spikes)
    self.add_files(self.EFileType.LFP, lfps)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_datacontainer.NDataContainer.add_axona_files_from_dir"><code class="name flex">
<span>def <span class="ident">add_axona_files_from_dir</span></span>(<span>self, directory, recursive=False, verbose=False, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Go through a directory, extracting files from it.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>directory</code></strong> :&ensp;<code>str</code></dt>
<dd>The directory to parse through</dd>
</dl>
<p>recursive : bool, optional. Defaults to False.
Whether to recurse through dirs
verbose: bool, optional. Defaults to False.
Whether to print the files being added.</p>
<dl>
<dt><strong><code>**kwargs</code></strong> :&ensp;<code>keyword</code> <code>arguments</code></dt>
<dd>tetrode_list : list
list of tetrodes to consider
data_extension : str default .set
cluster_extension : str default .cut
pos_extension : str default .txt
lfp_extension : str default .eeg
re_filter : str default None
regex string for matching filenames
save_result : bool default True
should save the resulting collection to a file
unit_cutoff : tuple of ints
don't consider any recordings with units outside this range</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def add_axona_files_from_dir(
        self, directory, recursive=False, verbose=False, **kwargs):
    &#34;&#34;&#34;
    Go through a directory, extracting files from it.

    Parameters
    ----------
    directory : str
        The directory to parse through
    recursive : bool, optional. Defaults to False.
        Whether to recurse through dirs
    verbose: bool, optional. Defaults to False.
        Whether to print the files being added.

    **kwargs: keyword arguments
        tetrode_list : list
            list of tetrodes to consider
        data_extension : str default .set
        cluster_extension : str default .cut
        pos_extension : str default .txt
        lfp_extension : str default .eeg
        re_filter : str default None 
            regex string for matching filenames
        save_result : bool default True
            should save the resulting collection to a file
        unit_cutoff : tuple of ints
            don&#39;t consider any recordings with units outside this range

    Returns
    -------
    None

    &#34;&#34;&#34;
    default_tetrode_list = [
        1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
    tetrode_list = kwargs.get(&#34;tetrode_list&#34;, default_tetrode_list)
    data_extension = kwargs.get(&#34;data_extension&#34;, &#34;.set&#34;)
    cluster_extension = kwargs.get(&#34;cluster_extension&#34;, &#34;.cut&#34;)
    clu_extension = kwargs.get(&#34;clu_extension&#34;, &#34;.clu.X&#34;)
    pos_extension = kwargs.get(&#34;pos_extension&#34;, &#34;.txt&#34;)
    lfp_extension = kwargs.get(&#34;lfp_extension&#34;, &#34;.eeg&#34;)
    re_filter = kwargs.get(&#34;re_filter&#34;, None)
    save_result = kwargs.get(&#34;save_result&#34;, True)
    unit_cutoff = kwargs.get(&#34;unit_cutoff&#34;, None)

    files = get_all_files_in_dir(
        directory, data_extension,
        recursive=recursive, verbose=verbose,
        re_filter=re_filter, return_absolute=True)
    txt_files = get_all_files_in_dir(
        directory, pos_extension,
        recursive=recursive, verbose=verbose,
        re_filter=re_filter, return_absolute=True)

    for filename in files:
        filename = filename[:-len(data_extension)]
        for tetrode in tetrode_list:
            spike_name = filename + &#39;.&#39; + str(tetrode)
            cut_name = filename + &#39;_&#39; + str(tetrode) + cluster_extension
            clu_name = filename + clu_extension[:-1] + str(tetrode)
            lfp_name = filename + lfp_extension

            if not os.path.isfile(os.path.join(directory, spike_name)):
                continue
            # Don&#39;t consider files that have not been clustered
            if not (
                    os.path.isfile(os.path.join(directory, cut_name)) or
                    os.path.isfile(os.path.join(directory, clu_name))):
                logging.info(
                    &#34;Skipping tetrode {} - no cluster file named {} or {}&#34;.format(tetrode, cut_name, os.path.basename(clu_name)))
                continue

            for fname in txt_files:
                if fname[:len(filename)] == filename:
                    pos_name = fname
                    break

            else:
                logging.info(
                    &#34;Skipping tetrode {} - no position file for {}&#34;.format(tetrode, filename))
                continue

            self.add_files(NDataContainer.EFileType.Spike, [spike_name])
            self.add_files(NDataContainer.EFileType.Position, [pos_name])
            self.add_files(NDataContainer.EFileType.LFP, [lfp_name])
    self.set_units()

    if unit_cutoff:
        self.remove_recordings_units(
            unit_cutoff[0], unit_cutoff[1], verbose=verbose)
    if save_result:
        friendly_re = &#34;&#34;
        if re_filter:
            friendly_re = &#34;_&#34; + \
                &#34; &#34;.join(re.findall(&#34;[a-zA-Z]+&#34;, re_filter))
        name = (
            &#34;file_list_&#34; + os.path.basename(directory) +
            friendly_re + &#34;.txt&#34;)
        out_loc = os.path.join(directory, &#34;nc_results&#34;, name)
        make_dir_if_not_exists(out_loc)
        with open(out_loc, &#39;w&#39;) as f:
            f.write(str(self))
        print(&#34;Wrote list of files considered to {}&#34;.format(out_loc))
        return out_loc
    return None</code></pre>
</details>
</dd>
<dt id="neurochat.nc_datacontainer.NDataContainer.add_data"><code class="name flex">
<span>def <span class="ident">add_data</span></span>(<span>self, data)</span>
</code></dt>
<dd>
<section class="desc"><p>Add an NData object to this container.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def add_data(self, data):
    &#34;&#34;&#34;Add an NData object to this container.&#34;&#34;&#34;
    if isinstance(data, NData):
        self._container.append(data)
    else:
        logging.error(&#34;Adding incorrect object to data container&#34;)
        return</code></pre>
</details>
</dd>
<dt id="neurochat.nc_datacontainer.NDataContainer.add_files"><code class="name flex">
<span>def <span class="ident">add_files</span></span>(<span>self, f_type, descriptors)</span>
</code></dt>
<dd>
<section class="desc"><p>Add a list of filenames of the given type to the container.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>f_type</code></strong> :&ensp;<code>EFileType</code>:</dt>
<dd>The type of file being added (Spike, LFP, Position)</dd>
<dt><strong><code>descriptors</code></strong> :&ensp;<code>list</code></dt>
<dd>Either a list of filenames, or a list of tuples in the order
(filenames, obj_names, data_sytem). Filenames should be absolute</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def add_files(self, f_type, descriptors):
    &#34;&#34;&#34;
    Add a list of filenames of the given type to the container.

    Parameters
    ----------
    f_type : EFileType:
        The type of file being added (Spike, LFP, Position)
    descriptors : list
        Either a list of filenames, or a list of tuples in the order
        (filenames, obj_names, data_sytem). Filenames should be absolute

    Returns
    -------
    None

    &#34;&#34;&#34;
    if isinstance(descriptors, list):
        descriptors = (descriptors, None, None)
    filenames, _, _ = descriptors
    if not isinstance(f_type, self.EFileType):
        logging.error(
            &#34;Parameter f_type in add files must be of EFileType\n&#34; +
            &#34;given {}&#34;.format(f_type))
        return

    if f_type.name == &#34;Position&#34; and self._share_positions and len(filenames) == 1:
        for _ in range(len(self.get_file_dict()[&#34;Spike&#34;]) - 1):
            filenames.append(filenames[0])

    # Ensure lists are empty or of equal size
    for l in descriptors:
        if l is not None:
            if len(l) != len(filenames):
                logging.error(
                    &#34;add_files called with differing number of filenames and other data&#34;
                )
                return

    for idx in range(len(filenames)):
        description = []
        for el in descriptors:
            if el is not None:
                description.append(el[idx])
            else:
                description.append(None)
        self._file_names_dict.setdefault(
            f_type.name, []).append(description)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_datacontainer.NDataContainer.add_files_from_excel"><code class="name flex">
<span>def <span class="ident">add_files_from_excel</span></span>(<span>self, file_loc, unit_sep=' ')</span>
</code></dt>
<dd>
<section class="desc"><p>Add filepaths from an excel file.</p>
<p>These should be setup to be in the order:
directory | position file | spike file | unit numbers | eeg extension</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_loc</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the excel file that contains the data specifications</dd>
<dt><strong><code>unit_sep</code></strong> :&ensp;<code>str</code></dt>
<dd>Optional separator character for unit numbers, default " "</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>excel_info</code> :</dt>
<dd>The raw info parsed from the excel file for further use</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def add_files_from_excel(self, file_loc, unit_sep=&#34; &#34;):
    &#34;&#34;&#34;
    Add filepaths from an excel file.

    These should be setup to be in the order:
    directory | position file | spike file | unit numbers | eeg extension

    Parameters
    ----------
    file_loc : str
        Name of the excel file that contains the data specifications
    unit_sep : str
        Optional separator character for unit numbers, default &#34; &#34;

    Returns
    -------
    excel_info :
        The raw info parsed from the excel file for further use

    &#34;&#34;&#34;
    pos_files = []
    spike_files = []
    units = []
    lfp_files = []
    to_merge = []

    if os.path.exists(file_loc):
        excel_info = pd.read_excel(file_loc, index_col=None)
        if excel_info.shape[1] % 5 != 0:
            logging.error(
                &#34;Incorrect excel file format, it should be:\n&#34; +
                &#34;directory | position file | spike file&#34; +
                &#34;| unit numbers | eeg extension&#34;)
            return

        # excel_info = excel_info.iloc[:, 1:] # Can be used to remove index
        count = 0
        for full_row in excel_info.itertuples():
            split = [full_row[i:i + 5]
                     for i in range(1, len(full_row), 5)
                     if not pd.isna(full_row[i])]
            merge = True if len(split) &gt; 1 else False
            merge_list = []
            for row in split:
                base_dir = row[0]
                pos_name = row[1]
                tetrode_name = row[2]

                if pos_name[-4:] == &#39;.txt&#39;:
                    spat_file = base_dir + os.sep + pos_name
                else:
                    spat_file = base_dir + os.sep + pos_name + &#39;.txt&#39;

                spike_file = base_dir + os.sep + tetrode_name

                # Load the unit numbers
                unit_info = row[3]
                if unit_info == &#34;all&#34;:
                    unit_list = &#34;all&#34;
                elif isinstance(unit_info, int):
                    unit_list = unit_info
                elif isinstance(unit_info, float):
                    unit_list = int(unit_info)
                else:
                    unit_list = [
                        int(x) for x in unit_info.split(&#34; &#34;) if x is not &#34;&#34;]

                # Load the lfp
                lfp_ext = row[4]
                if lfp_ext[0] != &#34;.&#34;:
                    lfp_ext = &#34;.&#34; + lfp_ext
                spike_name = remove_extension(spike_file, keep_dot=False)
                lfp_file = spike_name + lfp_ext

                pos_files.append(spat_file)
                spike_files.append(spike_file)
                lfp_files.append(lfp_file)
                units.append(unit_list)
                merge_list.append(count)
                count += 1
            if merge:
                to_merge.append(merge_list)

        # Complete the file setup based on parsing from the excel file
        self.add_all_files(pos_files, spike_files, lfp_files)
        self.setup()
        self.set_units(units)

        for idx, merge_list in enumerate(to_merge):
            self.merge(merge_list)
            for j in range(idx + 1, len(to_merge)):
                to_merge[j] = [
                    k - len(merge_list) + 1 for k in to_merge[j]]
        return excel_info
    else:
        logging.error(&#39;Excel file does not exist!&#39;)
        return None</code></pre>
</details>
</dd>
<dt id="neurochat.nc_datacontainer.NDataContainer.get_data"><code class="name flex">
<span>def <span class="ident">get_data</span></span>(<span>self, index=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Return the NData objects in this collection, or a specific object.</p>
<p>Do not call this with no index if loading data on the fly</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>index</code></strong> :&ensp;<code>int</code></dt>
<dd>Optional index to get data at</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>NData</code> or <code>list</code> of <code>NData</code> <code>objects</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_data(self, index=None):
    &#34;&#34;&#34;
    Return the NData objects in this collection, or a specific object.

    Do not call this with no index if loading data on the fly

    Parameters
    ----------
    index : int
        Optional index to get data at

    Returns
    -------
    NData or list of NData objects

    &#34;&#34;&#34;
    if self._load_on_fly:
        if index is None:
            logging.error(&#34;Can&#39;t load all data when loading on the fly&#34;)
        result = NData()
        for key, vals in self.get_file_dict().items():
            descriptor = vals[index]
            self._load(key, descriptor, ndata=result)
        return result
    if index is None:
        return self._container
    if index &gt;= self.get_num_data():
        logging.error(&#34;Input index to get_data out of range&#34;)
        return
    return self._container[index]</code></pre>
</details>
</dd>
<dt id="neurochat.nc_datacontainer.NDataContainer.get_data_at"><code class="name flex">
<span>def <span class="ident">get_data_at</span></span>(<span>self, data_index, unit_index)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_data_at(self, data_index, unit_index):
    if self._load_on_fly:
        try:
            if data_index == self._last_data_pt[0]:
                result = self._last_data_pt[1]
            else:
                result = NData()
                for key, vals in self.get_file_dict().items():
                    descriptor = vals[data_index]
                    self._load(key, descriptor,
                               idx=data_index, ndata=result)
                self._last_data_pt = (data_index, result)
        except Exception as e:
            log_exception(e, &#34;During loading data&#34;)
    else:
        result = self.get_data(data_index)
    if len(self.get_units()) &gt; 0:
        result.set_unit_no(self.get_units(data_index)[unit_index])
    return result</code></pre>
</details>
</dd>
<dt id="neurochat.nc_datacontainer.NDataContainer.get_file_dict"><code class="name flex">
<span>def <span class="ident">get_file_dict</span></span>(<span>self, key=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Return the key value filename dictionary for this collection.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_file_dict(self, key=None):
    &#34;&#34;&#34;Return the key value filename dictionary for this collection.&#34;&#34;&#34;
    if key:
        return self._file_names_dict.get(key, None)
    return self._file_names_dict</code></pre>
</details>
</dd>
<dt id="neurochat.nc_datacontainer.NDataContainer.get_index_info"><code class="name flex">
<span>def <span class="ident">get_index_info</span></span>(<span>self, idx, absolute=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Return the Spike, LFP, Position and Unit info at idx.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_index_info(self, idx, absolute=False):
    &#34;&#34;&#34;Return the Spike, LFP, Position and Unit info at idx.&#34;&#34;&#34;
    str_info = {}
    dirnames = []
    if absolute:
        idx, u_idx = self._index_to_data_pos(idx)

    for key in [&#34;Spike&#34;, &#34;LFP&#34;, &#34;Position&#34;]:
        name = self.get_file_dict(key)[idx][0]
        str_info[key] = (os.path.basename(name))
        dirnames.append(os.path.dirname(name))

    if absolute:
        str_info[&#34;Units&#34;] = (self.get_units(idx)[u_idx])
    else:
        str_info[&#34;Units&#34;] = (self.get_units(idx))

    if len(set(dirnames)) == 1:
        str_info[&#34;Root&#34;] = dirnames[0]
    else:
        print(&#34;Not all files are in the same directory {} {}&#34;.format(
            &#34;:Spike, LFP, Position: &#34;, dirnames))
        str_info[&#34;Root&#34;] = dirnames
    return str_info</code></pre>
</details>
</dd>
<dt id="neurochat.nc_datacontainer.NDataContainer.get_num_data"><code class="name flex">
<span>def <span class="ident">get_num_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Return the number of Ndata objects in the container.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_num_data(self):
    &#34;&#34;&#34;Return the number of Ndata objects in the container.&#34;&#34;&#34;
    if self._load_on_fly:
        for _, vals in self.get_file_dict().items():
            return len(vals)
    return len(self._container)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_datacontainer.NDataContainer.get_units"><code class="name flex">
<span>def <span class="ident">get_units</span></span>(<span>self, index=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Return the units in this collection, optionally at a given index.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>index</code></strong> :&ensp;<code>int</code></dt>
<dd>Optional collection data index to get the units for</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>Either a list containing lists of all units in the collection
or the list of units for the given data index</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_units(self, index=None):
    &#34;&#34;&#34;
    Return the units in this collection, optionally at a given index.

    Parameters
    ----------
    index : int
        Optional collection data index to get the units for

    Returns
    -------
    list
        Either a list containing lists of all units in the collection
        or the list of units for the given data index

    &#34;&#34;&#34;
    if index is None:
        return self._units
    if index &gt;= self.get_num_data() and (not self._load_on_fly):
        logging.error(&#34;Input index to get_data out of range&#34;)
        return
    return self._units[index]</code></pre>
</details>
</dd>
<dt id="neurochat.nc_datacontainer.NDataContainer.list_all_units"><code class="name flex">
<span>def <span class="ident">list_all_units</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Print all the units in the container.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def list_all_units(self):
    &#34;&#34;&#34;Print all the units in the container.&#34;&#34;&#34;
    if self._load_on_fly:
        for key, vals in self.get_file_dict().items():
            if key == &#34;Spike&#34;:
                for descriptor in vals:
                    result = NData()
                    self._load(key, descriptor, ndata=result)
                    print(&#34;units are {}&#34;.format(result.get_unit_list()))
    else:
        for data in self._container:
            print(&#34;units are {}&#34;.format(data.get_unit_list()))</code></pre>
</details>
</dd>
<dt id="neurochat.nc_datacontainer.NDataContainer.merge"><code class="name flex">
<span>def <span class="ident">merge</span></span>(<span>self, indices, force_equal_units=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Merge the data from multiple indices together into the first index.</p>
<p>ONLY FUNCTIONS FOR POSITIONS AND SPIKES CURRENTLY - DOES NOT MERGE LFP.
Only call this after loading the data, and not while loading on the fly</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>indices</code></strong> :&ensp;<code>list</code></dt>
<dd>The list of indices in the data to merge together</dd>
</dl>
<p>force_equal_units:
The merged indexes must have the same unit numbers available</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>The</code> <code>merged</code> <code>data</code> <code>point</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def merge(self, indices, force_equal_units=True):
    &#34;&#34;&#34;
    Merge the data from multiple indices together into the first index.

    ONLY FUNCTIONS FOR POSITIONS AND SPIKES CURRENTLY - DOES NOT MERGE LFP.
    Only call this after loading the data, and not while loading on the fly

    Parameters
    ----------
    indices: list
        The list of indices in the data to merge together
    force_equal_units:
        The merged indexes must have the same unit numbers available

    Returns
    -------
    The merged data point

    &#34;&#34;&#34;
    if self._load_on_fly:
        logging.error(&#34;Don&#39;t call merge when loading on the fly&#34;)
        return

    target_index = indices[0]

    data_to_merge = []
    target_data = self.get_data(target_index)
    for idx in indices[1:]:
        data = self.get_data(idx)
        data_to_merge.append(data)

    units1 = self.get_units(target_index)
    for idx, data in zip(indices[1:], data_to_merge):
        units2 = self.get_units(idx)
        if force_equal_units and (not units1 == units2):
            logging.error(
                &#34;Can&#39;t merge files with unequal units\n&#34; +
                &#34;Units are {} , {}&#34;.format(units1, units2))
            return

        # Merge the spikes based on times (waveforms not done yet)
        new_spike_times = (
            data.spike.get_timestamp() +
            target_data.spike.get_duration())
        new_duration = (
            target_data.spike.get_duration() +
            data.spike.get_duration())
        new_tags = data.spike.get_unit_tags()

        # Merge the spatial information based on times
        new_spat_times = (
            data.spatial._time +
            target_data.spike.get_duration())
        new_pos_x = data.spatial._pos_x
        new_pos_y = data.spatial._pos_y
        new_direction = data.spatial._direction
        new_speed = data.spatial._speed

        target_data.spike._timestamp = np.append(
            target_data.spike._timestamp, new_spike_times)
        target_data.spike._unit_Tags = np.append(
            target_data.spike._unit_Tags, new_tags)
        target_data.spike._set_duration(new_duration)

        target_data.spatial._time = np.append(
            target_data.spatial._time, new_spat_times)
        # NB this may not work properly due to different borders
        target_data.spatial._pos_x = np.append(
            target_data.spatial._pos_x, new_pos_x)
        target_data.spatial._pos_y = np.append(
            target_data.spatial._pos_y, new_pos_y)
        target_data.spatial._direction = np.append(
            target_data.spatial._direction, new_direction)
        target_data.spatial._speed = np.append(
            target_data.spatial._speed, new_speed)

    self._container[target_index] = target_data

    for idx in indices[1:]:
        self._container.pop(idx)
        self._units.pop(idx)
        indices[1:] = [a - 1 for a in indices[1:]]
    self._unit_count = self._count_num_units()

    self._container[target_index].set_unit_no(
        self.get_units(target_index)[0])
    return self.get_data(target_index)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_datacontainer.NDataContainer.remove_recordings_units"><code class="name flex">
<span>def <span class="ident">remove_recordings_units</span></span>(<span>self, unit_lb=0, unit_ub=10000, verbose=False)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def remove_recordings_units(
        self, unit_lb=0, unit_ub=10000, verbose=False):
    start_size = self.get_num_data()
    start_total = len(self)
    for i in range(self.get_num_data() - 1, -1, -1):
        unit_count = len(self.get_units(i))
        if (unit_count &gt; unit_ub) or (unit_count &lt; unit_lb):
            for key in (&#34;Spike&#34;, &#34;LFP&#34;, &#34;Position&#34;):
                name = self._file_names_dict[key].pop(i)
                if (key is &#34;Spike&#34;) and verbose:
                    print(&#34;Removed {} with {} units&#34;.format(
                        os.path.basename(name[0]), unit_count))
            if self._unit_count.pop(i) != unit_count:
                print(&#34;Error in remove recording {}&#34;.format(name))
            self._last_data_pt = (1, None)
            self._units.pop(i)
            if not self._load_on_fly:
                self._container.pop(i)
    end_size = self.get_num_data()
    self._count_num_units()
    end_total = len(self)

    print((&#34;{} tetrodes with {} units reduced to &#34;
           + &#34;{} tetrodes with {} units&#34;).format(
        start_size, start_total, end_size, end_total))</code></pre>
</details>
</dd>
<dt id="neurochat.nc_datacontainer.NDataContainer.set_units"><code class="name flex">
<span>def <span class="ident">set_units</span></span>(<span>self, units='all')</span>
</code></dt>
<dd>
<section class="desc"><p>Set the list of units for the collection.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_units(self, units=&#39;all&#39;):
    &#34;&#34;&#34;Set the list of units for the collection.&#34;&#34;&#34;
    self._units = []
    if self.get_file_dict() == {}:
        print(&#34;Error: Can&#39;t set units for empty collection&#34;)
        return
    if units == &#39;all&#39;:
        if self._load_on_fly:
            vals = self.get_file_dict()[&#34;Spike&#34;]
            for descriptor in vals:
                result = NData()
                self._load(&#34;Spike&#34;, descriptor, ndata=result)
                self._units.append(result.get_unit_list())
        else:
            for data in self.get_data():
                self._units.append(data.get_unit_list())

    elif isinstance(units, list):
        for idx, unit in enumerate(units):
            if unit == &#39;all&#39;:
                if self._load_on_fly:
                    vals = self.get_file_dict()[&#34;Spike&#34;]
                    descriptor = vals[idx]
                    result = NData()
                    self._load(&#34;Spike&#34;, descriptor, ndata=result)
                    all_units = result.get_unit_list()
                else:
                    all_units = self.get_data(idx).get_unit_list()
                self._units.append(all_units)
            elif isinstance(unit, int):
                self._units.append([unit])
            elif isinstance(unit, list):
                self._units.append(unit)
            else:
                logging.error(
                    &#34;Unrecognised type {} passed to set units&#34;.format(type(unit)))

    else:
        logging.error(
            &#34;Unrecognised type {} passed to set units&#34;.format(type(units)))
    self._unit_count = self._count_num_units()</code></pre>
</details>
</dd>
<dt id="neurochat.nc_datacontainer.NDataContainer.setup"><code class="name flex">
<span>def <span class="ident">setup</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Perform data initialisation based on the input filenames.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def setup(self):
    &#34;&#34;&#34;Perform data initialisation based on the input filenames.&#34;&#34;&#34;
    if self._load_on_fly:
        self._last_data_pt = (1, None)
    else:
        self._load_all_data()</code></pre>
</details>
</dd>
<dt id="neurochat.nc_datacontainer.NDataContainer.sort_units_spatially"><code class="name flex">
<span>def <span class="ident">sort_units_spatially</span></span>(<span>self, should_sort_list=None, mode='vertical')</span>
</code></dt>
<dd>
<section class="desc"><p>Sort the units in the collection based on the place field centroid.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>should_sort_list</code></strong> :&ensp;<code>list</code></dt>
<dd>Optional list of boolean values indicating what objects</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code></dt>
<dd>"horizontal" or "vertical", indicating what axis to sort on.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def sort_units_spatially(self, should_sort_list=None, mode=&#34;vertical&#34;):
    &#34;&#34;&#34;
    Sort the units in the collection based on the place field centroid.

    Parameters
    ----------
    should_sort_list: list
        Optional list of boolean values indicating what objects
    mode: str
        &#34;horizontal&#34; or &#34;vertical&#34;, indicating what axis to sort on.

    Returns
    -------
    None

    &#34;&#34;&#34;
    if mode == &#34;vertical&#34;:
        h = 1
    elif mode == &#34;horizontal&#34;:
        h = 0
    else:
        logging.error(
            &#34;NDataContainer: &#34;
            + &#34;Only modes horizontal and vertical are supported&#34;)

    if should_sort_list is None:
        should_sort_list = [True for _ in range(self.get_num_data())]

    for idx, bool_val in enumerate(should_sort_list):
        if bool_val:
            centroids = []
            data = self.get_data(idx)
            for unit in self.get_units()[idx]:
                data.set_unit_no(unit)
                place_info = data.place()
                centroid = place_info[&#34;centroid&#34;]
                centroids.append(centroid)
            self._units[idx] = [unit for _, unit in sorted(
                zip(centroids, self.get_units()[idx]),
                key=lambda pair: pair[0][h])]</code></pre>
</details>
</dd>
<dt id="neurochat.nc_datacontainer.NDataContainer.string_repr"><code class="name flex">
<span>def <span class="ident">string_repr</span></span>(<span>self, pretty=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Return a string representation of this class.
Parameters</p>
<hr>
<dl>
<dt><strong><code>pretty</code></strong> :&ensp;<code>str</code>, <code>Default</code> <code>True</code></dt>
<dd>Should return a pretty version or all the info.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def string_repr(self, pretty=True):
    &#34;&#34;&#34;
    Return a string representation of this class.
    Parameters
    ----------
    pretty : str, Default True
        Should return a pretty version or all the info.
    &#34;&#34;&#34;
    if pretty:
        return self._pretty_string()
    else:
        return self._full_string()</code></pre>
</details>
</dd>
<dt id="neurochat.nc_datacontainer.NDataContainer.subsample"><code class="name flex">
<span>def <span class="ident">subsample</span></span>(<span>self, key)</span>
</code></dt>
<dd>
<section class="desc"><p>Return a subsample of the original data collection.</p>
<p>This subsample is not a reference, but a deep copy</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>key</code></strong> :&ensp;<code>Slice</code> or <code>int</code></dt>
<dd>How to sample the original collection</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><a title="neurochat.nc_datacontainer.NDataContainer" href="#neurochat.nc_datacontainer.NDataContainer"><code>NDataContainer</code></a></dt>
<dd>The deep copied subsample</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def subsample(self, key):
    &#34;&#34;&#34;
    Return a subsample of the original data collection.

    This subsample is not a reference, but a deep copy

    Parameters
    ----------
    key : Slice or int
        How to sample the original collection

    Returns
    -------
    NDataContainer
        The deep copied subsample

    &#34;&#34;&#34;
    result = copy.deepcopy(self)

    for k in result._file_names_dict:
        result._file_names_dict[k] = result._file_names_dict[k][key]
        if isinstance(key, int):
            result._file_names_dict[k] = [result._file_names_dict[k]]

    if len(result._units) &gt; 0:
        result._units = result._units[key]
        if isinstance(key, int):
            result._units = [result._units]

    if len(result._container) &gt; 0:
        result._container = result._container[key]
        if isinstance(key, int):
            result._container = [result._container[key]]

    result._unit_count = result._count_num_units()
    return result</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="neurochat.nc_datacontainer.NDataContainerIterator"><code class="flex name class">
<span>class <span class="ident">NDataContainerIterator</span></span>
<span>(</span><span>container)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class NDataContainerIterator():
    def __init__(self, container):
        self._index = 0
        self._container = container

    def __next__(self):
        if self._index &lt; len(self._container):
            self._index += 1
            return self._container[self._index - 1]
        raise StopIteration</code></pre>
</details>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="neurochat" href="index.html">neurochat</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="neurochat.nc_datacontainer.NDataContainer" href="#neurochat.nc_datacontainer.NDataContainer">NDataContainer</a></code></h4>
<ul class="">
<li><code><a title="neurochat.nc_datacontainer.NDataContainer.EFileType" href="#neurochat.nc_datacontainer.NDataContainer.EFileType">EFileType</a></code></li>
<li><code><a title="neurochat.nc_datacontainer.NDataContainer.add_all_files" href="#neurochat.nc_datacontainer.NDataContainer.add_all_files">add_all_files</a></code></li>
<li><code><a title="neurochat.nc_datacontainer.NDataContainer.add_axona_files_from_dir" href="#neurochat.nc_datacontainer.NDataContainer.add_axona_files_from_dir">add_axona_files_from_dir</a></code></li>
<li><code><a title="neurochat.nc_datacontainer.NDataContainer.add_data" href="#neurochat.nc_datacontainer.NDataContainer.add_data">add_data</a></code></li>
<li><code><a title="neurochat.nc_datacontainer.NDataContainer.add_files" href="#neurochat.nc_datacontainer.NDataContainer.add_files">add_files</a></code></li>
<li><code><a title="neurochat.nc_datacontainer.NDataContainer.add_files_from_excel" href="#neurochat.nc_datacontainer.NDataContainer.add_files_from_excel">add_files_from_excel</a></code></li>
<li><code><a title="neurochat.nc_datacontainer.NDataContainer.get_data" href="#neurochat.nc_datacontainer.NDataContainer.get_data">get_data</a></code></li>
<li><code><a title="neurochat.nc_datacontainer.NDataContainer.get_data_at" href="#neurochat.nc_datacontainer.NDataContainer.get_data_at">get_data_at</a></code></li>
<li><code><a title="neurochat.nc_datacontainer.NDataContainer.get_file_dict" href="#neurochat.nc_datacontainer.NDataContainer.get_file_dict">get_file_dict</a></code></li>
<li><code><a title="neurochat.nc_datacontainer.NDataContainer.get_index_info" href="#neurochat.nc_datacontainer.NDataContainer.get_index_info">get_index_info</a></code></li>
<li><code><a title="neurochat.nc_datacontainer.NDataContainer.get_num_data" href="#neurochat.nc_datacontainer.NDataContainer.get_num_data">get_num_data</a></code></li>
<li><code><a title="neurochat.nc_datacontainer.NDataContainer.get_units" href="#neurochat.nc_datacontainer.NDataContainer.get_units">get_units</a></code></li>
<li><code><a title="neurochat.nc_datacontainer.NDataContainer.list_all_units" href="#neurochat.nc_datacontainer.NDataContainer.list_all_units">list_all_units</a></code></li>
<li><code><a title="neurochat.nc_datacontainer.NDataContainer.merge" href="#neurochat.nc_datacontainer.NDataContainer.merge">merge</a></code></li>
<li><code><a title="neurochat.nc_datacontainer.NDataContainer.remove_recordings_units" href="#neurochat.nc_datacontainer.NDataContainer.remove_recordings_units">remove_recordings_units</a></code></li>
<li><code><a title="neurochat.nc_datacontainer.NDataContainer.set_units" href="#neurochat.nc_datacontainer.NDataContainer.set_units">set_units</a></code></li>
<li><code><a title="neurochat.nc_datacontainer.NDataContainer.setup" href="#neurochat.nc_datacontainer.NDataContainer.setup">setup</a></code></li>
<li><code><a title="neurochat.nc_datacontainer.NDataContainer.sort_units_spatially" href="#neurochat.nc_datacontainer.NDataContainer.sort_units_spatially">sort_units_spatially</a></code></li>
<li><code><a title="neurochat.nc_datacontainer.NDataContainer.string_repr" href="#neurochat.nc_datacontainer.NDataContainer.string_repr">string_repr</a></code></li>
<li><code><a title="neurochat.nc_datacontainer.NDataContainer.subsample" href="#neurochat.nc_datacontainer.NDataContainer.subsample">subsample</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="neurochat.nc_datacontainer.NDataContainerIterator" href="#neurochat.nc_datacontainer.NDataContainerIterator">NDataContainerIterator</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.6.3</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>