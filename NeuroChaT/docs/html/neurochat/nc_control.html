<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.6.3" />
<title>neurochat.nc_control API documentation</title>
<meta name="description" content="This module implements NeuroChaT Class for the NeuroChaT software â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>neurochat.nc_control</code></h1>
</header>
<section id="section-intro">
<p>This module implements NeuroChaT Class for the NeuroChaT software</p>
<p>@author: Md Nurul Islam; islammn at tcd dot ie</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
&#34;&#34;&#34;
This module implements NeuroChaT Class for the NeuroChaT software

@author: Md Nurul Islam; islammn at tcd dot ie
&#34;&#34;&#34;

import os.path
import logging
import inspect
from collections import OrderedDict as oDict

import numpy as np
import pandas as pd

from PyQt5 import QtCore

from neurochat.nc_utils import NLog, angle_between_points, log_exception
from neurochat.nc_utils import remove_extension
from neurochat.nc_data import NData
from neurochat.nc_datacontainer import NDataContainer
from neurochat.nc_hdf import Nhdf
from neurochat.nc_clust import NClust
from neurochat.nc_config import Configuration
import neurochat.nc_plot as nc_plot
import neurochat.nc_containeranalysis as nca

import matplotlib.pyplot as plt
import matplotlib.figure

from matplotlib.backends.backend_pdf import PdfPages

class NeuroChaT(QtCore.QThread):
    &#34;&#34;&#34;
    The NeuroChaT object is the controller object in NeuroChaT and works as the
    backend to the NeuroChaT graphical user interface. It reads data, parameter
    and analysis specifications from the Configuration class and executes accordingly.
    It also interafces the GUI to the rest of the NeuroChaT elements.
    
    &#34;&#34;&#34;
    
    finished = QtCore.pyqtSignal()
    def __init__(self, config=Configuration(), data=NData(), parent=None):
        &#34;&#34;&#34;
        Attributes
        ----------
        ndata : NData
            NData oject
        config : Configuration
            Configuration object
        log : NLog
            Central logger object
        hdf : Nhdf
            A Nhdf object
            
        &#34;&#34;&#34;
        
        super().__init__(parent)
        self.ndata = data
        self.config = config
        self.log = NLog()
        self.hdf = Nhdf()
        self.reset()
        
    def reset(self):
        &#34;&#34;&#34;
        Reset NeuroChaT&#39;s internal attributes and prepares it for another set of
        analysis or new session.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None
        
        &#34;&#34;&#34;
        
        self.__count = 0
        self.nwb_files = []
        self.graphic_files = []
        self.cellid = []
        self.results = []
        self.save_to_file = False
        self._pdf_file = None
        
        if not self.get_graphic_format():
            self.set_graphic_format(&#39;PDF&#39;)
        nc_plot.set_backend(self.get_graphic_format())

    def get_output_files(self):
        &#34;&#34;&#34;
        Returns a DataFrame of output graphic files and HDF5 files after the completion of the analysis.
        Index are the unit IDs of the analysed units.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        op_files : pandas.DataFrame
            Column 1 contains the name of the output graphic files. Column 2 gives the
            the name of the NWB files
        
        &#34;&#34;&#34;
        
        op_files = {&#39;Graphics Files&#39;: self.graphic_files,
                    &#39;NWB Files&#39;: self.nwb_files}
        op_files = pd.DataFrame.from_dict(op_files)
        op_files.index = self.cellid
        op_files = op_files[[&#39;Graphics Files&#39;, &#39;NWB Files&#39;]]

        return op_files

    def update_results(self, _results):
        &#34;&#34;&#34;
        Updates the results with new analysis results.
        
        Parameters
        ----------
        _results : OrderedDict
            Dictionary of the new results
        
        Returns
        -------
        None
        
        &#34;&#34;&#34;
        
        self.results.append(_results.copy()) # without copy, list contains a reference to the original dictionary, and old results are replaced by the new one
        
    def get_results(self):
        &#34;&#34;&#34;
        Returns the parametric results of the analyses.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        results : OrderedDict
            Parametric results of the analysis
            
        &#34;&#34;&#34;
        try:
            keys = []
            for d in self.results:
                [keys.append(k) for k in list(d.keys()) if k not in keys]
            results = pd.DataFrame(self.results, columns=keys)
            results.index = self.cellid
        except Exception as ex:
            log_exception(
                ex, &#34;Error in getting results&#34;)
        
        return results

    def open_pdf(self, filename=None):
        &#34;&#34;&#34;
        Opens the PDF file object using PdfPages from matplotlib.backends.backend_pdf
        
        Parameters
        ----------
        filename : str
            Filename of the PDF output
        
        Returns
        -------
        None
            
        &#34;&#34;&#34;
        
        if filename is not None:
            words = filename.split(os.sep)
            directory = os.sep.join(words[:-1])
            if os.path.exists(directory):
                self._pdf_file = filename # Current PDF file being handled
                try:
                    self.pdf = PdfPages(self._pdf_file)
                    self.save_to_file = True
                except PermissionError:
                    logging.error(
                        &#34;Please close PDF with name {} before writing to it&#34;.format(
                            self._pdf_file))
                    self.save_to_file = False
                    self._pdf_file = None
            else:
                self.save_to_file = False
                self._pdf_file = None
                logging.error(&#39;Cannot create PDF, file path is invalid&#39;)
        else:
            logging.error(&#39;No valid PDf file is specified&#39;)

    def close_pdf(self):
        &#34;&#34;&#34;
        closes the PDF file object.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None
            
        &#34;&#34;&#34;
        
        if self._pdf_file is not None:
            self.pdf.close()
            logging.info(&#39;Output graphics saved to &#39;+ self._pdf_file)
        else:
            logging.warning(&#39;No PDF file for graphic output!&#39;)

    def close_fig(self, fig):
        &#34;&#34;&#34;
        Closes a matplotlib.fiure.Figure() object after saving it to the output PDF.
        A a tuple or list of such figures are provided, each of them saved and closed
        accordingly.
        
        Parameters
        ----------
        fig 
           matplotlib.fiure.Figure() or a list or tuple of them.
        
        Returns
        -------
        None
            
        &#34;&#34;&#34;
        
        if isinstance(fig, (tuple, list)):
            for f in fig:
                if isinstance(f, matplotlib.figure.Figure):
                    if self.save_to_file:
                        try:
                            self.pdf.savefig(f, dpi=400)
                        except PermissionError:
                            logging.error(&#34;Please close pdf before saving output to it&#34;)
                    plt.close(f)
                else:
                    logging.error(&#39;Invalid matplotlib.figure instance&#39;)
        elif isinstance(fig, matplotlib.figure.Figure):
            if self.save_to_file:
                try:
                    self.pdf.savefig(fig)
                except PermissionError:
                    logging.error(&#34;Please close pdf before saving output to it&#34;)
            plt.close(fig)
        else:
            logging.error(&#39;Invalid matplotlib.figure instance&#39;)

    def run(self):
        &#34;&#34;&#34;
        After calling start(), the NeuroChaT thread calls this function. It
        verifies the input specifications and calls the mode() method.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None
            
        &#34;&#34;&#34;

    
        self.reset()
        verified = True
        # Deduce the configuration
        # Same filename for spike, lfp and spatial will go for NWB
        if not any(self.get_analysis(&#39;all&#39;)):
            verified = False
            # Handle menu functions
            special_analysis = self.get_special_analysis()
            if special_analysis:
                key = special_analysis[&#34;key&#34;]
                logging.info(&#34;Starting special analysis {}&#34;.format(
                    key))
                if key == &#34;place_cell_plots&#34;:
                    self.place_cell_plots(
                        special_analysis[&#34;directory&#34;],
                        special_analysis[&#34;dpi&#34;])
                elif key == &#34;angle_calculation&#34;:
                    self.open_pdf(special_analysis[&#34;pdf_name&#34;])
                    self.angle_calculation(special_analysis[&#34;excel_file&#34;])
                    self.close_pdf()
                else:
                    logging.error(&#39;No analysis method has been selected&#39;)
        else:
            # Could take this to mode, 
            # but replication would occur for each data format
            mode_id = self.get_analysis_mode()[1]
            if (mode_id == 0 or mode_id == 1) and \
                (self.get_data_format() == &#39;Axona&#39; or self.get_data_format() == &#39;Neuralynx&#39;):
                if not os.path.isfile(self.get_spike_file()):
                    verified = False
                    logging.error(&#39;Spike file does not exist&#39;)

                if not os.path.isfile(self.get_spatial_file()):
                    logging.warning(&#39;Position file does not exist&#39;)
            elif mode_id == 2:
                if not os.path.isfile(self.get_excel_file()):
                    verified = False
                    logging.error(&#39;Excel file does not exist&#39;)

        if verified:
            self.__count = 0
            self.ndata.set_data_format(self.get_data_format())
            self.mode()
        self.finished.emit()

    def mode(self):
        &#34;&#34;&#34;
        Reads the specifications and analyzes data according to the mode that is set
        in the Configuration file. Thsi is the principle method in NeuroChaT that
        sets the input and output data files and calls the execute() method for 
        running the analyses after it sets the data and filenames to NData() object.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None
            
        &#34;&#34;&#34;
        
        info = {&#39;spat&#39;: [], &#39;spike&#39;: [], &#39;unit&#39;: [], &#39;lfp&#39;: [], &#39;nwb&#39;: [], &#39;graphics&#39;: [], &#39;cellid&#39;: []}
        mode_id = self.get_analysis_mode()[1]
        # All the cells in the same tetrode will use the same lfp channel
        if mode_id == 0 or mode_id == 1: 
            spatial_file = self.get_spatial_file()
            spike_file = self.get_spike_file()
            lfp_file = self.get_lfp_file()

            self.ndata.set_spike_file(spike_file)
            self.ndata.load_spike()

            units = [self.get_unit_no()] if mode_id == 0 else self.ndata.get_unit_list()
            if not units:
                logging.error(&#39;No unit found in analysis&#39;)
            else:
                for unit_no in units:
                    info[&#39;spat&#39;].append(spatial_file)
                    info[&#39;spike&#39;].append(spike_file)
                    info[&#39;unit&#39;].append(unit_no)
                    info[&#39;lfp&#39;].append(lfp_file)

        elif mode_id == 2:
            excel_file = self.get_excel_file()
            if os.path.exists(excel_file):
                excel_info = pd.read_excel(excel_file)
                for row in excel_info.itertuples():
                    spike_file = row[1] + os.sep + row[3]
                    unit_no = int(row[4])
                    lfp_id = str(row[5])

                    if self.get_data_format() == &#39;Axona&#39;:
                        end = &#34;&#34; if row[2][-4:] == &#34;.txt&#34; else &#34;.txt&#34;
                        spatial_file = row[1] + os.sep + row[2] + end
                        lfp_file = remove_extension(spike_file) + lfp_id

                    elif self.get_data_format() == &#39;Neuralynx&#39;:
                        spatial_file = row[1] + os.sep+ row[2]+ &#39;.nvt&#39;
                        lfp_file = row[1]+ os.sep+ lfp_id+ &#39;.ncs&#39;

                    elif self.get_data_format() == &#39;NWB&#39;:
                        # excel list: directory| hdf5 file name w/o extension| spike group| unit_no| lfp group
                        hdf_name = row[1] + os.sep+ row[2]+ &#39;.hdf5&#39;
                        spike_file = hdf_name+ &#39;/processing/Shank/&#39; + row[3]
                        spatial_file = hdf_name+ &#39;+/processing/Behavioural/Position&#39;
                        lfp_file = hdf_name+ &#39;+/processing/Neural Continuous/LFP/&#39; + lfp_id

                    info[&#39;spat&#39;].append(spatial_file)
                    info[&#39;spike&#39;].append(spike_file)
                    info[&#39;unit&#39;].append(unit_no)
                    info[&#39;lfp&#39;].append(lfp_file)

        if info[&#39;unit&#39;]:
            for i, unit_no in enumerate(info[&#39;unit&#39;]):
                logging.info(&#39;Starting a new unit...&#39;)
                self.ndata.set_spatial_file(info[&#39;spat&#39;][i])
                self.ndata.set_spike_file(info[&#39;spike&#39;][i])
                self.ndata.set_lfp_file(info[&#39;lfp&#39;][i])
                self.ndata.load()
                self.ndata.set_unit_no(info[&#39;unit&#39;][i])

                self.ndata.reset_results()

                cell_id = self.hdf.resolve_analysis_path(spike=self.ndata.spike, lfp=self.ndata.lfp)
                nwb_name = self.hdf.resolve_hdfname(data=self.ndata.spike)
                pdf_name = (
                    remove_extension(nwb_name, keep_dot=False) +
                    &#39;_&#39; + cell_id+ &#39;.&#39; + self.get_graphic_format())

                info[&#39;nwb&#39;].append(nwb_name)
                info[&#39;cellid&#39;].append(cell_id)
                info[&#39;graphics&#39;].append(pdf_name)

                self.open_pdf(pdf_name)

                fig = plt.figure()
                ax = fig.add_subplot(111)
                ax.text(0.1, 0.6, &#39;Cell ID = &#39;+ cell_id+ &#39;\n&#39;+ \
                        &#39;HDF5 file = &#39;+ nwb_name.split(os.sep)[-1]+ &#39;\n&#39;+ \
                        &#39;Graphics file = &#39;+ pdf_name.split(os.sep)[-1], \
                        horizontalalignment=&#39;left&#39;, \
                        verticalalignment=&#39;center&#39;,\
                        transform=ax.transAxes,
                        clip_on=True)
                ax.set_axis_off()
                self.close_fig(fig)

                # Set and open hdf5 file for saving graph data within self.execute()
                self.hdf.set_filename(nwb_name)
                if &#39;/analysis/&#39;+ cell_id in self.hdf.f:
                    del self.hdf.f[&#39;/analysis/&#39;+ cell_id]
                self.execute(name=cell_id)

                self.close_pdf()

                _results = self.ndata.get_results()

                self.update_results(_results)
                self.hdf.save_dict_recursive(path=&#39;/analysis/&#39;+ cell_id+ &#39;/&#39;, name=&#39;results&#39;, data=_results)

                self.hdf.close()
                self.ndata.save_to_hdf5() # Saving data to hdf file

                self.__count += 1
                logging.info(&#39;Units already analyzed = &#39; + str(self.__count))

        logging.info(&#39;Total cell analyzed: &#39;+ str(self.__count))
        self.cellid = info[&#39;cellid&#39;]
        self.nwb_files = info[&#39;nwb&#39;]
        self.graphic_files = info[&#39;graphics&#39;]

    def execute(self, name=None):
        &#34;&#34;&#34;
        Checks the selection of each analyses, and executes if they are selected.
        It also exports the plot data from individual analyses to the hdf file and 
        figures to the graphics file that are set in the mode() method.
        
        Parameters
        ----------
        name : str
            Name of the unit or the unique unit ID
        
        Returns
        -------
        None
            
        &#34;&#34;&#34;
        
        try:
            logging.info(&#39;Calculating environmental border...&#39;)
            self.set_border(self.calc_border())

        except:
            logging.warning(&#39;Border calculation was not properly completed!&#39;)

        if self.get_analysis(&#39;wave_property&#39;):
            logging.info(&#39;Assessing waveform properties...&#39;)
            try:
                graph_data = self.wave_property() # gd = graph_data
                fig = nc_plot.wave_property(graph_data, [int(self.get_total_channels()/2), 2])
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/waveProperty/&#39;, graph_data=graph_data)
            except:
                logging.error(&#39;Error in assessing waveform property&#39;)

        if self.get_analysis(&#39;isi&#39;):
            # ISI analysis
            logging.info(&#39;Calculating inter-spike interval distribution...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;isi&#39;)
                graph_data = self.isi(
                    bins=int(params[&#39;isi_length&#39;]/params[&#39;isi_bin&#39;]),
                    bound=[0, params[&#39;isi_length&#39;]],
                    refractory_threshold=params[&#39;isi_refractory&#39;])
                fig = nc_plot.isi(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/isi/&#39;, graph_data=graph_data)
            except Exception as ex:
                log_exception(
                    ex, &#39;Error in assessing interspike interval distribution&#39;)

        if self.get_analysis(&#39;isi_corr&#39;):
            ##Autocorr 1000ms
            logging.info(&#39;Calculating inter-spike interval autocorrelation histogram...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;isi_corr&#39;)

                graph_data = self.isi_corr(bins=params[&#39;isi_corr_bin_long&#39;], \
                                        bound=[-params[&#39;isi_corr_len_long&#39;], params[&#39;isi_corr_len_long&#39;]])
                fig = nc_plot.isi_corr(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/isiCorrLong/&#39;, graph_data=graph_data)
                # Autocorr 10ms
                graph_data = self.isi_corr(bins=params[&#39;isi_corr_bin_short&#39;], \
                                        bound=[-params[&#39;isi_corr_len_short&#39;], params[&#39;isi_corr_len_short&#39;]])
                fig = nc_plot.isi_corr(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/isiCorrShort/&#39;, graph_data=graph_data)
            except:
                logging.error(&#39;Error in assessing ISI autocorrelation&#39;)

        if self.get_analysis(&#39;theta_cell&#39;):
            ## Theta-Index analysis
            logging.info(&#39;Estimating theta-modulation index...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;theta_cell&#39;)

                graph_data = self.theta_index(start=[params[&#39;theta_cell_freq_start&#39;], params[&#39;theta_cell_tau1_start&#39;], params[&#39;theta_cell_tau2_start&#39;]], \
                               lower=[params[&#39;theta_cell_freq_min&#39;], 0, 0], \
                               upper=[params[&#39;theta_cell_freq_max&#39;], params[&#39;theta_cell_tau1_max&#39;], params[&#39;theta_cell_tau2_max&#39;]], \
                               bins=params[&#39;isi_corr_bin_long&#39;], \
                               bound=[-params[&#39;isi_corr_len_long&#39;], params[&#39;isi_corr_len_long&#39;]])
                fig = nc_plot.theta_cell(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/theta_cell/&#39;, graph_data=graph_data)
            except:
                logging.error(&#39;Error in theta-index analysis&#39;)

        if self.get_analysis(&#39;theta_skip_cell&#39;):
            logging.info(&#39;Estimating theta-skipping index...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;theta_cell&#39;)

                graph_data = self.theta_skip_index(start=[params[&#39;theta_cell_freq_start&#39;], params[&#39;theta_cell_tau1_start&#39;], params[&#39;theta_cell_tau2_start&#39;]], \
                               lower=[params[&#39;theta_cell_freq_min&#39;], 0, 0], \
                               upper=[params[&#39;theta_cell_freq_max&#39;], params[&#39;theta_cell_tau1_max&#39;], params[&#39;theta_cell_tau2_max&#39;]], \
                               bins=params[&#39;isi_corr_bin_long&#39;], \
                               bound=[-params[&#39;isi_corr_len_long&#39;], params[&#39;isi_corr_len_long&#39;]])
                fig = nc_plot.theta_cell(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/theta_skip_cell/&#39;, graph_data=graph_data)
            except:
                logging.error(&#39;Error in theta-skipping cell index analysis&#39;)

        if self.get_analysis(&#39;burst&#39;):
            ### Burst analysis
            logging.info(&#39;Analyzing bursting property...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;burst&#39;)

                self.burst(burst_thresh=params[&#39;burst_thresh&#39;],\
                           ibi_thresh=params[&#39;ibi_thresh&#39;])
            except:
                logging.error(&#39;Error in analysing bursting property&#39;)

        if self.get_analysis(&#39;speed&#39;):
            ## Speed analysis
            logging.info(&#39;Calculating spike-rate vs running speed...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;speed&#39;)

                graph_data = self.speed(range=[params[&#39;speed_min&#39;], params[&#39;speed_max&#39;]], \
                                      binsize=params[&#39;speed_bin&#39;], update=True)
                fig = nc_plot.speed(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/speed/&#39;, graph_data=graph_data)
            except:
                logging.error(&#39;Error in analysis of spike rate vs speed&#39;)

        if self.get_analysis(&#39;ang_vel&#39;):
            ## Angular velocity analysis
            logging.info(&#39;Calculating spike-rate vs angular head velocity...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;ang_vel&#39;)

                graph_data = self.angular_velocity(range=[params[&#39;ang_vel_min&#39;], params[&#39;ang_vel_max&#39;]], \
                                    binsize=params[&#39;ang_vel_bin&#39;], cutoff=params[&#39;ang_vel_cutoff&#39;], update=True)
                fig = nc_plot.angular_velocity(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/ang_vel/&#39;, graph_data=graph_data)
            except:
                logging.error(&#39;Error in analysis of spike rate vs angular velocity&#39;)

        if self.get_analysis(&#39;hd_rate&#39;):
            logging.info(&#39;Assessing head-directional tuning...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;hd_rate&#39;)

                hdData = self.hd_rate(binsize=params[&#39;hd_bin&#39;], \
                        filter=[&#39;b&#39;, params[&#39;hd_rate_kern_len&#39;]],\
                        pixel=params[&#39;loc_pixel_size&#39;], update=True)
                fig = nc_plot.hd_firing(hdData)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/hd_rate/&#39;, graph_data=hdData)

                hdData = self.hd_rate_ccw(binsize=params[&#39;hd_bin&#39;], \
                        filter=[&#39;b&#39;, params[&#39;hd_rate_kern_len&#39;]],\
                        thresh=params[&#39;hd_ang_vel_cutoff&#39;],\
                        pixel=params[&#39;loc_pixel_size&#39;], update=True)
                fig = nc_plot.hd_rate_ccw(hdData)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/hd_rate_CCW/&#39;, graph_data=hdData)

            except:
                logging.error(&#39;Error in analysis of spike rate vs head direction&#39;)

        if self.get_analysis(&#39;hd_shuffle&#39;):
            logging.info(&#39;Shuffling analysis of head-directional tuning...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;hd_shuffle&#39;)

                graph_data = self.hd_shuffle(bins=params[&#39;hd_shuffle_bins&#39;], \
                                          nshuff=params[&#39;hd_shuffle_total&#39;], limit=params[&#39;hd_shuffle_limit&#39;])
                fig = nc_plot.hd_shuffle(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/hd_shuffle/&#39;, graph_data=graph_data)
            except:
                logging.error(&#39;Error in head directional shuffling analysis&#39;)

        if self.get_analysis(&#39;hd_time_lapse&#39;):
            logging.info(&#39;Time-lapsed head-directional tuning...&#39;)
            try:
                graph_data = self.hd_time_lapse()

                fig = nc_plot.hd_spike_time_lapse(graph_data)
                self.close_fig(fig)

                fig = nc_plot.hd_rate_time_lapse(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/hd_time_lapse/&#39;, graph_data=graph_data)

            except:
                logging.error(&#39;Error in locational time-lapse analysis&#39;)

        if self.get_analysis(&#39;hd_time_shift&#39;):
            logging.info(&#39;Time-shift analysis of head-directional tuning...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;hd_time_shift&#39;)

                hdData = self.hd_shift(shift_ind=np.arange(params[&#39;hd_shift_min&#39;], \
                                                        params[&#39;hd_shift_max&#39;]+ params[&#39;hd_shift_step&#39;], \
                                                        params[&#39;hd_shift_step&#39;]))
                fig = nc_plot.hd_time_shift(hdData)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/hd_time_shift/&#39;, graph_data=hdData)
            except:
                logging.error(&#39;Error in head directional time-shift analysis&#39;)

        if self.get_analysis(&#39;loc_rate&#39;):
            logging.info(&#39;Assessing of locational tuning...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;loc_rate&#39;)

                if params[&#39;loc_rate_filter&#39;] == &#39;Gaussian&#39;:
                    filttype = &#39;g&#39;
                else:
                    filttype = &#39;b&#39;

                place_data = self.ndata.place(
                              pixel=params[&#39;loc_pixel_size&#39;],
                              chop_bound=params[&#39;loc_chop_bound&#39;],
                              filter=[filttype, params[&#39;loc_rate_kern_len&#39;]],
                              fieldThresh=params[&#39;loc_field_thresh&#39;],
                              smoothPlace=params[&#39;loc_field_smooth&#39;],
                              brAdjust=True, update=True)
                fig1 = nc_plot.loc_firing(place_data)
                self.close_fig(fig1)
                fig2 = nc_plot.loc_firing_and_place(place_data)
                self.close_fig(fig2)
                self.plot_data_to_hdf(name=name+ &#39;/loc_rate/&#39;, graph_data=place_data)

            except:
                logging.error(&#39;Error in analysis of locational firing rate&#39;)

        if self.get_analysis(&#39;loc_shuffle&#39;):
            logging.info(&#39;Shuffling analysis of locational tuning...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;loc_shuffle&#39;)

                if params[&#39;loc_rate_filter&#39;] == &#39;Gaussian&#39;:
                    filttype = &#39;g&#39;
                else:
                    filttype = &#39;b&#39;

                place_data = self.loc_shuffle(bins=params[&#39;loc_shuffle_nbins&#39;], \
                                          nshuff=params[&#39;loc_shuffle_total&#39;], \
                                          limit=params[&#39;loc_shuffle_limit&#39;], \
                                          pixel=params[&#39;loc_pixel_size&#39;], \
                                          chop_bound=params[&#39;loc_chop_bound&#39;], \
                                          filter=[filttype, params[&#39;loc_rate_kern_len&#39;]],\
                                          brAdjust=True, update=False)
                fig = nc_plot.loc_shuffle(place_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/loc_shuffle/&#39;, graph_data=place_data)
            except:
                logging.error(&#39;Error in locational shiffling analysis&#39;)

        if self.get_analysis(&#39;loc_time_lapse&#39;):
            logging.info(&#39;Time-lapse analysis of locational tuning...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;loc_time_lapse&#39;)

                if params[&#39;loc_rate_filter&#39;] == &#39;Gaussian&#39;:
                    filttype = &#39;g&#39;
                else:
                    filttype = &#39;b&#39;

                graph_data = self.loc_time_lapse(pixel=params[&#39;loc_pixel_size&#39;], \
                              chop_bound=params[&#39;loc_chop_bound&#39;],\
                              filter=[filttype, params[&#39;loc_rate_kern_len&#39;]],\
                              brAdjust=True)

                fig = nc_plot.loc_spike_time_lapse(graph_data)
                self.close_fig(fig)

                fig = nc_plot.loc_rate_time_lapse(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/loc_time_lapse/&#39;, graph_data=graph_data)
            except:
                logging.error(&#39;Error in locational time-lapse analysis&#39;)

        if self.get_analysis(&#39;loc_time_shift&#39;):
            logging.info(&#39;Time-shift analysis of locational tuning...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;loc_time_shift&#39;)

                if params[&#39;loc_rate_filter&#39;] == &#39;Gaussian&#39;:
                    filttype = &#39;g&#39;
                else:
                    filttype = &#39;b&#39;

                plot_data = self.loc_shift(shift_ind=np.arange(params[&#39;loc_shift_min&#39;], \
                                        params[&#39;loc_shift_max&#39;]+ params[&#39;loc_shift_step&#39;], \
                                        params[&#39;loc_shift_step&#39;]), \
                                        pixel=params[&#39;loc_pixel_size&#39;], \
                                        chop_bound=params[&#39;loc_chop_bound&#39;], \
                                        filter=[filttype, params[&#39;loc_rate_kern_len&#39;]],\
                                        brAdjust=True, update=False)
                fig = nc_plot.loc_time_shift(plot_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/loc_time_shift/&#39;, graph_data=plot_data)
            except:
                logging.error(&#39;Error in locational time-shift analysis&#39;)

        if self.get_analysis(&#39;spatial_corr&#39;):
            logging.info(&#39;Spatial and rotational correlation of locational tuning...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;spatial_corr&#39;)

                if params[&#39;spatial_corr_filter&#39;] == &#39;Gaussian&#39;:
                    filttype = &#39;g&#39;
                else:
                    filttype = &#39;b&#39;

                plot_data = self.loc_auto_corr(pixel=params[&#39;loc_pixel_size&#39;], \
                              chop_bound=params[&#39;loc_chop_bound&#39;],\
                              filter=[filttype, params[&#39;spatial_corr_kern_len&#39;]],\
                              minPixel=params[&#39;spatial_corr_min_obs&#39;], brAdjust=True)
                fig = nc_plot.loc_auto_corr(plot_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/spatial_corr/&#39;, graph_data=plot_data)

                plot_data = self.loc_rot_corr(binsize=params[&#39;rot_corr_bin&#39;], \
                                          pixel=params[&#39;loc_pixel_size&#39;], \
                                          chop_bound=params[&#39;loc_chop_bound&#39;],\
                                          filter=[filttype, params[&#39;spatial_corr_kern_len&#39;]],\
                                          minPixel=params[&#39;spatial_corr_min_obs&#39;], brAdjust=True)
                fig = nc_plot.rot_corr(plot_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/spatial_corr/&#39;, graph_data=plot_data)

            except:
                logging.error(&#39;Error in assessing spatial autocorrelation&#39;)

        if self.get_analysis(&#39;grid&#39;):
            logging.info(&#39;Assessing gridness...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;grid&#39;)

                if params[&#39;spatial_corr_filter&#39;] == &#39;Gaussian&#39;:
                    filttype = &#39;g&#39;
                else:
                    filttype = &#39;b&#39;

                graph_data = self.grid(angtol=params[&#39;grid_ang_tol&#39;],\
                                     binsize=params[&#39;grid_ang_bin&#39;], \
                                     pixel=params[&#39;loc_pixel_size&#39;], \
                                     chop_bound=params[&#39;loc_chop_bound&#39;],\
                                     filter=[filttype, params[&#39;spatial_corr_kern_len&#39;]],\
                                     minPixel=params[&#39;spatial_corr_min_obs&#39;], \
                                     brAdjust=True) # Add other paramaters
                fig = nc_plot.grid(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/grid/&#39;, graph_data=graph_data)

            except:
                logging.error(&#39;Error in grid cell analysis&#39;)

        if self.get_analysis(&#39;border&#39;):
            logging.info(&#39;Estimating tuning to border...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;border&#39;)

                if params[&#39;loc_rate_filter&#39;] == &#39;Gaussian&#39;:
                    filttype = &#39;g&#39;
                else:
                    filttype = &#39;b&#39;

                graph_data = self.border(update=True, thresh=params[&#39;border_firing_thresh&#39;], \
                                       cbinsize=params[&#39;border_ang_bin&#39;], \
                                       nstep=params[&#39;border_stair_steps&#39;], \
                                       pixel=params[&#39;loc_pixel_size&#39;], \
                                       chop_bound=params[&#39;loc_chop_bound&#39;],\
                                       filter=[filttype, params[&#39;loc_rate_kern_len&#39;]],\
                                       brAdjust=True)

                fig = nc_plot.border(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/border/&#39;, graph_data=graph_data)
            except:
                logging.error(&#39;Error in border cell analysis&#39;)

        if self.get_analysis(&#39;gradient&#39;):
            logging.info(&#39;Calculating gradient-cell properties...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;gradient&#39;)

                if params[&#39;loc_rate_filter&#39;] == &#39;Gaussian&#39;:
                    filttype = &#39;g&#39;
                else:
                    filttype = &#39;b&#39;

                graph_data = self.gradient(alim=params[&#39;grad_asymp_lim&#39;], \
                                         blim=params[&#39;grad_displace_lim&#39;], \
                                         clim=params[&#39;grad_growth_rate_lim&#39;], \
                                         pixel=params[&#39;loc_pixel_size&#39;], \
                                         chop_bound=params[&#39;loc_chop_bound&#39;],\
                                         filter=[filttype, params[&#39;loc_rate_kern_len&#39;]],\
                                         brAdjust=True)
                fig = nc_plot.gradient(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/gradient/&#39;, graph_data=graph_data)
            except:
                logging.error(&#39;Error in gradient cell analysis&#39;)

        if self.get_analysis(&#39;multiple_regression&#39;):
            logging.info(&#39;Multiple-regression analysis...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;multiple_regression&#39;)

                graph_data = self.multiple_regression(nrep=params[&#39;mra_nrep&#39;], \
                                    episode=params[&#39;mra_episode&#39;], \
                                    subsampInterv=params[&#39;mra_interval&#39;])

                fig = nc_plot.multiple_regression(graph_data)
                self.close_fig(fig)                
                self.plot_data_to_hdf(name=name+ &#39;/multiple_regression/&#39;, graph_data=graph_data)
            except Exception as ex:
                log_exception(
                    ex, &#34;in multiple-regression analysis&#34;)

        if self.get_analysis(&#39;inter_depend&#39;):
            # No plot
            logging.info(&#39;Assessing dependence of variables to...&#39;)
            try:
                self.interdependence(pixel=3, hdbinsize=5, spbinsize=1, sprange=[0, 40], \
                                    abinsize=10, angvelrange=[-500, 500])
            except:
                logging.error(&#39;Error in interdependence analysis&#39;)

        if self.get_analysis(&#39;lfp_spectrum&#39;):
            try:
                params= self.get_params_by_analysis(&#39;lfp_spectrum&#39;)

                graph_data = self.spectrum(window=params[&#39;lfp_pwelch_seg_size&#39;],\
                                         noverlap=params[&#39;lfp_pwelch_overlap&#39;], \
                                         nfft=params[&#39;lfp_pwelch_nfft&#39;],\
                                         ptype=&#39;psd&#39;, prefilt=True, \
                                         filtset=[params[&#39;lfp_prefilt_order&#39;], \
                                                   params[&#39;lfp_prefilt_lowcut&#39;], \
                                                   params[&#39;lfp_prefilt_highcut&#39;], &#39;bandpass&#39;], \
                                         fmax=params[&#39;lfp_pwelch_freq_max&#39;],\
                                         db=False, tr=False)
                fig = nc_plot.lfp_spectrum(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/lfp_spectrum/&#39;, graph_data=graph_data)

                graph_data = self.spectrum(window=params[&#39;lfp_stft_seg_size&#39;],\
                                         noverlap=params[&#39;lfp_stft_overlap&#39;],\
                                         nfft=params[&#39;lfp_stft_nfft&#39;],\
                                         ptype=&#39;psd&#39;, prefilt=True,\
                                         filtset=[params[&#39;lfp_prefilt_order&#39;], \
                                                   params[&#39;lfp_prefilt_lowcut&#39;], \
                                                   params[&#39;lfp_prefilt_highcut&#39;], &#39;bandpass&#39;], \
                                         fmax=params[&#39;lfp_stft_freq_max&#39;],\
                                         db=True, tr=True)
                fig = nc_plot.lfp_spectrum_tr(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/lfp_spectrum_TR/&#39;, graph_data=graph_data)
                
                # These ranges are from Muessig et al. 2019
                # Coordinated Emergence of Hippocampal Replay and
                # Theta Sequences during Post - natal Development
                self.bandpower_ratio(
                    [5, 11], [1.5, 4], 1.6, band_total=True,
                    first_name=&#34;Theta&#34;, second_name=&#34;Delta&#34;)
            except:
                logging.error(&#39;Error in analyzing lfp spectrum&#39;)

        if self.get_analysis(&#39;spike_phase&#39;):
            ### Analysis of Phase distribution
            logging.info(&#39;Analysing distribution of spike-phase in lfp...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;spike_phase&#39;)

                graph_data = self.phase_dist(binsize=params[&#39;phase_bin&#39;], \
                                           rbinsize=params[&#39;phase_raster_bin&#39;],\
                                           fwin=[params[&#39;phase_freq_min&#39;], params[&#39;phase_freq_max&#39;]],\
                                           pratio=params[&#39;phase_power_thresh&#39;],\
                                           aratio=params[&#39;phase_amp_thresh&#39;],
                                           filtset=[params[&#39;lfp_prefilt_order&#39;],
                                                    params[&#39;lfp_prefilt_lowcut&#39;],
                                                    params[&#39;lfp_prefilt_highcut&#39;], &#39;bandpass&#39;])
                fig = nc_plot.spike_phase(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/spike_phase/&#39;, graph_data=graph_data)

            except:
                logging.error(&#39;Error in assessing spike-phase distribution&#39;)

        if self.get_analysis(&#39;phase_lock&#39;):
            # PLV with mode = None (all events or spikes)
            logging.info(&#39;Analysis of Phase-locking value and spike-filed coherence...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;phase_lock&#39;)

                reparam = {&#39;window&#39; : [params[&#39;phase_loc_win_low&#39;], params[&#39;phase_loc_win_up&#39;]],
                           &#39;nfft&#39;: params[&#39;phase_loc_nfft&#39;],
                           &#39;fwin&#39;: [2, params[&#39;phase_loc_freq_max&#39;]],
                           &#39;nsample&#39;: 2000,
                           &#39;slide&#39;: 25,
                           &#39;nrep&#39;: 500,
                           &#39;mode&#39;: &#39;tr&#39;}

                graph_data = self.plv(**reparam)
                fig = nc_plot.plv_tr(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/phase_lock_TR/&#39;, graph_data=graph_data)

                reparam.update({&#39;mode&#39;: &#39;bs&#39;, &#39;nsample&#39;: 100})
                graph_data = self.plv(**reparam)
                fig = nc_plot.plv_bs(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/phase_lock_BS/&#39;, graph_data=graph_data)

                reparam.update({&#39;mode&#39;: None})
                graph_data = self.plv(**reparam)
                fig = nc_plot.plv(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/phase_lock/&#39;, graph_data=graph_data)
            except:
                logging.error(&#39;Error in spike-phase locking analysis&#39;)

            if self.get_analysis(&#39;lfp_spike_causality&#39;):
                logging.warning(&#39;Unit-LFP analysis has not been implemented yet!&#39;)

    def open_hdf_file(self, filename=None):
        &#34;&#34;&#34;
        Sets the filename and opens the file object for the HDF5 file.
        
        Parameters
        ----------
        filename : str
            Filename of the HDF5 object
        
        Returns
        -------
        None
            
        &#34;&#34;&#34;
        if not filename:
            filename = self.config.get_nwb_file()
            
        self.hdf.set_filename(filename=filename)
    
    def close_hdf_file(self):
        &#34;&#34;&#34;
        Closes the HDF5 file object.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None
            
        &#34;&#34;&#34;
        
        self.hdf.close()
        
    def get_hdf_groups(self, path=&#39;&#39;):
        &#34;&#34;&#34;
        Returns the names of groups or datasets in a path 
        
        Parameters
        ----------
        path : str
            path to HDF5 file group
        
        Returns
        -------
        list
            Names of the groups or datasets in the path

        &#34;&#34;&#34;
        
        return self.hdf.get_groups_in_path(path=path)
    
    def exist_hdf_path(self, path=&#39;&#39;):
        &#34;&#34;&#34;
        Check and returns if an HDF5 file path exists
        
        Parameters
        ----------
        path : str
            path to HDF5 file group
        
        Returns
        -------
        exists : bool
            True if the path exists

        &#34;&#34;&#34;
        
        exists= False
        if path in self.f:
            exists = True
            return exists
        
    def plot_data_to_hdf(self, name=None, graph_data=None):
        &#34;&#34;&#34;
        Stores plot data to the HDF5 file in the &#39;/analysis/&#39; path
        
        Parameters
        ----------
        name : str
            Unit ID which is also the name of the group in the  &#39;/analysis/&#39; path
        graph_data : dict
            Dictionary of data that are plotted
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        self.hdf.save_dict_recursive(path=&#39;/analysis/&#39;, \
             name=name, data=graph_data)

    def set_neuro_data(self, ndata):
        &#34;&#34;&#34;
        Sets a new NData() object or its subclass object.
        
        Parameters
        ----------
        ndata : NData
            Object of NData class or its subclass.
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if inspect.isclass(ndata):
            ndata = ndata()
        if isinstance(ndata, NData):
            self.ndata = ndata
        else:
            logging.warning(&#39;Inappropriate NeuroData object or class&#39;)
            
    def get_neuro_data(self):
        &#34;&#34;&#34;
        Returns the NData() object from this class.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        NData
            NeuroChaT&#39;s ndata attribute

        &#34;&#34;&#34;
        return self.ndata

    def set_configuration(self, config):
        &#34;&#34;&#34;
        Sets a new Configuration() object or its subclass object.
        
        Parameters
        ----------
        config : Configuration
            Object of Configuration class or its subclass.
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if inspect.isclass(config):
            config = config()
        if isinstance(config, Configuration):
            self.config = config
        else:
            logging.warning(&#39;Inappropriate Configuration object or class&#39;)

    def get_configuration(self):
        &#34;&#34;&#34;
        Returns the Configuration() object from this class.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        Configuration
            NeuroChaT&#39;s config attribute

        &#34;&#34;&#34;
        
        return self.config
    
    # Forwarding to configuration class
    def __getattr__(self, arg):
        if hasattr(self.config, arg):
            return getattr(self.config, arg)
        elif hasattr(self.ndata, arg):
            return getattr(self.ndata, arg)
        else:
            logging.warning(&#39;No &#39;+ arg+ &#39; method or attribute in NeuroChaT class&#39;)

    def convert_to_nwb(self, excel_file=None):
        &#34;&#34;&#34;
        Takes a list of datasets in Excel file and converts them into NWB file format. This 
        method currently supports Axona and Neuralynx data formats.
        
        Parameters
        ----------
        excel_file : str
            Name of the excel file that contains data specifications
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if self.get_data_format() == &#39;NWB&#39;:
            logging.error(&#39;NWB files do not need to be converted! Check file format option again!&#39;)
        info = {&#39;spat&#39;: [], &#39;spike&#39;: [], &#39;lfp&#39;: []}
        export_info = oDict({&#39;dir&#39;: [], &#39;nwb&#39;: [], &#39;spike&#39;: [], &#39;lfp&#39;: []})
        if os.path.exists(excel_file):
            excel_info = pd.read_excel(excel_file)
            for row in excel_info.itertuples():
                spike_file = row[1]+ os.sep+ row[3]
                lfp_id = row[4]

                if self.get_data_format() == &#39;Axona&#39;:
                    spatial_file = row[1]+ os.sep+ row[2]+ &#39;.txt&#39;
                    lfp_file = remove_extension(spike_file) + lfp_id

                elif self.get_data_format() == &#39;Neuralynx&#39;:
                    spatial_file = row[1] + os.sep+ row[2]+ &#39;.nvt&#39;
                    lfp_file = row[1]+ os.sep+ lfp_id+ &#39;.ncs&#39;

                info[&#39;spat&#39;].append(spatial_file)
                info[&#39;spike&#39;].append(spike_file)
                info[&#39;lfp&#39;].append(lfp_file)

        if info[&#39;spike&#39;]:
            for i, spike_file in enumerate(info[&#39;spike&#39;]):

                logging.info(&#39;Converting file groups: &#39;+ str(i+ 1))
                self.ndata.set_spatial_file(info[&#39;spat&#39;][i])
                self.ndata.set_spike_file(info[&#39;spike&#39;][i])
                self.ndata.set_lfp_file(info[&#39;lfp&#39;][i])
                self.ndata.load()
                self.ndata.save_to_hdf5()

                f_name = self.hdf.resolve_hdfname(data=self.ndata.spike)
                export_info[&#39;dir&#39;].append(os.sep.join(f_name.split(os.sep)[:-1]))
                export_info[&#39;nwb&#39;].append(f_name.split(os.sep)[-1].split(&#39;.&#39;)[0])

                export_info[&#39;spike&#39;].append(self.hdf.get_file_tag(self.ndata.spike))
                export_info[&#39;lfp&#39;].append(self.hdf.get_file_tag(self.ndata.lfp))

        export_info = pd.DataFrame(export_info, columns=[&#39;dir&#39;, &#39;nwb&#39;, &#39;spike&#39;, &#39;lfp&#39;])
        words = excel_file.split(os.sep)
        name = &#39;NWB_list_&#39; + words[-1]
        export_info.to_excel(os.path.join(os.sep.join(words[:-1]), name))
        logging.info(&#39;Conversion process completed!&#39;)

    def verify_units(self, excel_file=None):
        &#34;&#34;&#34;
        Takes a list of datasets and verify the specifications of the units.
        The verification tool is useful for prescreening of units before the 
        batch-mode analysis using &#39;Listed Units&#39; mode of NeuroChaT
        
        Parameters
        ----------
        excel_file : str
            Name of the excel file that contains data specifications
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        info = {&#39;spike&#39;: [], &#39;unit&#39;: []}
        if os.path.exists(excel_file):
            excel_info = pd.read_excel(excel_file)
            for row in excel_info.itertuples():
                spike_file = row[1]+ os.sep+ row[3]
                unit_no = int(row[4])
                if self.get_data_format() == &#39;NWB&#39;:
                # excel list: directory| spike group| unit_no
                    hdf_name = row[1] + os.sep+ row[3]+ &#39;.hdf5&#39;
                    spike_file = hdf_name+ &#39;/processing/Shank&#39;+ &#39;/&#39;+ row[4]
                info[&#39;spike&#39;].append(spike_file)
                info[&#39;unit&#39;].append(unit_no)
            n_units = excel_info.shape[0]

            excel_info = excel_info.assign(fileExists=pd.Series(np.zeros(n_units, dtype=bool)))
            excel_info = excel_info.assign(unitExists=pd.Series(np.zeros(n_units, dtype=bool)))

            if info[&#39;spike&#39;]:
                for i, spike_file in enumerate(info[&#39;spike&#39;]):

                    logging.info(&#39;Verifying unit: &#39;+ str(i+ 1))
                    if os.path.exists(spike_file):
                        excel_info.loc[i, &#39;fileExists&#39;] = True
                        self.ndata.set_spike_file(spike_file)
                        self.ndata.load_spike()
                        units = self.ndata.get_unit_list()

                        if info[&#39;unit&#39;][i] in units:
                            excel_info.loc[i, &#39;unitExists&#39;] = True

            excel_info.to_excel(excel_file, index=False)
            logging.info(&#39;Verification process completed!&#39;)
        else:
            logging.error(&#39;Excel  file does not exist!&#39;)
    
    def angle_calculation(self, excel_file=None, should_plot=True):
        &#34;&#34;&#34;
        Takes a list of unit specifications and finds the angle between the place field centroids
        The results of the analysis are written back to the input Excel file.
    
        Parameters
        ----------
        excel_file : str
            Name of the excel file that contains data specifications
    
        Returns
        -------
        None
        &#34;&#34;&#34;
        params= self.get_params_by_analysis(&#39;loc_rate&#39;)

        if params[&#39;loc_rate_filter&#39;] == &#39;Gaussian&#39;:
            filttype = &#39;g&#39;
        else:
            filttype = &#39;b&#39;

        collection = NDataContainer()
        excel_info = collection.add_files_from_excel(excel_file)
        if excel_info is None:
            return

        n_units = len(collection)
        if (n_units % 3 != 0) :
            logging.error(&#34;angle_calculation: Can&#39;t compute the angle for a number of units not divisible by 3, given &#34; + str(n_units))
            return

        excel_info = excel_info.assign(CentroidX=pd.Series(np.zeros(n_units)))
        excel_info = excel_info.assign(CentroidY=pd.Series(np.zeros(n_units)))
        excel_info = excel_info.assign(
            AngleInDegrees=pd.Series(np.zeros(n_units)))
        excel_info = excel_info.assign(
            StrongPlaceField=pd.Series(np.zeros(n_units)))
        excel_info = excel_info.assign(
            Skaggs=pd.Series(np.zeros(n_units)))
        
        centroids = []
        figs = []
        for i, data in enumerate(collection):
            place_data = data.place(
                pixel=params[&#39;loc_pixel_size&#39;],
                chop_bound=params[&#39;loc_chop_bound&#39;],
                filter=[filttype, params[&#39;loc_rate_kern_len&#39;]],
                fieldThresh=params[&#39;loc_field_thresh&#39;],
                smoothPlace=params[&#39;loc_field_smooth&#39;],
                brAdjust=True, update=True)
            centroid = place_data[&#39;centroid&#39;]
            centroids.append(centroid)
            excel_info.loc[i, &#34;CentroidX&#34;] = centroid[0]
            excel_info.loc[i, &#34;CentroidY&#34;] = centroid[1]
            _res = data.get_results()
            excel_info.loc[i, &#34;Skaggs&#34;] = _res[&#34;Spatial Skaggs&#34;]
            excel_info.loc[i, &#34;StrongPlaceField&#34;] = (
                _res[&#34;Found strong place field&#34;])
            if should_plot:
                fig = nc_plot.loc_firing_and_place(place_data)
                figs.append(fig)

            if (i + 1) % 3 == 0: #then spit out the angle
                first_centroid = centroids[0]
                second_centroid = centroids[1]
                angle = angle_between_points(
                    first_centroid, second_centroid, centroid)
                excel_info.loc[i, &#34;AngleInDegrees&#34;] = angle
                if should_plot:
                    fig = nc_plot.plot_angle_between_points(
                        centroids, 
                        place_data[&#39;xedges&#39;].max(), 
                        place_data[&#39;yedges&#39;].max())
                    figs.append(fig)
                centroids = []
        
        if should_plot:
            self.close_fig(figs)

        try:
            split_up = remove_extension(
                excel_file, keep_dot=False, return_ext=True)
            output_file = split_up[0] + &#34;_result.&#34; + split_up[1]
            excel_info.to_excel(output_file, index=False)
        except PermissionError:
            logging.warning(
                &#34;Please close the excel file to&#34; 
                + &#34; write the result back to it at&#34; 
                + &#34; {}&#34;.format(output_file))

        logging.info(&#39;Angle calculation completed! Value was {}&#39;.format(angle))

    def cluster_evaluate(self, excel_file=None):
        &#34;&#34;&#34;
        Takes a list of unit specifications and evaluates the quality of the clustering.
        The results of the analysis are written back to the input Excel file.
        
        Parameters
        ----------
        excel_file : str
            Name of the excel file that contains data specifications
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        info = {&#39;spike&#39;: [], &#39;unit&#39;: []}
        if os.path.exists(excel_file):
            excel_info = pd.read_excel(excel_file)
            for row in excel_info.itertuples():
                spike_file = row[1]+ os.sep+ row[2]
                unit_no = int(row[3])
                if self.get_data_format() == &#39;NWB&#39;:
                # excel list: directory| spike group| unit_no
                    hdf_name = row[1] + os.sep+ row[2]+ &#39;.hdf5&#39;
                    spike_file = hdf_name+ &#39;/processing/Shank&#39;+ &#39;/&#39;+ row[3]
                info[&#39;spike&#39;].append(spike_file)
                info[&#39;unit&#39;].append(unit_no)
            n_units = excel_info.shape[0]

            excel_info = excel_info.assign(BC=pd.Series(np.zeros(n_units)))
            excel_info = excel_info.assign(Dh=pd.Series(np.zeros(n_units)))

            if info[&#39;spike&#39;]:
                for i, spike_file in enumerate(info[&#39;spike&#39;]):

                    logging.info(&#39;Evaluating unit: &#39;+ str(i+ 1))
                    if os.path.exists(spike_file):
                        self.ndata.set_spike_file(spike_file)
                        self.ndata.load_spike()
                        units = self.ndata.get_unit_list()

                        if info[&#39;unit&#39;][i] in units:
                            nclust = NClust(spike=self.ndata.spike)
                            bc, dh = nclust.cluster_separation(unit_no=info[&#39;unit&#39;][i])
                            excel_info.loc[i, &#39;BC&#39;] = np.max(bc)
                            excel_info.loc[i, &#39;Dh&#39;] = np.min(dh)
            excel_info.to_excel(excel_file, index=False)
            logging.info(&#39;Cluster evaluation completed!&#39;)
        else:
            logging.error(&#39;Excel  file does not exist!&#39;)

    def cluster_similarity(self, excel_file=None):
        &#34;&#34;&#34;
        Takes a list of specifications for pairwise comparison of units. The results
        are written back to the input Excel file.
        
        Parameters
        ----------
        excel_file : str
            Name of the excel file that contains unit specifications
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        # test on Pawel&#39;s data
        nclust_1 = NClust()
        nclust_2 = NClust()
        info = {&#39;spike_1&#39;: [], &#39;unit_1&#39;: [], &#39;spike_2&#39;: [], &#39;unit_2&#39;: []}
        if os.path.exists(excel_file):
            excel_info = pd.read_excel(excel_file)
            for row in excel_info.itertuples():
                spike_file = row[1]+ os.sep+ row[2]
                unit_1 = int(row[3])
                if self.get_data_format() == &#39;NWB&#39;:
                        # excel list: directory| spike group| unit_no
                    hdf_name = row[1] + os.sep+ row[2]+ &#39;.hdf5&#39;
                    spike_file = hdf_name+ &#39;/processing/Shank&#39;+ &#39;/&#39;+ row[3]
                info[&#39;spike_1&#39;].append(spike_file)
                info[&#39;unit_1&#39;].append(unit_1)

                spike_file = row[4]+ os.sep+ row[5]
                unit_2 = int(row[6])
                if self.get_data_format() == &#39;NWB&#39;:
                        # excel list: directory| spike group| unit_no
                    hdf_name = row[4] + os.sep+ row[5]+ &#39;.hdf5&#39;
                    spike_file = hdf_name+ &#39;/processing/Shank&#39;+ &#39;/&#39;+ row[6]
                info[&#39;spike_2&#39;].append(spike_file)
                info[&#39;unit_2&#39;].append(unit_2)

            n_comparison = excel_info.shape[0]

            excel_info = excel_info.assign(BC=pd.Series(np.zeros(n_comparison)))
            excel_info = excel_info.assign(Dh=pd.Series(np.zeros(n_comparison)))

            if info[&#39;spike_1&#39;]:
                for i in np.arange(n_comparison):
                    logging.info(&#39;Evaluating unit similarity row: &#39;+ str(i+ 1))
                    if os.path.exists(info[&#39;spike_1&#39;]) and os.path.exists(info[&#39;spike_2&#39;]):
                        nclust_1.load(filename=info[&#39;spike_1&#39;], system=self.get_data_format())
                        nclust_2.load(filename=info[&#39;spike_2&#39;], system=self.get_data_format())
                        bc, dh = nclust_1.cluster_similarity(nclust=nclust_2, \
                                unit_1=info[&#39;unit_1&#39;][i], unit_2=info[&#39;unit_2&#39;][i])
                        excel_info.loc[i, &#39;BC&#39;] = bc
                        excel_info.loc[i, &#39;Dh&#39;] = dh
            excel_info.to_excel(excel_file, index=False)
            logging.info(&#39;Cluster similarity analysis completed!&#39;)
        else:
            logging.error(&#39;Excel  file does not exist!&#39;)

    def place_cell_plots(self, directory, dpi=400):
        &#34;&#34;&#34;
        Plot png images of place cell figures, looping over a directory.

        Currently only works for axona files, but can be extended.

        Parameters
        ----------
        dir : str
            The directory to get files from.
        dpi : int
            The desired dpi of the pngs.

        &#34;&#34;&#34;
        try:
            container = NDataContainer(load_on_fly=True)
            container.add_axona_files_from_dir(
                directory, tetrode_list = [i for i in range(1, 17)])
            nca.place_cell_summary(
                container, dpi=dpi, 
                filter_place_cells=False, filter_low_freq=False)
        except Exception as ex:
            log_exception(ex, &#34;In walking a directory for place cell summaries&#34;)
        return </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="neurochat.nc_control.NeuroChaT"><code class="flex name class">
<span>class <span class="ident">NeuroChaT</span></span>
<span>(</span><span>config=<neurochat.nc_config.Configuration object at 0x0000000017992A88>, data=<neurochat.nc_data.NData object at 0x000000001492D508>, parent=None)</span>
</code></dt>
<dd>
<section class="desc"><p>The NeuroChaT object is the controller object in NeuroChaT and works as the
backend to the NeuroChaT graphical user interface. It reads data, parameter
and analysis specifications from the Configuration class and executes accordingly.
It also interafces the GUI to the rest of the NeuroChaT elements.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>ndata</code></strong> :&ensp;<code>NData</code></dt>
<dd>NData oject</dd>
<dt><strong><code>config</code></strong> :&ensp;<code>Configuration</code></dt>
<dd>Configuration object</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>NLog</code></dt>
<dd>Central logger object</dd>
<dt><strong><code>hdf</code></strong> :&ensp;<code>Nhdf</code></dt>
<dd>A Nhdf object</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class NeuroChaT(QtCore.QThread):
    &#34;&#34;&#34;
    The NeuroChaT object is the controller object in NeuroChaT and works as the
    backend to the NeuroChaT graphical user interface. It reads data, parameter
    and analysis specifications from the Configuration class and executes accordingly.
    It also interafces the GUI to the rest of the NeuroChaT elements.
    
    &#34;&#34;&#34;
    
    finished = QtCore.pyqtSignal()
    def __init__(self, config=Configuration(), data=NData(), parent=None):
        &#34;&#34;&#34;
        Attributes
        ----------
        ndata : NData
            NData oject
        config : Configuration
            Configuration object
        log : NLog
            Central logger object
        hdf : Nhdf
            A Nhdf object
            
        &#34;&#34;&#34;
        
        super().__init__(parent)
        self.ndata = data
        self.config = config
        self.log = NLog()
        self.hdf = Nhdf()
        self.reset()
        
    def reset(self):
        &#34;&#34;&#34;
        Reset NeuroChaT&#39;s internal attributes and prepares it for another set of
        analysis or new session.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None
        
        &#34;&#34;&#34;
        
        self.__count = 0
        self.nwb_files = []
        self.graphic_files = []
        self.cellid = []
        self.results = []
        self.save_to_file = False
        self._pdf_file = None
        
        if not self.get_graphic_format():
            self.set_graphic_format(&#39;PDF&#39;)
        nc_plot.set_backend(self.get_graphic_format())

    def get_output_files(self):
        &#34;&#34;&#34;
        Returns a DataFrame of output graphic files and HDF5 files after the completion of the analysis.
        Index are the unit IDs of the analysed units.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        op_files : pandas.DataFrame
            Column 1 contains the name of the output graphic files. Column 2 gives the
            the name of the NWB files
        
        &#34;&#34;&#34;
        
        op_files = {&#39;Graphics Files&#39;: self.graphic_files,
                    &#39;NWB Files&#39;: self.nwb_files}
        op_files = pd.DataFrame.from_dict(op_files)
        op_files.index = self.cellid
        op_files = op_files[[&#39;Graphics Files&#39;, &#39;NWB Files&#39;]]

        return op_files

    def update_results(self, _results):
        &#34;&#34;&#34;
        Updates the results with new analysis results.
        
        Parameters
        ----------
        _results : OrderedDict
            Dictionary of the new results
        
        Returns
        -------
        None
        
        &#34;&#34;&#34;
        
        self.results.append(_results.copy()) # without copy, list contains a reference to the original dictionary, and old results are replaced by the new one
        
    def get_results(self):
        &#34;&#34;&#34;
        Returns the parametric results of the analyses.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        results : OrderedDict
            Parametric results of the analysis
            
        &#34;&#34;&#34;
        try:
            keys = []
            for d in self.results:
                [keys.append(k) for k in list(d.keys()) if k not in keys]
            results = pd.DataFrame(self.results, columns=keys)
            results.index = self.cellid
        except Exception as ex:
            log_exception(
                ex, &#34;Error in getting results&#34;)
        
        return results

    def open_pdf(self, filename=None):
        &#34;&#34;&#34;
        Opens the PDF file object using PdfPages from matplotlib.backends.backend_pdf
        
        Parameters
        ----------
        filename : str
            Filename of the PDF output
        
        Returns
        -------
        None
            
        &#34;&#34;&#34;
        
        if filename is not None:
            words = filename.split(os.sep)
            directory = os.sep.join(words[:-1])
            if os.path.exists(directory):
                self._pdf_file = filename # Current PDF file being handled
                try:
                    self.pdf = PdfPages(self._pdf_file)
                    self.save_to_file = True
                except PermissionError:
                    logging.error(
                        &#34;Please close PDF with name {} before writing to it&#34;.format(
                            self._pdf_file))
                    self.save_to_file = False
                    self._pdf_file = None
            else:
                self.save_to_file = False
                self._pdf_file = None
                logging.error(&#39;Cannot create PDF, file path is invalid&#39;)
        else:
            logging.error(&#39;No valid PDf file is specified&#39;)

    def close_pdf(self):
        &#34;&#34;&#34;
        closes the PDF file object.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None
            
        &#34;&#34;&#34;
        
        if self._pdf_file is not None:
            self.pdf.close()
            logging.info(&#39;Output graphics saved to &#39;+ self._pdf_file)
        else:
            logging.warning(&#39;No PDF file for graphic output!&#39;)

    def close_fig(self, fig):
        &#34;&#34;&#34;
        Closes a matplotlib.fiure.Figure() object after saving it to the output PDF.
        A a tuple or list of such figures are provided, each of them saved and closed
        accordingly.
        
        Parameters
        ----------
        fig 
           matplotlib.fiure.Figure() or a list or tuple of them.
        
        Returns
        -------
        None
            
        &#34;&#34;&#34;
        
        if isinstance(fig, (tuple, list)):
            for f in fig:
                if isinstance(f, matplotlib.figure.Figure):
                    if self.save_to_file:
                        try:
                            self.pdf.savefig(f, dpi=400)
                        except PermissionError:
                            logging.error(&#34;Please close pdf before saving output to it&#34;)
                    plt.close(f)
                else:
                    logging.error(&#39;Invalid matplotlib.figure instance&#39;)
        elif isinstance(fig, matplotlib.figure.Figure):
            if self.save_to_file:
                try:
                    self.pdf.savefig(fig)
                except PermissionError:
                    logging.error(&#34;Please close pdf before saving output to it&#34;)
            plt.close(fig)
        else:
            logging.error(&#39;Invalid matplotlib.figure instance&#39;)

    def run(self):
        &#34;&#34;&#34;
        After calling start(), the NeuroChaT thread calls this function. It
        verifies the input specifications and calls the mode() method.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None
            
        &#34;&#34;&#34;

    
        self.reset()
        verified = True
        # Deduce the configuration
        # Same filename for spike, lfp and spatial will go for NWB
        if not any(self.get_analysis(&#39;all&#39;)):
            verified = False
            # Handle menu functions
            special_analysis = self.get_special_analysis()
            if special_analysis:
                key = special_analysis[&#34;key&#34;]
                logging.info(&#34;Starting special analysis {}&#34;.format(
                    key))
                if key == &#34;place_cell_plots&#34;:
                    self.place_cell_plots(
                        special_analysis[&#34;directory&#34;],
                        special_analysis[&#34;dpi&#34;])
                elif key == &#34;angle_calculation&#34;:
                    self.open_pdf(special_analysis[&#34;pdf_name&#34;])
                    self.angle_calculation(special_analysis[&#34;excel_file&#34;])
                    self.close_pdf()
                else:
                    logging.error(&#39;No analysis method has been selected&#39;)
        else:
            # Could take this to mode, 
            # but replication would occur for each data format
            mode_id = self.get_analysis_mode()[1]
            if (mode_id == 0 or mode_id == 1) and \
                (self.get_data_format() == &#39;Axona&#39; or self.get_data_format() == &#39;Neuralynx&#39;):
                if not os.path.isfile(self.get_spike_file()):
                    verified = False
                    logging.error(&#39;Spike file does not exist&#39;)

                if not os.path.isfile(self.get_spatial_file()):
                    logging.warning(&#39;Position file does not exist&#39;)
            elif mode_id == 2:
                if not os.path.isfile(self.get_excel_file()):
                    verified = False
                    logging.error(&#39;Excel file does not exist&#39;)

        if verified:
            self.__count = 0
            self.ndata.set_data_format(self.get_data_format())
            self.mode()
        self.finished.emit()

    def mode(self):
        &#34;&#34;&#34;
        Reads the specifications and analyzes data according to the mode that is set
        in the Configuration file. Thsi is the principle method in NeuroChaT that
        sets the input and output data files and calls the execute() method for 
        running the analyses after it sets the data and filenames to NData() object.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None
            
        &#34;&#34;&#34;
        
        info = {&#39;spat&#39;: [], &#39;spike&#39;: [], &#39;unit&#39;: [], &#39;lfp&#39;: [], &#39;nwb&#39;: [], &#39;graphics&#39;: [], &#39;cellid&#39;: []}
        mode_id = self.get_analysis_mode()[1]
        # All the cells in the same tetrode will use the same lfp channel
        if mode_id == 0 or mode_id == 1: 
            spatial_file = self.get_spatial_file()
            spike_file = self.get_spike_file()
            lfp_file = self.get_lfp_file()

            self.ndata.set_spike_file(spike_file)
            self.ndata.load_spike()

            units = [self.get_unit_no()] if mode_id == 0 else self.ndata.get_unit_list()
            if not units:
                logging.error(&#39;No unit found in analysis&#39;)
            else:
                for unit_no in units:
                    info[&#39;spat&#39;].append(spatial_file)
                    info[&#39;spike&#39;].append(spike_file)
                    info[&#39;unit&#39;].append(unit_no)
                    info[&#39;lfp&#39;].append(lfp_file)

        elif mode_id == 2:
            excel_file = self.get_excel_file()
            if os.path.exists(excel_file):
                excel_info = pd.read_excel(excel_file)
                for row in excel_info.itertuples():
                    spike_file = row[1] + os.sep + row[3]
                    unit_no = int(row[4])
                    lfp_id = str(row[5])

                    if self.get_data_format() == &#39;Axona&#39;:
                        end = &#34;&#34; if row[2][-4:] == &#34;.txt&#34; else &#34;.txt&#34;
                        spatial_file = row[1] + os.sep + row[2] + end
                        lfp_file = remove_extension(spike_file) + lfp_id

                    elif self.get_data_format() == &#39;Neuralynx&#39;:
                        spatial_file = row[1] + os.sep+ row[2]+ &#39;.nvt&#39;
                        lfp_file = row[1]+ os.sep+ lfp_id+ &#39;.ncs&#39;

                    elif self.get_data_format() == &#39;NWB&#39;:
                        # excel list: directory| hdf5 file name w/o extension| spike group| unit_no| lfp group
                        hdf_name = row[1] + os.sep+ row[2]+ &#39;.hdf5&#39;
                        spike_file = hdf_name+ &#39;/processing/Shank/&#39; + row[3]
                        spatial_file = hdf_name+ &#39;+/processing/Behavioural/Position&#39;
                        lfp_file = hdf_name+ &#39;+/processing/Neural Continuous/LFP/&#39; + lfp_id

                    info[&#39;spat&#39;].append(spatial_file)
                    info[&#39;spike&#39;].append(spike_file)
                    info[&#39;unit&#39;].append(unit_no)
                    info[&#39;lfp&#39;].append(lfp_file)

        if info[&#39;unit&#39;]:
            for i, unit_no in enumerate(info[&#39;unit&#39;]):
                logging.info(&#39;Starting a new unit...&#39;)
                self.ndata.set_spatial_file(info[&#39;spat&#39;][i])
                self.ndata.set_spike_file(info[&#39;spike&#39;][i])
                self.ndata.set_lfp_file(info[&#39;lfp&#39;][i])
                self.ndata.load()
                self.ndata.set_unit_no(info[&#39;unit&#39;][i])

                self.ndata.reset_results()

                cell_id = self.hdf.resolve_analysis_path(spike=self.ndata.spike, lfp=self.ndata.lfp)
                nwb_name = self.hdf.resolve_hdfname(data=self.ndata.spike)
                pdf_name = (
                    remove_extension(nwb_name, keep_dot=False) +
                    &#39;_&#39; + cell_id+ &#39;.&#39; + self.get_graphic_format())

                info[&#39;nwb&#39;].append(nwb_name)
                info[&#39;cellid&#39;].append(cell_id)
                info[&#39;graphics&#39;].append(pdf_name)

                self.open_pdf(pdf_name)

                fig = plt.figure()
                ax = fig.add_subplot(111)
                ax.text(0.1, 0.6, &#39;Cell ID = &#39;+ cell_id+ &#39;\n&#39;+ \
                        &#39;HDF5 file = &#39;+ nwb_name.split(os.sep)[-1]+ &#39;\n&#39;+ \
                        &#39;Graphics file = &#39;+ pdf_name.split(os.sep)[-1], \
                        horizontalalignment=&#39;left&#39;, \
                        verticalalignment=&#39;center&#39;,\
                        transform=ax.transAxes,
                        clip_on=True)
                ax.set_axis_off()
                self.close_fig(fig)

                # Set and open hdf5 file for saving graph data within self.execute()
                self.hdf.set_filename(nwb_name)
                if &#39;/analysis/&#39;+ cell_id in self.hdf.f:
                    del self.hdf.f[&#39;/analysis/&#39;+ cell_id]
                self.execute(name=cell_id)

                self.close_pdf()

                _results = self.ndata.get_results()

                self.update_results(_results)
                self.hdf.save_dict_recursive(path=&#39;/analysis/&#39;+ cell_id+ &#39;/&#39;, name=&#39;results&#39;, data=_results)

                self.hdf.close()
                self.ndata.save_to_hdf5() # Saving data to hdf file

                self.__count += 1
                logging.info(&#39;Units already analyzed = &#39; + str(self.__count))

        logging.info(&#39;Total cell analyzed: &#39;+ str(self.__count))
        self.cellid = info[&#39;cellid&#39;]
        self.nwb_files = info[&#39;nwb&#39;]
        self.graphic_files = info[&#39;graphics&#39;]

    def execute(self, name=None):
        &#34;&#34;&#34;
        Checks the selection of each analyses, and executes if they are selected.
        It also exports the plot data from individual analyses to the hdf file and 
        figures to the graphics file that are set in the mode() method.
        
        Parameters
        ----------
        name : str
            Name of the unit or the unique unit ID
        
        Returns
        -------
        None
            
        &#34;&#34;&#34;
        
        try:
            logging.info(&#39;Calculating environmental border...&#39;)
            self.set_border(self.calc_border())

        except:
            logging.warning(&#39;Border calculation was not properly completed!&#39;)

        if self.get_analysis(&#39;wave_property&#39;):
            logging.info(&#39;Assessing waveform properties...&#39;)
            try:
                graph_data = self.wave_property() # gd = graph_data
                fig = nc_plot.wave_property(graph_data, [int(self.get_total_channels()/2), 2])
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/waveProperty/&#39;, graph_data=graph_data)
            except:
                logging.error(&#39;Error in assessing waveform property&#39;)

        if self.get_analysis(&#39;isi&#39;):
            # ISI analysis
            logging.info(&#39;Calculating inter-spike interval distribution...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;isi&#39;)
                graph_data = self.isi(
                    bins=int(params[&#39;isi_length&#39;]/params[&#39;isi_bin&#39;]),
                    bound=[0, params[&#39;isi_length&#39;]],
                    refractory_threshold=params[&#39;isi_refractory&#39;])
                fig = nc_plot.isi(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/isi/&#39;, graph_data=graph_data)
            except Exception as ex:
                log_exception(
                    ex, &#39;Error in assessing interspike interval distribution&#39;)

        if self.get_analysis(&#39;isi_corr&#39;):
            ##Autocorr 1000ms
            logging.info(&#39;Calculating inter-spike interval autocorrelation histogram...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;isi_corr&#39;)

                graph_data = self.isi_corr(bins=params[&#39;isi_corr_bin_long&#39;], \
                                        bound=[-params[&#39;isi_corr_len_long&#39;], params[&#39;isi_corr_len_long&#39;]])
                fig = nc_plot.isi_corr(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/isiCorrLong/&#39;, graph_data=graph_data)
                # Autocorr 10ms
                graph_data = self.isi_corr(bins=params[&#39;isi_corr_bin_short&#39;], \
                                        bound=[-params[&#39;isi_corr_len_short&#39;], params[&#39;isi_corr_len_short&#39;]])
                fig = nc_plot.isi_corr(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/isiCorrShort/&#39;, graph_data=graph_data)
            except:
                logging.error(&#39;Error in assessing ISI autocorrelation&#39;)

        if self.get_analysis(&#39;theta_cell&#39;):
            ## Theta-Index analysis
            logging.info(&#39;Estimating theta-modulation index...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;theta_cell&#39;)

                graph_data = self.theta_index(start=[params[&#39;theta_cell_freq_start&#39;], params[&#39;theta_cell_tau1_start&#39;], params[&#39;theta_cell_tau2_start&#39;]], \
                               lower=[params[&#39;theta_cell_freq_min&#39;], 0, 0], \
                               upper=[params[&#39;theta_cell_freq_max&#39;], params[&#39;theta_cell_tau1_max&#39;], params[&#39;theta_cell_tau2_max&#39;]], \
                               bins=params[&#39;isi_corr_bin_long&#39;], \
                               bound=[-params[&#39;isi_corr_len_long&#39;], params[&#39;isi_corr_len_long&#39;]])
                fig = nc_plot.theta_cell(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/theta_cell/&#39;, graph_data=graph_data)
            except:
                logging.error(&#39;Error in theta-index analysis&#39;)

        if self.get_analysis(&#39;theta_skip_cell&#39;):
            logging.info(&#39;Estimating theta-skipping index...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;theta_cell&#39;)

                graph_data = self.theta_skip_index(start=[params[&#39;theta_cell_freq_start&#39;], params[&#39;theta_cell_tau1_start&#39;], params[&#39;theta_cell_tau2_start&#39;]], \
                               lower=[params[&#39;theta_cell_freq_min&#39;], 0, 0], \
                               upper=[params[&#39;theta_cell_freq_max&#39;], params[&#39;theta_cell_tau1_max&#39;], params[&#39;theta_cell_tau2_max&#39;]], \
                               bins=params[&#39;isi_corr_bin_long&#39;], \
                               bound=[-params[&#39;isi_corr_len_long&#39;], params[&#39;isi_corr_len_long&#39;]])
                fig = nc_plot.theta_cell(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/theta_skip_cell/&#39;, graph_data=graph_data)
            except:
                logging.error(&#39;Error in theta-skipping cell index analysis&#39;)

        if self.get_analysis(&#39;burst&#39;):
            ### Burst analysis
            logging.info(&#39;Analyzing bursting property...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;burst&#39;)

                self.burst(burst_thresh=params[&#39;burst_thresh&#39;],\
                           ibi_thresh=params[&#39;ibi_thresh&#39;])
            except:
                logging.error(&#39;Error in analysing bursting property&#39;)

        if self.get_analysis(&#39;speed&#39;):
            ## Speed analysis
            logging.info(&#39;Calculating spike-rate vs running speed...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;speed&#39;)

                graph_data = self.speed(range=[params[&#39;speed_min&#39;], params[&#39;speed_max&#39;]], \
                                      binsize=params[&#39;speed_bin&#39;], update=True)
                fig = nc_plot.speed(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/speed/&#39;, graph_data=graph_data)
            except:
                logging.error(&#39;Error in analysis of spike rate vs speed&#39;)

        if self.get_analysis(&#39;ang_vel&#39;):
            ## Angular velocity analysis
            logging.info(&#39;Calculating spike-rate vs angular head velocity...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;ang_vel&#39;)

                graph_data = self.angular_velocity(range=[params[&#39;ang_vel_min&#39;], params[&#39;ang_vel_max&#39;]], \
                                    binsize=params[&#39;ang_vel_bin&#39;], cutoff=params[&#39;ang_vel_cutoff&#39;], update=True)
                fig = nc_plot.angular_velocity(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/ang_vel/&#39;, graph_data=graph_data)
            except:
                logging.error(&#39;Error in analysis of spike rate vs angular velocity&#39;)

        if self.get_analysis(&#39;hd_rate&#39;):
            logging.info(&#39;Assessing head-directional tuning...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;hd_rate&#39;)

                hdData = self.hd_rate(binsize=params[&#39;hd_bin&#39;], \
                        filter=[&#39;b&#39;, params[&#39;hd_rate_kern_len&#39;]],\
                        pixel=params[&#39;loc_pixel_size&#39;], update=True)
                fig = nc_plot.hd_firing(hdData)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/hd_rate/&#39;, graph_data=hdData)

                hdData = self.hd_rate_ccw(binsize=params[&#39;hd_bin&#39;], \
                        filter=[&#39;b&#39;, params[&#39;hd_rate_kern_len&#39;]],\
                        thresh=params[&#39;hd_ang_vel_cutoff&#39;],\
                        pixel=params[&#39;loc_pixel_size&#39;], update=True)
                fig = nc_plot.hd_rate_ccw(hdData)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/hd_rate_CCW/&#39;, graph_data=hdData)

            except:
                logging.error(&#39;Error in analysis of spike rate vs head direction&#39;)

        if self.get_analysis(&#39;hd_shuffle&#39;):
            logging.info(&#39;Shuffling analysis of head-directional tuning...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;hd_shuffle&#39;)

                graph_data = self.hd_shuffle(bins=params[&#39;hd_shuffle_bins&#39;], \
                                          nshuff=params[&#39;hd_shuffle_total&#39;], limit=params[&#39;hd_shuffle_limit&#39;])
                fig = nc_plot.hd_shuffle(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/hd_shuffle/&#39;, graph_data=graph_data)
            except:
                logging.error(&#39;Error in head directional shuffling analysis&#39;)

        if self.get_analysis(&#39;hd_time_lapse&#39;):
            logging.info(&#39;Time-lapsed head-directional tuning...&#39;)
            try:
                graph_data = self.hd_time_lapse()

                fig = nc_plot.hd_spike_time_lapse(graph_data)
                self.close_fig(fig)

                fig = nc_plot.hd_rate_time_lapse(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/hd_time_lapse/&#39;, graph_data=graph_data)

            except:
                logging.error(&#39;Error in locational time-lapse analysis&#39;)

        if self.get_analysis(&#39;hd_time_shift&#39;):
            logging.info(&#39;Time-shift analysis of head-directional tuning...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;hd_time_shift&#39;)

                hdData = self.hd_shift(shift_ind=np.arange(params[&#39;hd_shift_min&#39;], \
                                                        params[&#39;hd_shift_max&#39;]+ params[&#39;hd_shift_step&#39;], \
                                                        params[&#39;hd_shift_step&#39;]))
                fig = nc_plot.hd_time_shift(hdData)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/hd_time_shift/&#39;, graph_data=hdData)
            except:
                logging.error(&#39;Error in head directional time-shift analysis&#39;)

        if self.get_analysis(&#39;loc_rate&#39;):
            logging.info(&#39;Assessing of locational tuning...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;loc_rate&#39;)

                if params[&#39;loc_rate_filter&#39;] == &#39;Gaussian&#39;:
                    filttype = &#39;g&#39;
                else:
                    filttype = &#39;b&#39;

                place_data = self.ndata.place(
                              pixel=params[&#39;loc_pixel_size&#39;],
                              chop_bound=params[&#39;loc_chop_bound&#39;],
                              filter=[filttype, params[&#39;loc_rate_kern_len&#39;]],
                              fieldThresh=params[&#39;loc_field_thresh&#39;],
                              smoothPlace=params[&#39;loc_field_smooth&#39;],
                              brAdjust=True, update=True)
                fig1 = nc_plot.loc_firing(place_data)
                self.close_fig(fig1)
                fig2 = nc_plot.loc_firing_and_place(place_data)
                self.close_fig(fig2)
                self.plot_data_to_hdf(name=name+ &#39;/loc_rate/&#39;, graph_data=place_data)

            except:
                logging.error(&#39;Error in analysis of locational firing rate&#39;)

        if self.get_analysis(&#39;loc_shuffle&#39;):
            logging.info(&#39;Shuffling analysis of locational tuning...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;loc_shuffle&#39;)

                if params[&#39;loc_rate_filter&#39;] == &#39;Gaussian&#39;:
                    filttype = &#39;g&#39;
                else:
                    filttype = &#39;b&#39;

                place_data = self.loc_shuffle(bins=params[&#39;loc_shuffle_nbins&#39;], \
                                          nshuff=params[&#39;loc_shuffle_total&#39;], \
                                          limit=params[&#39;loc_shuffle_limit&#39;], \
                                          pixel=params[&#39;loc_pixel_size&#39;], \
                                          chop_bound=params[&#39;loc_chop_bound&#39;], \
                                          filter=[filttype, params[&#39;loc_rate_kern_len&#39;]],\
                                          brAdjust=True, update=False)
                fig = nc_plot.loc_shuffle(place_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/loc_shuffle/&#39;, graph_data=place_data)
            except:
                logging.error(&#39;Error in locational shiffling analysis&#39;)

        if self.get_analysis(&#39;loc_time_lapse&#39;):
            logging.info(&#39;Time-lapse analysis of locational tuning...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;loc_time_lapse&#39;)

                if params[&#39;loc_rate_filter&#39;] == &#39;Gaussian&#39;:
                    filttype = &#39;g&#39;
                else:
                    filttype = &#39;b&#39;

                graph_data = self.loc_time_lapse(pixel=params[&#39;loc_pixel_size&#39;], \
                              chop_bound=params[&#39;loc_chop_bound&#39;],\
                              filter=[filttype, params[&#39;loc_rate_kern_len&#39;]],\
                              brAdjust=True)

                fig = nc_plot.loc_spike_time_lapse(graph_data)
                self.close_fig(fig)

                fig = nc_plot.loc_rate_time_lapse(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/loc_time_lapse/&#39;, graph_data=graph_data)
            except:
                logging.error(&#39;Error in locational time-lapse analysis&#39;)

        if self.get_analysis(&#39;loc_time_shift&#39;):
            logging.info(&#39;Time-shift analysis of locational tuning...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;loc_time_shift&#39;)

                if params[&#39;loc_rate_filter&#39;] == &#39;Gaussian&#39;:
                    filttype = &#39;g&#39;
                else:
                    filttype = &#39;b&#39;

                plot_data = self.loc_shift(shift_ind=np.arange(params[&#39;loc_shift_min&#39;], \
                                        params[&#39;loc_shift_max&#39;]+ params[&#39;loc_shift_step&#39;], \
                                        params[&#39;loc_shift_step&#39;]), \
                                        pixel=params[&#39;loc_pixel_size&#39;], \
                                        chop_bound=params[&#39;loc_chop_bound&#39;], \
                                        filter=[filttype, params[&#39;loc_rate_kern_len&#39;]],\
                                        brAdjust=True, update=False)
                fig = nc_plot.loc_time_shift(plot_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/loc_time_shift/&#39;, graph_data=plot_data)
            except:
                logging.error(&#39;Error in locational time-shift analysis&#39;)

        if self.get_analysis(&#39;spatial_corr&#39;):
            logging.info(&#39;Spatial and rotational correlation of locational tuning...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;spatial_corr&#39;)

                if params[&#39;spatial_corr_filter&#39;] == &#39;Gaussian&#39;:
                    filttype = &#39;g&#39;
                else:
                    filttype = &#39;b&#39;

                plot_data = self.loc_auto_corr(pixel=params[&#39;loc_pixel_size&#39;], \
                              chop_bound=params[&#39;loc_chop_bound&#39;],\
                              filter=[filttype, params[&#39;spatial_corr_kern_len&#39;]],\
                              minPixel=params[&#39;spatial_corr_min_obs&#39;], brAdjust=True)
                fig = nc_plot.loc_auto_corr(plot_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/spatial_corr/&#39;, graph_data=plot_data)

                plot_data = self.loc_rot_corr(binsize=params[&#39;rot_corr_bin&#39;], \
                                          pixel=params[&#39;loc_pixel_size&#39;], \
                                          chop_bound=params[&#39;loc_chop_bound&#39;],\
                                          filter=[filttype, params[&#39;spatial_corr_kern_len&#39;]],\
                                          minPixel=params[&#39;spatial_corr_min_obs&#39;], brAdjust=True)
                fig = nc_plot.rot_corr(plot_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/spatial_corr/&#39;, graph_data=plot_data)

            except:
                logging.error(&#39;Error in assessing spatial autocorrelation&#39;)

        if self.get_analysis(&#39;grid&#39;):
            logging.info(&#39;Assessing gridness...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;grid&#39;)

                if params[&#39;spatial_corr_filter&#39;] == &#39;Gaussian&#39;:
                    filttype = &#39;g&#39;
                else:
                    filttype = &#39;b&#39;

                graph_data = self.grid(angtol=params[&#39;grid_ang_tol&#39;],\
                                     binsize=params[&#39;grid_ang_bin&#39;], \
                                     pixel=params[&#39;loc_pixel_size&#39;], \
                                     chop_bound=params[&#39;loc_chop_bound&#39;],\
                                     filter=[filttype, params[&#39;spatial_corr_kern_len&#39;]],\
                                     minPixel=params[&#39;spatial_corr_min_obs&#39;], \
                                     brAdjust=True) # Add other paramaters
                fig = nc_plot.grid(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/grid/&#39;, graph_data=graph_data)

            except:
                logging.error(&#39;Error in grid cell analysis&#39;)

        if self.get_analysis(&#39;border&#39;):
            logging.info(&#39;Estimating tuning to border...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;border&#39;)

                if params[&#39;loc_rate_filter&#39;] == &#39;Gaussian&#39;:
                    filttype = &#39;g&#39;
                else:
                    filttype = &#39;b&#39;

                graph_data = self.border(update=True, thresh=params[&#39;border_firing_thresh&#39;], \
                                       cbinsize=params[&#39;border_ang_bin&#39;], \
                                       nstep=params[&#39;border_stair_steps&#39;], \
                                       pixel=params[&#39;loc_pixel_size&#39;], \
                                       chop_bound=params[&#39;loc_chop_bound&#39;],\
                                       filter=[filttype, params[&#39;loc_rate_kern_len&#39;]],\
                                       brAdjust=True)

                fig = nc_plot.border(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/border/&#39;, graph_data=graph_data)
            except:
                logging.error(&#39;Error in border cell analysis&#39;)

        if self.get_analysis(&#39;gradient&#39;):
            logging.info(&#39;Calculating gradient-cell properties...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;gradient&#39;)

                if params[&#39;loc_rate_filter&#39;] == &#39;Gaussian&#39;:
                    filttype = &#39;g&#39;
                else:
                    filttype = &#39;b&#39;

                graph_data = self.gradient(alim=params[&#39;grad_asymp_lim&#39;], \
                                         blim=params[&#39;grad_displace_lim&#39;], \
                                         clim=params[&#39;grad_growth_rate_lim&#39;], \
                                         pixel=params[&#39;loc_pixel_size&#39;], \
                                         chop_bound=params[&#39;loc_chop_bound&#39;],\
                                         filter=[filttype, params[&#39;loc_rate_kern_len&#39;]],\
                                         brAdjust=True)
                fig = nc_plot.gradient(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/gradient/&#39;, graph_data=graph_data)
            except:
                logging.error(&#39;Error in gradient cell analysis&#39;)

        if self.get_analysis(&#39;multiple_regression&#39;):
            logging.info(&#39;Multiple-regression analysis...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;multiple_regression&#39;)

                graph_data = self.multiple_regression(nrep=params[&#39;mra_nrep&#39;], \
                                    episode=params[&#39;mra_episode&#39;], \
                                    subsampInterv=params[&#39;mra_interval&#39;])

                fig = nc_plot.multiple_regression(graph_data)
                self.close_fig(fig)                
                self.plot_data_to_hdf(name=name+ &#39;/multiple_regression/&#39;, graph_data=graph_data)
            except Exception as ex:
                log_exception(
                    ex, &#34;in multiple-regression analysis&#34;)

        if self.get_analysis(&#39;inter_depend&#39;):
            # No plot
            logging.info(&#39;Assessing dependence of variables to...&#39;)
            try:
                self.interdependence(pixel=3, hdbinsize=5, spbinsize=1, sprange=[0, 40], \
                                    abinsize=10, angvelrange=[-500, 500])
            except:
                logging.error(&#39;Error in interdependence analysis&#39;)

        if self.get_analysis(&#39;lfp_spectrum&#39;):
            try:
                params= self.get_params_by_analysis(&#39;lfp_spectrum&#39;)

                graph_data = self.spectrum(window=params[&#39;lfp_pwelch_seg_size&#39;],\
                                         noverlap=params[&#39;lfp_pwelch_overlap&#39;], \
                                         nfft=params[&#39;lfp_pwelch_nfft&#39;],\
                                         ptype=&#39;psd&#39;, prefilt=True, \
                                         filtset=[params[&#39;lfp_prefilt_order&#39;], \
                                                   params[&#39;lfp_prefilt_lowcut&#39;], \
                                                   params[&#39;lfp_prefilt_highcut&#39;], &#39;bandpass&#39;], \
                                         fmax=params[&#39;lfp_pwelch_freq_max&#39;],\
                                         db=False, tr=False)
                fig = nc_plot.lfp_spectrum(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/lfp_spectrum/&#39;, graph_data=graph_data)

                graph_data = self.spectrum(window=params[&#39;lfp_stft_seg_size&#39;],\
                                         noverlap=params[&#39;lfp_stft_overlap&#39;],\
                                         nfft=params[&#39;lfp_stft_nfft&#39;],\
                                         ptype=&#39;psd&#39;, prefilt=True,\
                                         filtset=[params[&#39;lfp_prefilt_order&#39;], \
                                                   params[&#39;lfp_prefilt_lowcut&#39;], \
                                                   params[&#39;lfp_prefilt_highcut&#39;], &#39;bandpass&#39;], \
                                         fmax=params[&#39;lfp_stft_freq_max&#39;],\
                                         db=True, tr=True)
                fig = nc_plot.lfp_spectrum_tr(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/lfp_spectrum_TR/&#39;, graph_data=graph_data)
                
                # These ranges are from Muessig et al. 2019
                # Coordinated Emergence of Hippocampal Replay and
                # Theta Sequences during Post - natal Development
                self.bandpower_ratio(
                    [5, 11], [1.5, 4], 1.6, band_total=True,
                    first_name=&#34;Theta&#34;, second_name=&#34;Delta&#34;)
            except:
                logging.error(&#39;Error in analyzing lfp spectrum&#39;)

        if self.get_analysis(&#39;spike_phase&#39;):
            ### Analysis of Phase distribution
            logging.info(&#39;Analysing distribution of spike-phase in lfp...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;spike_phase&#39;)

                graph_data = self.phase_dist(binsize=params[&#39;phase_bin&#39;], \
                                           rbinsize=params[&#39;phase_raster_bin&#39;],\
                                           fwin=[params[&#39;phase_freq_min&#39;], params[&#39;phase_freq_max&#39;]],\
                                           pratio=params[&#39;phase_power_thresh&#39;],\
                                           aratio=params[&#39;phase_amp_thresh&#39;],
                                           filtset=[params[&#39;lfp_prefilt_order&#39;],
                                                    params[&#39;lfp_prefilt_lowcut&#39;],
                                                    params[&#39;lfp_prefilt_highcut&#39;], &#39;bandpass&#39;])
                fig = nc_plot.spike_phase(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/spike_phase/&#39;, graph_data=graph_data)

            except:
                logging.error(&#39;Error in assessing spike-phase distribution&#39;)

        if self.get_analysis(&#39;phase_lock&#39;):
            # PLV with mode = None (all events or spikes)
            logging.info(&#39;Analysis of Phase-locking value and spike-filed coherence...&#39;)
            try:
                params= self.get_params_by_analysis(&#39;phase_lock&#39;)

                reparam = {&#39;window&#39; : [params[&#39;phase_loc_win_low&#39;], params[&#39;phase_loc_win_up&#39;]],
                           &#39;nfft&#39;: params[&#39;phase_loc_nfft&#39;],
                           &#39;fwin&#39;: [2, params[&#39;phase_loc_freq_max&#39;]],
                           &#39;nsample&#39;: 2000,
                           &#39;slide&#39;: 25,
                           &#39;nrep&#39;: 500,
                           &#39;mode&#39;: &#39;tr&#39;}

                graph_data = self.plv(**reparam)
                fig = nc_plot.plv_tr(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/phase_lock_TR/&#39;, graph_data=graph_data)

                reparam.update({&#39;mode&#39;: &#39;bs&#39;, &#39;nsample&#39;: 100})
                graph_data = self.plv(**reparam)
                fig = nc_plot.plv_bs(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/phase_lock_BS/&#39;, graph_data=graph_data)

                reparam.update({&#39;mode&#39;: None})
                graph_data = self.plv(**reparam)
                fig = nc_plot.plv(graph_data)
                self.close_fig(fig)
                self.plot_data_to_hdf(name=name+ &#39;/phase_lock/&#39;, graph_data=graph_data)
            except:
                logging.error(&#39;Error in spike-phase locking analysis&#39;)

            if self.get_analysis(&#39;lfp_spike_causality&#39;):
                logging.warning(&#39;Unit-LFP analysis has not been implemented yet!&#39;)

    def open_hdf_file(self, filename=None):
        &#34;&#34;&#34;
        Sets the filename and opens the file object for the HDF5 file.
        
        Parameters
        ----------
        filename : str
            Filename of the HDF5 object
        
        Returns
        -------
        None
            
        &#34;&#34;&#34;
        if not filename:
            filename = self.config.get_nwb_file()
            
        self.hdf.set_filename(filename=filename)
    
    def close_hdf_file(self):
        &#34;&#34;&#34;
        Closes the HDF5 file object.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None
            
        &#34;&#34;&#34;
        
        self.hdf.close()
        
    def get_hdf_groups(self, path=&#39;&#39;):
        &#34;&#34;&#34;
        Returns the names of groups or datasets in a path 
        
        Parameters
        ----------
        path : str
            path to HDF5 file group
        
        Returns
        -------
        list
            Names of the groups or datasets in the path

        &#34;&#34;&#34;
        
        return self.hdf.get_groups_in_path(path=path)
    
    def exist_hdf_path(self, path=&#39;&#39;):
        &#34;&#34;&#34;
        Check and returns if an HDF5 file path exists
        
        Parameters
        ----------
        path : str
            path to HDF5 file group
        
        Returns
        -------
        exists : bool
            True if the path exists

        &#34;&#34;&#34;
        
        exists= False
        if path in self.f:
            exists = True
            return exists
        
    def plot_data_to_hdf(self, name=None, graph_data=None):
        &#34;&#34;&#34;
        Stores plot data to the HDF5 file in the &#39;/analysis/&#39; path
        
        Parameters
        ----------
        name : str
            Unit ID which is also the name of the group in the  &#39;/analysis/&#39; path
        graph_data : dict
            Dictionary of data that are plotted
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        self.hdf.save_dict_recursive(path=&#39;/analysis/&#39;, \
             name=name, data=graph_data)

    def set_neuro_data(self, ndata):
        &#34;&#34;&#34;
        Sets a new NData() object or its subclass object.
        
        Parameters
        ----------
        ndata : NData
            Object of NData class or its subclass.
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if inspect.isclass(ndata):
            ndata = ndata()
        if isinstance(ndata, NData):
            self.ndata = ndata
        else:
            logging.warning(&#39;Inappropriate NeuroData object or class&#39;)
            
    def get_neuro_data(self):
        &#34;&#34;&#34;
        Returns the NData() object from this class.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        NData
            NeuroChaT&#39;s ndata attribute

        &#34;&#34;&#34;
        return self.ndata

    def set_configuration(self, config):
        &#34;&#34;&#34;
        Sets a new Configuration() object or its subclass object.
        
        Parameters
        ----------
        config : Configuration
            Object of Configuration class or its subclass.
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if inspect.isclass(config):
            config = config()
        if isinstance(config, Configuration):
            self.config = config
        else:
            logging.warning(&#39;Inappropriate Configuration object or class&#39;)

    def get_configuration(self):
        &#34;&#34;&#34;
        Returns the Configuration() object from this class.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        Configuration
            NeuroChaT&#39;s config attribute

        &#34;&#34;&#34;
        
        return self.config
    
    # Forwarding to configuration class
    def __getattr__(self, arg):
        if hasattr(self.config, arg):
            return getattr(self.config, arg)
        elif hasattr(self.ndata, arg):
            return getattr(self.ndata, arg)
        else:
            logging.warning(&#39;No &#39;+ arg+ &#39; method or attribute in NeuroChaT class&#39;)

    def convert_to_nwb(self, excel_file=None):
        &#34;&#34;&#34;
        Takes a list of datasets in Excel file and converts them into NWB file format. This 
        method currently supports Axona and Neuralynx data formats.
        
        Parameters
        ----------
        excel_file : str
            Name of the excel file that contains data specifications
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if self.get_data_format() == &#39;NWB&#39;:
            logging.error(&#39;NWB files do not need to be converted! Check file format option again!&#39;)
        info = {&#39;spat&#39;: [], &#39;spike&#39;: [], &#39;lfp&#39;: []}
        export_info = oDict({&#39;dir&#39;: [], &#39;nwb&#39;: [], &#39;spike&#39;: [], &#39;lfp&#39;: []})
        if os.path.exists(excel_file):
            excel_info = pd.read_excel(excel_file)
            for row in excel_info.itertuples():
                spike_file = row[1]+ os.sep+ row[3]
                lfp_id = row[4]

                if self.get_data_format() == &#39;Axona&#39;:
                    spatial_file = row[1]+ os.sep+ row[2]+ &#39;.txt&#39;
                    lfp_file = remove_extension(spike_file) + lfp_id

                elif self.get_data_format() == &#39;Neuralynx&#39;:
                    spatial_file = row[1] + os.sep+ row[2]+ &#39;.nvt&#39;
                    lfp_file = row[1]+ os.sep+ lfp_id+ &#39;.ncs&#39;

                info[&#39;spat&#39;].append(spatial_file)
                info[&#39;spike&#39;].append(spike_file)
                info[&#39;lfp&#39;].append(lfp_file)

        if info[&#39;spike&#39;]:
            for i, spike_file in enumerate(info[&#39;spike&#39;]):

                logging.info(&#39;Converting file groups: &#39;+ str(i+ 1))
                self.ndata.set_spatial_file(info[&#39;spat&#39;][i])
                self.ndata.set_spike_file(info[&#39;spike&#39;][i])
                self.ndata.set_lfp_file(info[&#39;lfp&#39;][i])
                self.ndata.load()
                self.ndata.save_to_hdf5()

                f_name = self.hdf.resolve_hdfname(data=self.ndata.spike)
                export_info[&#39;dir&#39;].append(os.sep.join(f_name.split(os.sep)[:-1]))
                export_info[&#39;nwb&#39;].append(f_name.split(os.sep)[-1].split(&#39;.&#39;)[0])

                export_info[&#39;spike&#39;].append(self.hdf.get_file_tag(self.ndata.spike))
                export_info[&#39;lfp&#39;].append(self.hdf.get_file_tag(self.ndata.lfp))

        export_info = pd.DataFrame(export_info, columns=[&#39;dir&#39;, &#39;nwb&#39;, &#39;spike&#39;, &#39;lfp&#39;])
        words = excel_file.split(os.sep)
        name = &#39;NWB_list_&#39; + words[-1]
        export_info.to_excel(os.path.join(os.sep.join(words[:-1]), name))
        logging.info(&#39;Conversion process completed!&#39;)

    def verify_units(self, excel_file=None):
        &#34;&#34;&#34;
        Takes a list of datasets and verify the specifications of the units.
        The verification tool is useful for prescreening of units before the 
        batch-mode analysis using &#39;Listed Units&#39; mode of NeuroChaT
        
        Parameters
        ----------
        excel_file : str
            Name of the excel file that contains data specifications
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        info = {&#39;spike&#39;: [], &#39;unit&#39;: []}
        if os.path.exists(excel_file):
            excel_info = pd.read_excel(excel_file)
            for row in excel_info.itertuples():
                spike_file = row[1]+ os.sep+ row[3]
                unit_no = int(row[4])
                if self.get_data_format() == &#39;NWB&#39;:
                # excel list: directory| spike group| unit_no
                    hdf_name = row[1] + os.sep+ row[3]+ &#39;.hdf5&#39;
                    spike_file = hdf_name+ &#39;/processing/Shank&#39;+ &#39;/&#39;+ row[4]
                info[&#39;spike&#39;].append(spike_file)
                info[&#39;unit&#39;].append(unit_no)
            n_units = excel_info.shape[0]

            excel_info = excel_info.assign(fileExists=pd.Series(np.zeros(n_units, dtype=bool)))
            excel_info = excel_info.assign(unitExists=pd.Series(np.zeros(n_units, dtype=bool)))

            if info[&#39;spike&#39;]:
                for i, spike_file in enumerate(info[&#39;spike&#39;]):

                    logging.info(&#39;Verifying unit: &#39;+ str(i+ 1))
                    if os.path.exists(spike_file):
                        excel_info.loc[i, &#39;fileExists&#39;] = True
                        self.ndata.set_spike_file(spike_file)
                        self.ndata.load_spike()
                        units = self.ndata.get_unit_list()

                        if info[&#39;unit&#39;][i] in units:
                            excel_info.loc[i, &#39;unitExists&#39;] = True

            excel_info.to_excel(excel_file, index=False)
            logging.info(&#39;Verification process completed!&#39;)
        else:
            logging.error(&#39;Excel  file does not exist!&#39;)
    
    def angle_calculation(self, excel_file=None, should_plot=True):
        &#34;&#34;&#34;
        Takes a list of unit specifications and finds the angle between the place field centroids
        The results of the analysis are written back to the input Excel file.
    
        Parameters
        ----------
        excel_file : str
            Name of the excel file that contains data specifications
    
        Returns
        -------
        None
        &#34;&#34;&#34;
        params= self.get_params_by_analysis(&#39;loc_rate&#39;)

        if params[&#39;loc_rate_filter&#39;] == &#39;Gaussian&#39;:
            filttype = &#39;g&#39;
        else:
            filttype = &#39;b&#39;

        collection = NDataContainer()
        excel_info = collection.add_files_from_excel(excel_file)
        if excel_info is None:
            return

        n_units = len(collection)
        if (n_units % 3 != 0) :
            logging.error(&#34;angle_calculation: Can&#39;t compute the angle for a number of units not divisible by 3, given &#34; + str(n_units))
            return

        excel_info = excel_info.assign(CentroidX=pd.Series(np.zeros(n_units)))
        excel_info = excel_info.assign(CentroidY=pd.Series(np.zeros(n_units)))
        excel_info = excel_info.assign(
            AngleInDegrees=pd.Series(np.zeros(n_units)))
        excel_info = excel_info.assign(
            StrongPlaceField=pd.Series(np.zeros(n_units)))
        excel_info = excel_info.assign(
            Skaggs=pd.Series(np.zeros(n_units)))
        
        centroids = []
        figs = []
        for i, data in enumerate(collection):
            place_data = data.place(
                pixel=params[&#39;loc_pixel_size&#39;],
                chop_bound=params[&#39;loc_chop_bound&#39;],
                filter=[filttype, params[&#39;loc_rate_kern_len&#39;]],
                fieldThresh=params[&#39;loc_field_thresh&#39;],
                smoothPlace=params[&#39;loc_field_smooth&#39;],
                brAdjust=True, update=True)
            centroid = place_data[&#39;centroid&#39;]
            centroids.append(centroid)
            excel_info.loc[i, &#34;CentroidX&#34;] = centroid[0]
            excel_info.loc[i, &#34;CentroidY&#34;] = centroid[1]
            _res = data.get_results()
            excel_info.loc[i, &#34;Skaggs&#34;] = _res[&#34;Spatial Skaggs&#34;]
            excel_info.loc[i, &#34;StrongPlaceField&#34;] = (
                _res[&#34;Found strong place field&#34;])
            if should_plot:
                fig = nc_plot.loc_firing_and_place(place_data)
                figs.append(fig)

            if (i + 1) % 3 == 0: #then spit out the angle
                first_centroid = centroids[0]
                second_centroid = centroids[1]
                angle = angle_between_points(
                    first_centroid, second_centroid, centroid)
                excel_info.loc[i, &#34;AngleInDegrees&#34;] = angle
                if should_plot:
                    fig = nc_plot.plot_angle_between_points(
                        centroids, 
                        place_data[&#39;xedges&#39;].max(), 
                        place_data[&#39;yedges&#39;].max())
                    figs.append(fig)
                centroids = []
        
        if should_plot:
            self.close_fig(figs)

        try:
            split_up = remove_extension(
                excel_file, keep_dot=False, return_ext=True)
            output_file = split_up[0] + &#34;_result.&#34; + split_up[1]
            excel_info.to_excel(output_file, index=False)
        except PermissionError:
            logging.warning(
                &#34;Please close the excel file to&#34; 
                + &#34; write the result back to it at&#34; 
                + &#34; {}&#34;.format(output_file))

        logging.info(&#39;Angle calculation completed! Value was {}&#39;.format(angle))

    def cluster_evaluate(self, excel_file=None):
        &#34;&#34;&#34;
        Takes a list of unit specifications and evaluates the quality of the clustering.
        The results of the analysis are written back to the input Excel file.
        
        Parameters
        ----------
        excel_file : str
            Name of the excel file that contains data specifications
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        info = {&#39;spike&#39;: [], &#39;unit&#39;: []}
        if os.path.exists(excel_file):
            excel_info = pd.read_excel(excel_file)
            for row in excel_info.itertuples():
                spike_file = row[1]+ os.sep+ row[2]
                unit_no = int(row[3])
                if self.get_data_format() == &#39;NWB&#39;:
                # excel list: directory| spike group| unit_no
                    hdf_name = row[1] + os.sep+ row[2]+ &#39;.hdf5&#39;
                    spike_file = hdf_name+ &#39;/processing/Shank&#39;+ &#39;/&#39;+ row[3]
                info[&#39;spike&#39;].append(spike_file)
                info[&#39;unit&#39;].append(unit_no)
            n_units = excel_info.shape[0]

            excel_info = excel_info.assign(BC=pd.Series(np.zeros(n_units)))
            excel_info = excel_info.assign(Dh=pd.Series(np.zeros(n_units)))

            if info[&#39;spike&#39;]:
                for i, spike_file in enumerate(info[&#39;spike&#39;]):

                    logging.info(&#39;Evaluating unit: &#39;+ str(i+ 1))
                    if os.path.exists(spike_file):
                        self.ndata.set_spike_file(spike_file)
                        self.ndata.load_spike()
                        units = self.ndata.get_unit_list()

                        if info[&#39;unit&#39;][i] in units:
                            nclust = NClust(spike=self.ndata.spike)
                            bc, dh = nclust.cluster_separation(unit_no=info[&#39;unit&#39;][i])
                            excel_info.loc[i, &#39;BC&#39;] = np.max(bc)
                            excel_info.loc[i, &#39;Dh&#39;] = np.min(dh)
            excel_info.to_excel(excel_file, index=False)
            logging.info(&#39;Cluster evaluation completed!&#39;)
        else:
            logging.error(&#39;Excel  file does not exist!&#39;)

    def cluster_similarity(self, excel_file=None):
        &#34;&#34;&#34;
        Takes a list of specifications for pairwise comparison of units. The results
        are written back to the input Excel file.
        
        Parameters
        ----------
        excel_file : str
            Name of the excel file that contains unit specifications
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        # test on Pawel&#39;s data
        nclust_1 = NClust()
        nclust_2 = NClust()
        info = {&#39;spike_1&#39;: [], &#39;unit_1&#39;: [], &#39;spike_2&#39;: [], &#39;unit_2&#39;: []}
        if os.path.exists(excel_file):
            excel_info = pd.read_excel(excel_file)
            for row in excel_info.itertuples():
                spike_file = row[1]+ os.sep+ row[2]
                unit_1 = int(row[3])
                if self.get_data_format() == &#39;NWB&#39;:
                        # excel list: directory| spike group| unit_no
                    hdf_name = row[1] + os.sep+ row[2]+ &#39;.hdf5&#39;
                    spike_file = hdf_name+ &#39;/processing/Shank&#39;+ &#39;/&#39;+ row[3]
                info[&#39;spike_1&#39;].append(spike_file)
                info[&#39;unit_1&#39;].append(unit_1)

                spike_file = row[4]+ os.sep+ row[5]
                unit_2 = int(row[6])
                if self.get_data_format() == &#39;NWB&#39;:
                        # excel list: directory| spike group| unit_no
                    hdf_name = row[4] + os.sep+ row[5]+ &#39;.hdf5&#39;
                    spike_file = hdf_name+ &#39;/processing/Shank&#39;+ &#39;/&#39;+ row[6]
                info[&#39;spike_2&#39;].append(spike_file)
                info[&#39;unit_2&#39;].append(unit_2)

            n_comparison = excel_info.shape[0]

            excel_info = excel_info.assign(BC=pd.Series(np.zeros(n_comparison)))
            excel_info = excel_info.assign(Dh=pd.Series(np.zeros(n_comparison)))

            if info[&#39;spike_1&#39;]:
                for i in np.arange(n_comparison):
                    logging.info(&#39;Evaluating unit similarity row: &#39;+ str(i+ 1))
                    if os.path.exists(info[&#39;spike_1&#39;]) and os.path.exists(info[&#39;spike_2&#39;]):
                        nclust_1.load(filename=info[&#39;spike_1&#39;], system=self.get_data_format())
                        nclust_2.load(filename=info[&#39;spike_2&#39;], system=self.get_data_format())
                        bc, dh = nclust_1.cluster_similarity(nclust=nclust_2, \
                                unit_1=info[&#39;unit_1&#39;][i], unit_2=info[&#39;unit_2&#39;][i])
                        excel_info.loc[i, &#39;BC&#39;] = bc
                        excel_info.loc[i, &#39;Dh&#39;] = dh
            excel_info.to_excel(excel_file, index=False)
            logging.info(&#39;Cluster similarity analysis completed!&#39;)
        else:
            logging.error(&#39;Excel  file does not exist!&#39;)

    def place_cell_plots(self, directory, dpi=400):
        &#34;&#34;&#34;
        Plot png images of place cell figures, looping over a directory.

        Currently only works for axona files, but can be extended.

        Parameters
        ----------
        dir : str
            The directory to get files from.
        dpi : int
            The desired dpi of the pngs.

        &#34;&#34;&#34;
        try:
            container = NDataContainer(load_on_fly=True)
            container.add_axona_files_from_dir(
                directory, tetrode_list = [i for i in range(1, 17)])
            nca.place_cell_summary(
                container, dpi=dpi, 
                filter_place_cells=False, filter_low_freq=False)
        except Exception as ex:
            log_exception(ex, &#34;In walking a directory for place cell summaries&#34;)
        return </code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>PyQt5.QtCore.QThread</li>
<li>PyQt5.QtCore.QObject</li>
<li>sip.wrapper</li>
<li>sip.simplewrapper</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="neurochat.nc_control.NeuroChaT.angle_calculation"><code class="name flex">
<span>def <span class="ident">angle_calculation</span></span>(<span>self, excel_file=None, should_plot=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Takes a list of unit specifications and finds the angle between the place field centroids
The results of the analysis are written back to the input Excel file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>excel_file</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the excel file that contains data specifications</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def angle_calculation(self, excel_file=None, should_plot=True):
    &#34;&#34;&#34;
    Takes a list of unit specifications and finds the angle between the place field centroids
    The results of the analysis are written back to the input Excel file.

    Parameters
    ----------
    excel_file : str
        Name of the excel file that contains data specifications

    Returns
    -------
    None
    &#34;&#34;&#34;
    params= self.get_params_by_analysis(&#39;loc_rate&#39;)

    if params[&#39;loc_rate_filter&#39;] == &#39;Gaussian&#39;:
        filttype = &#39;g&#39;
    else:
        filttype = &#39;b&#39;

    collection = NDataContainer()
    excel_info = collection.add_files_from_excel(excel_file)
    if excel_info is None:
        return

    n_units = len(collection)
    if (n_units % 3 != 0) :
        logging.error(&#34;angle_calculation: Can&#39;t compute the angle for a number of units not divisible by 3, given &#34; + str(n_units))
        return

    excel_info = excel_info.assign(CentroidX=pd.Series(np.zeros(n_units)))
    excel_info = excel_info.assign(CentroidY=pd.Series(np.zeros(n_units)))
    excel_info = excel_info.assign(
        AngleInDegrees=pd.Series(np.zeros(n_units)))
    excel_info = excel_info.assign(
        StrongPlaceField=pd.Series(np.zeros(n_units)))
    excel_info = excel_info.assign(
        Skaggs=pd.Series(np.zeros(n_units)))
    
    centroids = []
    figs = []
    for i, data in enumerate(collection):
        place_data = data.place(
            pixel=params[&#39;loc_pixel_size&#39;],
            chop_bound=params[&#39;loc_chop_bound&#39;],
            filter=[filttype, params[&#39;loc_rate_kern_len&#39;]],
            fieldThresh=params[&#39;loc_field_thresh&#39;],
            smoothPlace=params[&#39;loc_field_smooth&#39;],
            brAdjust=True, update=True)
        centroid = place_data[&#39;centroid&#39;]
        centroids.append(centroid)
        excel_info.loc[i, &#34;CentroidX&#34;] = centroid[0]
        excel_info.loc[i, &#34;CentroidY&#34;] = centroid[1]
        _res = data.get_results()
        excel_info.loc[i, &#34;Skaggs&#34;] = _res[&#34;Spatial Skaggs&#34;]
        excel_info.loc[i, &#34;StrongPlaceField&#34;] = (
            _res[&#34;Found strong place field&#34;])
        if should_plot:
            fig = nc_plot.loc_firing_and_place(place_data)
            figs.append(fig)

        if (i + 1) % 3 == 0: #then spit out the angle
            first_centroid = centroids[0]
            second_centroid = centroids[1]
            angle = angle_between_points(
                first_centroid, second_centroid, centroid)
            excel_info.loc[i, &#34;AngleInDegrees&#34;] = angle
            if should_plot:
                fig = nc_plot.plot_angle_between_points(
                    centroids, 
                    place_data[&#39;xedges&#39;].max(), 
                    place_data[&#39;yedges&#39;].max())
                figs.append(fig)
            centroids = []
    
    if should_plot:
        self.close_fig(figs)

    try:
        split_up = remove_extension(
            excel_file, keep_dot=False, return_ext=True)
        output_file = split_up[0] + &#34;_result.&#34; + split_up[1]
        excel_info.to_excel(output_file, index=False)
    except PermissionError:
        logging.warning(
            &#34;Please close the excel file to&#34; 
            + &#34; write the result back to it at&#34; 
            + &#34; {}&#34;.format(output_file))

    logging.info(&#39;Angle calculation completed! Value was {}&#39;.format(angle))</code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.close_fig"><code class="name flex">
<span>def <span class="ident">close_fig</span></span>(<span>self, fig)</span>
</code></dt>
<dd>
<section class="desc"><p>Closes a matplotlib.fiure.Figure() object after saving it to the output PDF.
A a tuple or list of such figures are provided, each of them saved and closed
accordingly.</p>
<h2 id="parameters">Parameters</h2>
<p>fig
matplotlib.fiure.Figure() or a list or tuple of them.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def close_fig(self, fig):
    &#34;&#34;&#34;
    Closes a matplotlib.fiure.Figure() object after saving it to the output PDF.
    A a tuple or list of such figures are provided, each of them saved and closed
    accordingly.
    
    Parameters
    ----------
    fig 
       matplotlib.fiure.Figure() or a list or tuple of them.
    
    Returns
    -------
    None
        
    &#34;&#34;&#34;
    
    if isinstance(fig, (tuple, list)):
        for f in fig:
            if isinstance(f, matplotlib.figure.Figure):
                if self.save_to_file:
                    try:
                        self.pdf.savefig(f, dpi=400)
                    except PermissionError:
                        logging.error(&#34;Please close pdf before saving output to it&#34;)
                plt.close(f)
            else:
                logging.error(&#39;Invalid matplotlib.figure instance&#39;)
    elif isinstance(fig, matplotlib.figure.Figure):
        if self.save_to_file:
            try:
                self.pdf.savefig(fig)
            except PermissionError:
                logging.error(&#34;Please close pdf before saving output to it&#34;)
        plt.close(fig)
    else:
        logging.error(&#39;Invalid matplotlib.figure instance&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.close_hdf_file"><code class="name flex">
<span>def <span class="ident">close_hdf_file</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Closes the HDF5 file object.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def close_hdf_file(self):
    &#34;&#34;&#34;
    Closes the HDF5 file object.
    
    Parameters
    ----------
    None
    
    Returns
    -------
    None
        
    &#34;&#34;&#34;
    
    self.hdf.close()</code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.close_pdf"><code class="name flex">
<span>def <span class="ident">close_pdf</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>closes the PDF file object.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def close_pdf(self):
    &#34;&#34;&#34;
    closes the PDF file object.
    
    Parameters
    ----------
    None
    
    Returns
    -------
    None
        
    &#34;&#34;&#34;
    
    if self._pdf_file is not None:
        self.pdf.close()
        logging.info(&#39;Output graphics saved to &#39;+ self._pdf_file)
    else:
        logging.warning(&#39;No PDF file for graphic output!&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.cluster_evaluate"><code class="name flex">
<span>def <span class="ident">cluster_evaluate</span></span>(<span>self, excel_file=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Takes a list of unit specifications and evaluates the quality of the clustering.
The results of the analysis are written back to the input Excel file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>excel_file</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the excel file that contains data specifications</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def cluster_evaluate(self, excel_file=None):
    &#34;&#34;&#34;
    Takes a list of unit specifications and evaluates the quality of the clustering.
    The results of the analysis are written back to the input Excel file.
    
    Parameters
    ----------
    excel_file : str
        Name of the excel file that contains data specifications
    
    Returns
    -------
    None

    &#34;&#34;&#34;
    
    info = {&#39;spike&#39;: [], &#39;unit&#39;: []}
    if os.path.exists(excel_file):
        excel_info = pd.read_excel(excel_file)
        for row in excel_info.itertuples():
            spike_file = row[1]+ os.sep+ row[2]
            unit_no = int(row[3])
            if self.get_data_format() == &#39;NWB&#39;:
            # excel list: directory| spike group| unit_no
                hdf_name = row[1] + os.sep+ row[2]+ &#39;.hdf5&#39;
                spike_file = hdf_name+ &#39;/processing/Shank&#39;+ &#39;/&#39;+ row[3]
            info[&#39;spike&#39;].append(spike_file)
            info[&#39;unit&#39;].append(unit_no)
        n_units = excel_info.shape[0]

        excel_info = excel_info.assign(BC=pd.Series(np.zeros(n_units)))
        excel_info = excel_info.assign(Dh=pd.Series(np.zeros(n_units)))

        if info[&#39;spike&#39;]:
            for i, spike_file in enumerate(info[&#39;spike&#39;]):

                logging.info(&#39;Evaluating unit: &#39;+ str(i+ 1))
                if os.path.exists(spike_file):
                    self.ndata.set_spike_file(spike_file)
                    self.ndata.load_spike()
                    units = self.ndata.get_unit_list()

                    if info[&#39;unit&#39;][i] in units:
                        nclust = NClust(spike=self.ndata.spike)
                        bc, dh = nclust.cluster_separation(unit_no=info[&#39;unit&#39;][i])
                        excel_info.loc[i, &#39;BC&#39;] = np.max(bc)
                        excel_info.loc[i, &#39;Dh&#39;] = np.min(dh)
        excel_info.to_excel(excel_file, index=False)
        logging.info(&#39;Cluster evaluation completed!&#39;)
    else:
        logging.error(&#39;Excel  file does not exist!&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.cluster_similarity"><code class="name flex">
<span>def <span class="ident">cluster_similarity</span></span>(<span>self, excel_file=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Takes a list of specifications for pairwise comparison of units. The results
are written back to the input Excel file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>excel_file</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the excel file that contains unit specifications</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def cluster_similarity(self, excel_file=None):
    &#34;&#34;&#34;
    Takes a list of specifications for pairwise comparison of units. The results
    are written back to the input Excel file.
    
    Parameters
    ----------
    excel_file : str
        Name of the excel file that contains unit specifications
    
    Returns
    -------
    None

    &#34;&#34;&#34;
    # test on Pawel&#39;s data
    nclust_1 = NClust()
    nclust_2 = NClust()
    info = {&#39;spike_1&#39;: [], &#39;unit_1&#39;: [], &#39;spike_2&#39;: [], &#39;unit_2&#39;: []}
    if os.path.exists(excel_file):
        excel_info = pd.read_excel(excel_file)
        for row in excel_info.itertuples():
            spike_file = row[1]+ os.sep+ row[2]
            unit_1 = int(row[3])
            if self.get_data_format() == &#39;NWB&#39;:
                    # excel list: directory| spike group| unit_no
                hdf_name = row[1] + os.sep+ row[2]+ &#39;.hdf5&#39;
                spike_file = hdf_name+ &#39;/processing/Shank&#39;+ &#39;/&#39;+ row[3]
            info[&#39;spike_1&#39;].append(spike_file)
            info[&#39;unit_1&#39;].append(unit_1)

            spike_file = row[4]+ os.sep+ row[5]
            unit_2 = int(row[6])
            if self.get_data_format() == &#39;NWB&#39;:
                    # excel list: directory| spike group| unit_no
                hdf_name = row[4] + os.sep+ row[5]+ &#39;.hdf5&#39;
                spike_file = hdf_name+ &#39;/processing/Shank&#39;+ &#39;/&#39;+ row[6]
            info[&#39;spike_2&#39;].append(spike_file)
            info[&#39;unit_2&#39;].append(unit_2)

        n_comparison = excel_info.shape[0]

        excel_info = excel_info.assign(BC=pd.Series(np.zeros(n_comparison)))
        excel_info = excel_info.assign(Dh=pd.Series(np.zeros(n_comparison)))

        if info[&#39;spike_1&#39;]:
            for i in np.arange(n_comparison):
                logging.info(&#39;Evaluating unit similarity row: &#39;+ str(i+ 1))
                if os.path.exists(info[&#39;spike_1&#39;]) and os.path.exists(info[&#39;spike_2&#39;]):
                    nclust_1.load(filename=info[&#39;spike_1&#39;], system=self.get_data_format())
                    nclust_2.load(filename=info[&#39;spike_2&#39;], system=self.get_data_format())
                    bc, dh = nclust_1.cluster_similarity(nclust=nclust_2, \
                            unit_1=info[&#39;unit_1&#39;][i], unit_2=info[&#39;unit_2&#39;][i])
                    excel_info.loc[i, &#39;BC&#39;] = bc
                    excel_info.loc[i, &#39;Dh&#39;] = dh
        excel_info.to_excel(excel_file, index=False)
        logging.info(&#39;Cluster similarity analysis completed!&#39;)
    else:
        logging.error(&#39;Excel  file does not exist!&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.convert_to_nwb"><code class="name flex">
<span>def <span class="ident">convert_to_nwb</span></span>(<span>self, excel_file=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Takes a list of datasets in Excel file and converts them into NWB file format. This
method currently supports Axona and Neuralynx data formats.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>excel_file</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the excel file that contains data specifications</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def convert_to_nwb(self, excel_file=None):
    &#34;&#34;&#34;
    Takes a list of datasets in Excel file and converts them into NWB file format. This 
    method currently supports Axona and Neuralynx data formats.
    
    Parameters
    ----------
    excel_file : str
        Name of the excel file that contains data specifications
    
    Returns
    -------
    None

    &#34;&#34;&#34;
    
    if self.get_data_format() == &#39;NWB&#39;:
        logging.error(&#39;NWB files do not need to be converted! Check file format option again!&#39;)
    info = {&#39;spat&#39;: [], &#39;spike&#39;: [], &#39;lfp&#39;: []}
    export_info = oDict({&#39;dir&#39;: [], &#39;nwb&#39;: [], &#39;spike&#39;: [], &#39;lfp&#39;: []})
    if os.path.exists(excel_file):
        excel_info = pd.read_excel(excel_file)
        for row in excel_info.itertuples():
            spike_file = row[1]+ os.sep+ row[3]
            lfp_id = row[4]

            if self.get_data_format() == &#39;Axona&#39;:
                spatial_file = row[1]+ os.sep+ row[2]+ &#39;.txt&#39;
                lfp_file = remove_extension(spike_file) + lfp_id

            elif self.get_data_format() == &#39;Neuralynx&#39;:
                spatial_file = row[1] + os.sep+ row[2]+ &#39;.nvt&#39;
                lfp_file = row[1]+ os.sep+ lfp_id+ &#39;.ncs&#39;

            info[&#39;spat&#39;].append(spatial_file)
            info[&#39;spike&#39;].append(spike_file)
            info[&#39;lfp&#39;].append(lfp_file)

    if info[&#39;spike&#39;]:
        for i, spike_file in enumerate(info[&#39;spike&#39;]):

            logging.info(&#39;Converting file groups: &#39;+ str(i+ 1))
            self.ndata.set_spatial_file(info[&#39;spat&#39;][i])
            self.ndata.set_spike_file(info[&#39;spike&#39;][i])
            self.ndata.set_lfp_file(info[&#39;lfp&#39;][i])
            self.ndata.load()
            self.ndata.save_to_hdf5()

            f_name = self.hdf.resolve_hdfname(data=self.ndata.spike)
            export_info[&#39;dir&#39;].append(os.sep.join(f_name.split(os.sep)[:-1]))
            export_info[&#39;nwb&#39;].append(f_name.split(os.sep)[-1].split(&#39;.&#39;)[0])

            export_info[&#39;spike&#39;].append(self.hdf.get_file_tag(self.ndata.spike))
            export_info[&#39;lfp&#39;].append(self.hdf.get_file_tag(self.ndata.lfp))

    export_info = pd.DataFrame(export_info, columns=[&#39;dir&#39;, &#39;nwb&#39;, &#39;spike&#39;, &#39;lfp&#39;])
    words = excel_file.split(os.sep)
    name = &#39;NWB_list_&#39; + words[-1]
    export_info.to_excel(os.path.join(os.sep.join(words[:-1]), name))
    logging.info(&#39;Conversion process completed!&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.execute"><code class="name flex">
<span>def <span class="ident">execute</span></span>(<span>self, name=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Checks the selection of each analyses, and executes if they are selected.
It also exports the plot data from individual analyses to the hdf file and
figures to the graphics file that are set in the mode() method.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the unit or the unique unit ID</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def execute(self, name=None):
    &#34;&#34;&#34;
    Checks the selection of each analyses, and executes if they are selected.
    It also exports the plot data from individual analyses to the hdf file and 
    figures to the graphics file that are set in the mode() method.
    
    Parameters
    ----------
    name : str
        Name of the unit or the unique unit ID
    
    Returns
    -------
    None
        
    &#34;&#34;&#34;
    
    try:
        logging.info(&#39;Calculating environmental border...&#39;)
        self.set_border(self.calc_border())

    except:
        logging.warning(&#39;Border calculation was not properly completed!&#39;)

    if self.get_analysis(&#39;wave_property&#39;):
        logging.info(&#39;Assessing waveform properties...&#39;)
        try:
            graph_data = self.wave_property() # gd = graph_data
            fig = nc_plot.wave_property(graph_data, [int(self.get_total_channels()/2), 2])
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/waveProperty/&#39;, graph_data=graph_data)
        except:
            logging.error(&#39;Error in assessing waveform property&#39;)

    if self.get_analysis(&#39;isi&#39;):
        # ISI analysis
        logging.info(&#39;Calculating inter-spike interval distribution...&#39;)
        try:
            params= self.get_params_by_analysis(&#39;isi&#39;)
            graph_data = self.isi(
                bins=int(params[&#39;isi_length&#39;]/params[&#39;isi_bin&#39;]),
                bound=[0, params[&#39;isi_length&#39;]],
                refractory_threshold=params[&#39;isi_refractory&#39;])
            fig = nc_plot.isi(graph_data)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/isi/&#39;, graph_data=graph_data)
        except Exception as ex:
            log_exception(
                ex, &#39;Error in assessing interspike interval distribution&#39;)

    if self.get_analysis(&#39;isi_corr&#39;):
        ##Autocorr 1000ms
        logging.info(&#39;Calculating inter-spike interval autocorrelation histogram...&#39;)
        try:
            params= self.get_params_by_analysis(&#39;isi_corr&#39;)

            graph_data = self.isi_corr(bins=params[&#39;isi_corr_bin_long&#39;], \
                                    bound=[-params[&#39;isi_corr_len_long&#39;], params[&#39;isi_corr_len_long&#39;]])
            fig = nc_plot.isi_corr(graph_data)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/isiCorrLong/&#39;, graph_data=graph_data)
            # Autocorr 10ms
            graph_data = self.isi_corr(bins=params[&#39;isi_corr_bin_short&#39;], \
                                    bound=[-params[&#39;isi_corr_len_short&#39;], params[&#39;isi_corr_len_short&#39;]])
            fig = nc_plot.isi_corr(graph_data)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/isiCorrShort/&#39;, graph_data=graph_data)
        except:
            logging.error(&#39;Error in assessing ISI autocorrelation&#39;)

    if self.get_analysis(&#39;theta_cell&#39;):
        ## Theta-Index analysis
        logging.info(&#39;Estimating theta-modulation index...&#39;)
        try:
            params= self.get_params_by_analysis(&#39;theta_cell&#39;)

            graph_data = self.theta_index(start=[params[&#39;theta_cell_freq_start&#39;], params[&#39;theta_cell_tau1_start&#39;], params[&#39;theta_cell_tau2_start&#39;]], \
                           lower=[params[&#39;theta_cell_freq_min&#39;], 0, 0], \
                           upper=[params[&#39;theta_cell_freq_max&#39;], params[&#39;theta_cell_tau1_max&#39;], params[&#39;theta_cell_tau2_max&#39;]], \
                           bins=params[&#39;isi_corr_bin_long&#39;], \
                           bound=[-params[&#39;isi_corr_len_long&#39;], params[&#39;isi_corr_len_long&#39;]])
            fig = nc_plot.theta_cell(graph_data)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/theta_cell/&#39;, graph_data=graph_data)
        except:
            logging.error(&#39;Error in theta-index analysis&#39;)

    if self.get_analysis(&#39;theta_skip_cell&#39;):
        logging.info(&#39;Estimating theta-skipping index...&#39;)
        try:
            params= self.get_params_by_analysis(&#39;theta_cell&#39;)

            graph_data = self.theta_skip_index(start=[params[&#39;theta_cell_freq_start&#39;], params[&#39;theta_cell_tau1_start&#39;], params[&#39;theta_cell_tau2_start&#39;]], \
                           lower=[params[&#39;theta_cell_freq_min&#39;], 0, 0], \
                           upper=[params[&#39;theta_cell_freq_max&#39;], params[&#39;theta_cell_tau1_max&#39;], params[&#39;theta_cell_tau2_max&#39;]], \
                           bins=params[&#39;isi_corr_bin_long&#39;], \
                           bound=[-params[&#39;isi_corr_len_long&#39;], params[&#39;isi_corr_len_long&#39;]])
            fig = nc_plot.theta_cell(graph_data)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/theta_skip_cell/&#39;, graph_data=graph_data)
        except:
            logging.error(&#39;Error in theta-skipping cell index analysis&#39;)

    if self.get_analysis(&#39;burst&#39;):
        ### Burst analysis
        logging.info(&#39;Analyzing bursting property...&#39;)
        try:
            params= self.get_params_by_analysis(&#39;burst&#39;)

            self.burst(burst_thresh=params[&#39;burst_thresh&#39;],\
                       ibi_thresh=params[&#39;ibi_thresh&#39;])
        except:
            logging.error(&#39;Error in analysing bursting property&#39;)

    if self.get_analysis(&#39;speed&#39;):
        ## Speed analysis
        logging.info(&#39;Calculating spike-rate vs running speed...&#39;)
        try:
            params= self.get_params_by_analysis(&#39;speed&#39;)

            graph_data = self.speed(range=[params[&#39;speed_min&#39;], params[&#39;speed_max&#39;]], \
                                  binsize=params[&#39;speed_bin&#39;], update=True)
            fig = nc_plot.speed(graph_data)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/speed/&#39;, graph_data=graph_data)
        except:
            logging.error(&#39;Error in analysis of spike rate vs speed&#39;)

    if self.get_analysis(&#39;ang_vel&#39;):
        ## Angular velocity analysis
        logging.info(&#39;Calculating spike-rate vs angular head velocity...&#39;)
        try:
            params= self.get_params_by_analysis(&#39;ang_vel&#39;)

            graph_data = self.angular_velocity(range=[params[&#39;ang_vel_min&#39;], params[&#39;ang_vel_max&#39;]], \
                                binsize=params[&#39;ang_vel_bin&#39;], cutoff=params[&#39;ang_vel_cutoff&#39;], update=True)
            fig = nc_plot.angular_velocity(graph_data)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/ang_vel/&#39;, graph_data=graph_data)
        except:
            logging.error(&#39;Error in analysis of spike rate vs angular velocity&#39;)

    if self.get_analysis(&#39;hd_rate&#39;):
        logging.info(&#39;Assessing head-directional tuning...&#39;)
        try:
            params= self.get_params_by_analysis(&#39;hd_rate&#39;)

            hdData = self.hd_rate(binsize=params[&#39;hd_bin&#39;], \
                    filter=[&#39;b&#39;, params[&#39;hd_rate_kern_len&#39;]],\
                    pixel=params[&#39;loc_pixel_size&#39;], update=True)
            fig = nc_plot.hd_firing(hdData)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/hd_rate/&#39;, graph_data=hdData)

            hdData = self.hd_rate_ccw(binsize=params[&#39;hd_bin&#39;], \
                    filter=[&#39;b&#39;, params[&#39;hd_rate_kern_len&#39;]],\
                    thresh=params[&#39;hd_ang_vel_cutoff&#39;],\
                    pixel=params[&#39;loc_pixel_size&#39;], update=True)
            fig = nc_plot.hd_rate_ccw(hdData)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/hd_rate_CCW/&#39;, graph_data=hdData)

        except:
            logging.error(&#39;Error in analysis of spike rate vs head direction&#39;)

    if self.get_analysis(&#39;hd_shuffle&#39;):
        logging.info(&#39;Shuffling analysis of head-directional tuning...&#39;)
        try:
            params= self.get_params_by_analysis(&#39;hd_shuffle&#39;)

            graph_data = self.hd_shuffle(bins=params[&#39;hd_shuffle_bins&#39;], \
                                      nshuff=params[&#39;hd_shuffle_total&#39;], limit=params[&#39;hd_shuffle_limit&#39;])
            fig = nc_plot.hd_shuffle(graph_data)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/hd_shuffle/&#39;, graph_data=graph_data)
        except:
            logging.error(&#39;Error in head directional shuffling analysis&#39;)

    if self.get_analysis(&#39;hd_time_lapse&#39;):
        logging.info(&#39;Time-lapsed head-directional tuning...&#39;)
        try:
            graph_data = self.hd_time_lapse()

            fig = nc_plot.hd_spike_time_lapse(graph_data)
            self.close_fig(fig)

            fig = nc_plot.hd_rate_time_lapse(graph_data)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/hd_time_lapse/&#39;, graph_data=graph_data)

        except:
            logging.error(&#39;Error in locational time-lapse analysis&#39;)

    if self.get_analysis(&#39;hd_time_shift&#39;):
        logging.info(&#39;Time-shift analysis of head-directional tuning...&#39;)
        try:
            params= self.get_params_by_analysis(&#39;hd_time_shift&#39;)

            hdData = self.hd_shift(shift_ind=np.arange(params[&#39;hd_shift_min&#39;], \
                                                    params[&#39;hd_shift_max&#39;]+ params[&#39;hd_shift_step&#39;], \
                                                    params[&#39;hd_shift_step&#39;]))
            fig = nc_plot.hd_time_shift(hdData)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/hd_time_shift/&#39;, graph_data=hdData)
        except:
            logging.error(&#39;Error in head directional time-shift analysis&#39;)

    if self.get_analysis(&#39;loc_rate&#39;):
        logging.info(&#39;Assessing of locational tuning...&#39;)
        try:
            params= self.get_params_by_analysis(&#39;loc_rate&#39;)

            if params[&#39;loc_rate_filter&#39;] == &#39;Gaussian&#39;:
                filttype = &#39;g&#39;
            else:
                filttype = &#39;b&#39;

            place_data = self.ndata.place(
                          pixel=params[&#39;loc_pixel_size&#39;],
                          chop_bound=params[&#39;loc_chop_bound&#39;],
                          filter=[filttype, params[&#39;loc_rate_kern_len&#39;]],
                          fieldThresh=params[&#39;loc_field_thresh&#39;],
                          smoothPlace=params[&#39;loc_field_smooth&#39;],
                          brAdjust=True, update=True)
            fig1 = nc_plot.loc_firing(place_data)
            self.close_fig(fig1)
            fig2 = nc_plot.loc_firing_and_place(place_data)
            self.close_fig(fig2)
            self.plot_data_to_hdf(name=name+ &#39;/loc_rate/&#39;, graph_data=place_data)

        except:
            logging.error(&#39;Error in analysis of locational firing rate&#39;)

    if self.get_analysis(&#39;loc_shuffle&#39;):
        logging.info(&#39;Shuffling analysis of locational tuning...&#39;)
        try:
            params= self.get_params_by_analysis(&#39;loc_shuffle&#39;)

            if params[&#39;loc_rate_filter&#39;] == &#39;Gaussian&#39;:
                filttype = &#39;g&#39;
            else:
                filttype = &#39;b&#39;

            place_data = self.loc_shuffle(bins=params[&#39;loc_shuffle_nbins&#39;], \
                                      nshuff=params[&#39;loc_shuffle_total&#39;], \
                                      limit=params[&#39;loc_shuffle_limit&#39;], \
                                      pixel=params[&#39;loc_pixel_size&#39;], \
                                      chop_bound=params[&#39;loc_chop_bound&#39;], \
                                      filter=[filttype, params[&#39;loc_rate_kern_len&#39;]],\
                                      brAdjust=True, update=False)
            fig = nc_plot.loc_shuffle(place_data)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/loc_shuffle/&#39;, graph_data=place_data)
        except:
            logging.error(&#39;Error in locational shiffling analysis&#39;)

    if self.get_analysis(&#39;loc_time_lapse&#39;):
        logging.info(&#39;Time-lapse analysis of locational tuning...&#39;)
        try:
            params= self.get_params_by_analysis(&#39;loc_time_lapse&#39;)

            if params[&#39;loc_rate_filter&#39;] == &#39;Gaussian&#39;:
                filttype = &#39;g&#39;
            else:
                filttype = &#39;b&#39;

            graph_data = self.loc_time_lapse(pixel=params[&#39;loc_pixel_size&#39;], \
                          chop_bound=params[&#39;loc_chop_bound&#39;],\
                          filter=[filttype, params[&#39;loc_rate_kern_len&#39;]],\
                          brAdjust=True)

            fig = nc_plot.loc_spike_time_lapse(graph_data)
            self.close_fig(fig)

            fig = nc_plot.loc_rate_time_lapse(graph_data)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/loc_time_lapse/&#39;, graph_data=graph_data)
        except:
            logging.error(&#39;Error in locational time-lapse analysis&#39;)

    if self.get_analysis(&#39;loc_time_shift&#39;):
        logging.info(&#39;Time-shift analysis of locational tuning...&#39;)
        try:
            params= self.get_params_by_analysis(&#39;loc_time_shift&#39;)

            if params[&#39;loc_rate_filter&#39;] == &#39;Gaussian&#39;:
                filttype = &#39;g&#39;
            else:
                filttype = &#39;b&#39;

            plot_data = self.loc_shift(shift_ind=np.arange(params[&#39;loc_shift_min&#39;], \
                                    params[&#39;loc_shift_max&#39;]+ params[&#39;loc_shift_step&#39;], \
                                    params[&#39;loc_shift_step&#39;]), \
                                    pixel=params[&#39;loc_pixel_size&#39;], \
                                    chop_bound=params[&#39;loc_chop_bound&#39;], \
                                    filter=[filttype, params[&#39;loc_rate_kern_len&#39;]],\
                                    brAdjust=True, update=False)
            fig = nc_plot.loc_time_shift(plot_data)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/loc_time_shift/&#39;, graph_data=plot_data)
        except:
            logging.error(&#39;Error in locational time-shift analysis&#39;)

    if self.get_analysis(&#39;spatial_corr&#39;):
        logging.info(&#39;Spatial and rotational correlation of locational tuning...&#39;)
        try:
            params= self.get_params_by_analysis(&#39;spatial_corr&#39;)

            if params[&#39;spatial_corr_filter&#39;] == &#39;Gaussian&#39;:
                filttype = &#39;g&#39;
            else:
                filttype = &#39;b&#39;

            plot_data = self.loc_auto_corr(pixel=params[&#39;loc_pixel_size&#39;], \
                          chop_bound=params[&#39;loc_chop_bound&#39;],\
                          filter=[filttype, params[&#39;spatial_corr_kern_len&#39;]],\
                          minPixel=params[&#39;spatial_corr_min_obs&#39;], brAdjust=True)
            fig = nc_plot.loc_auto_corr(plot_data)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/spatial_corr/&#39;, graph_data=plot_data)

            plot_data = self.loc_rot_corr(binsize=params[&#39;rot_corr_bin&#39;], \
                                      pixel=params[&#39;loc_pixel_size&#39;], \
                                      chop_bound=params[&#39;loc_chop_bound&#39;],\
                                      filter=[filttype, params[&#39;spatial_corr_kern_len&#39;]],\
                                      minPixel=params[&#39;spatial_corr_min_obs&#39;], brAdjust=True)
            fig = nc_plot.rot_corr(plot_data)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/spatial_corr/&#39;, graph_data=plot_data)

        except:
            logging.error(&#39;Error in assessing spatial autocorrelation&#39;)

    if self.get_analysis(&#39;grid&#39;):
        logging.info(&#39;Assessing gridness...&#39;)
        try:
            params= self.get_params_by_analysis(&#39;grid&#39;)

            if params[&#39;spatial_corr_filter&#39;] == &#39;Gaussian&#39;:
                filttype = &#39;g&#39;
            else:
                filttype = &#39;b&#39;

            graph_data = self.grid(angtol=params[&#39;grid_ang_tol&#39;],\
                                 binsize=params[&#39;grid_ang_bin&#39;], \
                                 pixel=params[&#39;loc_pixel_size&#39;], \
                                 chop_bound=params[&#39;loc_chop_bound&#39;],\
                                 filter=[filttype, params[&#39;spatial_corr_kern_len&#39;]],\
                                 minPixel=params[&#39;spatial_corr_min_obs&#39;], \
                                 brAdjust=True) # Add other paramaters
            fig = nc_plot.grid(graph_data)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/grid/&#39;, graph_data=graph_data)

        except:
            logging.error(&#39;Error in grid cell analysis&#39;)

    if self.get_analysis(&#39;border&#39;):
        logging.info(&#39;Estimating tuning to border...&#39;)
        try:
            params= self.get_params_by_analysis(&#39;border&#39;)

            if params[&#39;loc_rate_filter&#39;] == &#39;Gaussian&#39;:
                filttype = &#39;g&#39;
            else:
                filttype = &#39;b&#39;

            graph_data = self.border(update=True, thresh=params[&#39;border_firing_thresh&#39;], \
                                   cbinsize=params[&#39;border_ang_bin&#39;], \
                                   nstep=params[&#39;border_stair_steps&#39;], \
                                   pixel=params[&#39;loc_pixel_size&#39;], \
                                   chop_bound=params[&#39;loc_chop_bound&#39;],\
                                   filter=[filttype, params[&#39;loc_rate_kern_len&#39;]],\
                                   brAdjust=True)

            fig = nc_plot.border(graph_data)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/border/&#39;, graph_data=graph_data)
        except:
            logging.error(&#39;Error in border cell analysis&#39;)

    if self.get_analysis(&#39;gradient&#39;):
        logging.info(&#39;Calculating gradient-cell properties...&#39;)
        try:
            params= self.get_params_by_analysis(&#39;gradient&#39;)

            if params[&#39;loc_rate_filter&#39;] == &#39;Gaussian&#39;:
                filttype = &#39;g&#39;
            else:
                filttype = &#39;b&#39;

            graph_data = self.gradient(alim=params[&#39;grad_asymp_lim&#39;], \
                                     blim=params[&#39;grad_displace_lim&#39;], \
                                     clim=params[&#39;grad_growth_rate_lim&#39;], \
                                     pixel=params[&#39;loc_pixel_size&#39;], \
                                     chop_bound=params[&#39;loc_chop_bound&#39;],\
                                     filter=[filttype, params[&#39;loc_rate_kern_len&#39;]],\
                                     brAdjust=True)
            fig = nc_plot.gradient(graph_data)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/gradient/&#39;, graph_data=graph_data)
        except:
            logging.error(&#39;Error in gradient cell analysis&#39;)

    if self.get_analysis(&#39;multiple_regression&#39;):
        logging.info(&#39;Multiple-regression analysis...&#39;)
        try:
            params= self.get_params_by_analysis(&#39;multiple_regression&#39;)

            graph_data = self.multiple_regression(nrep=params[&#39;mra_nrep&#39;], \
                                episode=params[&#39;mra_episode&#39;], \
                                subsampInterv=params[&#39;mra_interval&#39;])

            fig = nc_plot.multiple_regression(graph_data)
            self.close_fig(fig)                
            self.plot_data_to_hdf(name=name+ &#39;/multiple_regression/&#39;, graph_data=graph_data)
        except Exception as ex:
            log_exception(
                ex, &#34;in multiple-regression analysis&#34;)

    if self.get_analysis(&#39;inter_depend&#39;):
        # No plot
        logging.info(&#39;Assessing dependence of variables to...&#39;)
        try:
            self.interdependence(pixel=3, hdbinsize=5, spbinsize=1, sprange=[0, 40], \
                                abinsize=10, angvelrange=[-500, 500])
        except:
            logging.error(&#39;Error in interdependence analysis&#39;)

    if self.get_analysis(&#39;lfp_spectrum&#39;):
        try:
            params= self.get_params_by_analysis(&#39;lfp_spectrum&#39;)

            graph_data = self.spectrum(window=params[&#39;lfp_pwelch_seg_size&#39;],\
                                     noverlap=params[&#39;lfp_pwelch_overlap&#39;], \
                                     nfft=params[&#39;lfp_pwelch_nfft&#39;],\
                                     ptype=&#39;psd&#39;, prefilt=True, \
                                     filtset=[params[&#39;lfp_prefilt_order&#39;], \
                                               params[&#39;lfp_prefilt_lowcut&#39;], \
                                               params[&#39;lfp_prefilt_highcut&#39;], &#39;bandpass&#39;], \
                                     fmax=params[&#39;lfp_pwelch_freq_max&#39;],\
                                     db=False, tr=False)
            fig = nc_plot.lfp_spectrum(graph_data)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/lfp_spectrum/&#39;, graph_data=graph_data)

            graph_data = self.spectrum(window=params[&#39;lfp_stft_seg_size&#39;],\
                                     noverlap=params[&#39;lfp_stft_overlap&#39;],\
                                     nfft=params[&#39;lfp_stft_nfft&#39;],\
                                     ptype=&#39;psd&#39;, prefilt=True,\
                                     filtset=[params[&#39;lfp_prefilt_order&#39;], \
                                               params[&#39;lfp_prefilt_lowcut&#39;], \
                                               params[&#39;lfp_prefilt_highcut&#39;], &#39;bandpass&#39;], \
                                     fmax=params[&#39;lfp_stft_freq_max&#39;],\
                                     db=True, tr=True)
            fig = nc_plot.lfp_spectrum_tr(graph_data)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/lfp_spectrum_TR/&#39;, graph_data=graph_data)
            
            # These ranges are from Muessig et al. 2019
            # Coordinated Emergence of Hippocampal Replay and
            # Theta Sequences during Post - natal Development
            self.bandpower_ratio(
                [5, 11], [1.5, 4], 1.6, band_total=True,
                first_name=&#34;Theta&#34;, second_name=&#34;Delta&#34;)
        except:
            logging.error(&#39;Error in analyzing lfp spectrum&#39;)

    if self.get_analysis(&#39;spike_phase&#39;):
        ### Analysis of Phase distribution
        logging.info(&#39;Analysing distribution of spike-phase in lfp...&#39;)
        try:
            params= self.get_params_by_analysis(&#39;spike_phase&#39;)

            graph_data = self.phase_dist(binsize=params[&#39;phase_bin&#39;], \
                                       rbinsize=params[&#39;phase_raster_bin&#39;],\
                                       fwin=[params[&#39;phase_freq_min&#39;], params[&#39;phase_freq_max&#39;]],\
                                       pratio=params[&#39;phase_power_thresh&#39;],\
                                       aratio=params[&#39;phase_amp_thresh&#39;],
                                       filtset=[params[&#39;lfp_prefilt_order&#39;],
                                                params[&#39;lfp_prefilt_lowcut&#39;],
                                                params[&#39;lfp_prefilt_highcut&#39;], &#39;bandpass&#39;])
            fig = nc_plot.spike_phase(graph_data)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/spike_phase/&#39;, graph_data=graph_data)

        except:
            logging.error(&#39;Error in assessing spike-phase distribution&#39;)

    if self.get_analysis(&#39;phase_lock&#39;):
        # PLV with mode = None (all events or spikes)
        logging.info(&#39;Analysis of Phase-locking value and spike-filed coherence...&#39;)
        try:
            params= self.get_params_by_analysis(&#39;phase_lock&#39;)

            reparam = {&#39;window&#39; : [params[&#39;phase_loc_win_low&#39;], params[&#39;phase_loc_win_up&#39;]],
                       &#39;nfft&#39;: params[&#39;phase_loc_nfft&#39;],
                       &#39;fwin&#39;: [2, params[&#39;phase_loc_freq_max&#39;]],
                       &#39;nsample&#39;: 2000,
                       &#39;slide&#39;: 25,
                       &#39;nrep&#39;: 500,
                       &#39;mode&#39;: &#39;tr&#39;}

            graph_data = self.plv(**reparam)
            fig = nc_plot.plv_tr(graph_data)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/phase_lock_TR/&#39;, graph_data=graph_data)

            reparam.update({&#39;mode&#39;: &#39;bs&#39;, &#39;nsample&#39;: 100})
            graph_data = self.plv(**reparam)
            fig = nc_plot.plv_bs(graph_data)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/phase_lock_BS/&#39;, graph_data=graph_data)

            reparam.update({&#39;mode&#39;: None})
            graph_data = self.plv(**reparam)
            fig = nc_plot.plv(graph_data)
            self.close_fig(fig)
            self.plot_data_to_hdf(name=name+ &#39;/phase_lock/&#39;, graph_data=graph_data)
        except:
            logging.error(&#39;Error in spike-phase locking analysis&#39;)

        if self.get_analysis(&#39;lfp_spike_causality&#39;):
            logging.warning(&#39;Unit-LFP analysis has not been implemented yet!&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.exist_hdf_path"><code class="name flex">
<span>def <span class="ident">exist_hdf_path</span></span>(<span>self, path='')</span>
</code></dt>
<dd>
<section class="desc"><p>Check and returns if an HDF5 file path exists</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>path to HDF5 file group</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>exists</code></strong> :&ensp;<code>bool</code></dt>
<dd>True if the path exists</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def exist_hdf_path(self, path=&#39;&#39;):
    &#34;&#34;&#34;
    Check and returns if an HDF5 file path exists
    
    Parameters
    ----------
    path : str
        path to HDF5 file group
    
    Returns
    -------
    exists : bool
        True if the path exists

    &#34;&#34;&#34;
    
    exists= False
    if path in self.f:
        exists = True
        return exists</code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.finished"><code class="name flex">
<span>def <span class="ident">finished</span></span>(<span>...)</span>
</code></dt>
<dd>
<section class="desc"></section>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.get_configuration"><code class="name flex">
<span>def <span class="ident">get_configuration</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the Configuration() object from this class.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Configuration</code></dt>
<dd>NeuroChaT's config attribute</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_configuration(self):
    &#34;&#34;&#34;
    Returns the Configuration() object from this class.
    
    Parameters
    ----------
    None
    
    Returns
    -------
    Configuration
        NeuroChaT&#39;s config attribute

    &#34;&#34;&#34;
    
    return self.config</code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.get_hdf_groups"><code class="name flex">
<span>def <span class="ident">get_hdf_groups</span></span>(<span>self, path='')</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the names of groups or datasets in a path </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>path to HDF5 file group</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>Names of the groups or datasets in the path</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_hdf_groups(self, path=&#39;&#39;):
    &#34;&#34;&#34;
    Returns the names of groups or datasets in a path 
    
    Parameters
    ----------
    path : str
        path to HDF5 file group
    
    Returns
    -------
    list
        Names of the groups or datasets in the path

    &#34;&#34;&#34;
    
    return self.hdf.get_groups_in_path(path=path)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.get_neuro_data"><code class="name flex">
<span>def <span class="ident">get_neuro_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the NData() object from this class.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>NData</code></dt>
<dd>NeuroChaT's ndata attribute</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_neuro_data(self):
    &#34;&#34;&#34;
    Returns the NData() object from this class.
    
    Parameters
    ----------
    None
    
    Returns
    -------
    NData
        NeuroChaT&#39;s ndata attribute

    &#34;&#34;&#34;
    return self.ndata</code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.get_output_files"><code class="name flex">
<span>def <span class="ident">get_output_files</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns a DataFrame of output graphic files and HDF5 files after the completion of the analysis.
Index are the unit IDs of the analysed units.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>op_files</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Column 1 contains the name of the output graphic files. Column 2 gives the
the name of the NWB files</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_output_files(self):
    &#34;&#34;&#34;
    Returns a DataFrame of output graphic files and HDF5 files after the completion of the analysis.
    Index are the unit IDs of the analysed units.
    
    Parameters
    ----------
    None
    
    Returns
    -------
    op_files : pandas.DataFrame
        Column 1 contains the name of the output graphic files. Column 2 gives the
        the name of the NWB files
    
    &#34;&#34;&#34;
    
    op_files = {&#39;Graphics Files&#39;: self.graphic_files,
                &#39;NWB Files&#39;: self.nwb_files}
    op_files = pd.DataFrame.from_dict(op_files)
    op_files.index = self.cellid
    op_files = op_files[[&#39;Graphics Files&#39;, &#39;NWB Files&#39;]]

    return op_files</code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.get_results"><code class="name flex">
<span>def <span class="ident">get_results</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the parametric results of the analyses.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>results</code></strong> :&ensp;<code>OrderedDict</code></dt>
<dd>Parametric results of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_results(self):
    &#34;&#34;&#34;
    Returns the parametric results of the analyses.
    
    Parameters
    ----------
    None
    
    Returns
    -------
    results : OrderedDict
        Parametric results of the analysis
        
    &#34;&#34;&#34;
    try:
        keys = []
        for d in self.results:
            [keys.append(k) for k in list(d.keys()) if k not in keys]
        results = pd.DataFrame(self.results, columns=keys)
        results.index = self.cellid
    except Exception as ex:
        log_exception(
            ex, &#34;Error in getting results&#34;)
    
    return results</code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.mode"><code class="name flex">
<span>def <span class="ident">mode</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Reads the specifications and analyzes data according to the mode that is set
in the Configuration file. Thsi is the principle method in NeuroChaT that
sets the input and output data files and calls the execute() method for
running the analyses after it sets the data and filenames to NData() object.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def mode(self):
    &#34;&#34;&#34;
    Reads the specifications and analyzes data according to the mode that is set
    in the Configuration file. Thsi is the principle method in NeuroChaT that
    sets the input and output data files and calls the execute() method for 
    running the analyses after it sets the data and filenames to NData() object.
    
    Parameters
    ----------
    None
    
    Returns
    -------
    None
        
    &#34;&#34;&#34;
    
    info = {&#39;spat&#39;: [], &#39;spike&#39;: [], &#39;unit&#39;: [], &#39;lfp&#39;: [], &#39;nwb&#39;: [], &#39;graphics&#39;: [], &#39;cellid&#39;: []}
    mode_id = self.get_analysis_mode()[1]
    # All the cells in the same tetrode will use the same lfp channel
    if mode_id == 0 or mode_id == 1: 
        spatial_file = self.get_spatial_file()
        spike_file = self.get_spike_file()
        lfp_file = self.get_lfp_file()

        self.ndata.set_spike_file(spike_file)
        self.ndata.load_spike()

        units = [self.get_unit_no()] if mode_id == 0 else self.ndata.get_unit_list()
        if not units:
            logging.error(&#39;No unit found in analysis&#39;)
        else:
            for unit_no in units:
                info[&#39;spat&#39;].append(spatial_file)
                info[&#39;spike&#39;].append(spike_file)
                info[&#39;unit&#39;].append(unit_no)
                info[&#39;lfp&#39;].append(lfp_file)

    elif mode_id == 2:
        excel_file = self.get_excel_file()
        if os.path.exists(excel_file):
            excel_info = pd.read_excel(excel_file)
            for row in excel_info.itertuples():
                spike_file = row[1] + os.sep + row[3]
                unit_no = int(row[4])
                lfp_id = str(row[5])

                if self.get_data_format() == &#39;Axona&#39;:
                    end = &#34;&#34; if row[2][-4:] == &#34;.txt&#34; else &#34;.txt&#34;
                    spatial_file = row[1] + os.sep + row[2] + end
                    lfp_file = remove_extension(spike_file) + lfp_id

                elif self.get_data_format() == &#39;Neuralynx&#39;:
                    spatial_file = row[1] + os.sep+ row[2]+ &#39;.nvt&#39;
                    lfp_file = row[1]+ os.sep+ lfp_id+ &#39;.ncs&#39;

                elif self.get_data_format() == &#39;NWB&#39;:
                    # excel list: directory| hdf5 file name w/o extension| spike group| unit_no| lfp group
                    hdf_name = row[1] + os.sep+ row[2]+ &#39;.hdf5&#39;
                    spike_file = hdf_name+ &#39;/processing/Shank/&#39; + row[3]
                    spatial_file = hdf_name+ &#39;+/processing/Behavioural/Position&#39;
                    lfp_file = hdf_name+ &#39;+/processing/Neural Continuous/LFP/&#39; + lfp_id

                info[&#39;spat&#39;].append(spatial_file)
                info[&#39;spike&#39;].append(spike_file)
                info[&#39;unit&#39;].append(unit_no)
                info[&#39;lfp&#39;].append(lfp_file)

    if info[&#39;unit&#39;]:
        for i, unit_no in enumerate(info[&#39;unit&#39;]):
            logging.info(&#39;Starting a new unit...&#39;)
            self.ndata.set_spatial_file(info[&#39;spat&#39;][i])
            self.ndata.set_spike_file(info[&#39;spike&#39;][i])
            self.ndata.set_lfp_file(info[&#39;lfp&#39;][i])
            self.ndata.load()
            self.ndata.set_unit_no(info[&#39;unit&#39;][i])

            self.ndata.reset_results()

            cell_id = self.hdf.resolve_analysis_path(spike=self.ndata.spike, lfp=self.ndata.lfp)
            nwb_name = self.hdf.resolve_hdfname(data=self.ndata.spike)
            pdf_name = (
                remove_extension(nwb_name, keep_dot=False) +
                &#39;_&#39; + cell_id+ &#39;.&#39; + self.get_graphic_format())

            info[&#39;nwb&#39;].append(nwb_name)
            info[&#39;cellid&#39;].append(cell_id)
            info[&#39;graphics&#39;].append(pdf_name)

            self.open_pdf(pdf_name)

            fig = plt.figure()
            ax = fig.add_subplot(111)
            ax.text(0.1, 0.6, &#39;Cell ID = &#39;+ cell_id+ &#39;\n&#39;+ \
                    &#39;HDF5 file = &#39;+ nwb_name.split(os.sep)[-1]+ &#39;\n&#39;+ \
                    &#39;Graphics file = &#39;+ pdf_name.split(os.sep)[-1], \
                    horizontalalignment=&#39;left&#39;, \
                    verticalalignment=&#39;center&#39;,\
                    transform=ax.transAxes,
                    clip_on=True)
            ax.set_axis_off()
            self.close_fig(fig)

            # Set and open hdf5 file for saving graph data within self.execute()
            self.hdf.set_filename(nwb_name)
            if &#39;/analysis/&#39;+ cell_id in self.hdf.f:
                del self.hdf.f[&#39;/analysis/&#39;+ cell_id]
            self.execute(name=cell_id)

            self.close_pdf()

            _results = self.ndata.get_results()

            self.update_results(_results)
            self.hdf.save_dict_recursive(path=&#39;/analysis/&#39;+ cell_id+ &#39;/&#39;, name=&#39;results&#39;, data=_results)

            self.hdf.close()
            self.ndata.save_to_hdf5() # Saving data to hdf file

            self.__count += 1
            logging.info(&#39;Units already analyzed = &#39; + str(self.__count))

    logging.info(&#39;Total cell analyzed: &#39;+ str(self.__count))
    self.cellid = info[&#39;cellid&#39;]
    self.nwb_files = info[&#39;nwb&#39;]
    self.graphic_files = info[&#39;graphics&#39;]</code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.open_hdf_file"><code class="name flex">
<span>def <span class="ident">open_hdf_file</span></span>(<span>self, filename=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Sets the filename and opens the file object for the HDF5 file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename of the HDF5 object</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def open_hdf_file(self, filename=None):
    &#34;&#34;&#34;
    Sets the filename and opens the file object for the HDF5 file.
    
    Parameters
    ----------
    filename : str
        Filename of the HDF5 object
    
    Returns
    -------
    None
        
    &#34;&#34;&#34;
    if not filename:
        filename = self.config.get_nwb_file()
        
    self.hdf.set_filename(filename=filename)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.open_pdf"><code class="name flex">
<span>def <span class="ident">open_pdf</span></span>(<span>self, filename=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Opens the PDF file object using PdfPages from matplotlib.backends.backend_pdf</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename of the PDF output</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def open_pdf(self, filename=None):
    &#34;&#34;&#34;
    Opens the PDF file object using PdfPages from matplotlib.backends.backend_pdf
    
    Parameters
    ----------
    filename : str
        Filename of the PDF output
    
    Returns
    -------
    None
        
    &#34;&#34;&#34;
    
    if filename is not None:
        words = filename.split(os.sep)
        directory = os.sep.join(words[:-1])
        if os.path.exists(directory):
            self._pdf_file = filename # Current PDF file being handled
            try:
                self.pdf = PdfPages(self._pdf_file)
                self.save_to_file = True
            except PermissionError:
                logging.error(
                    &#34;Please close PDF with name {} before writing to it&#34;.format(
                        self._pdf_file))
                self.save_to_file = False
                self._pdf_file = None
        else:
            self.save_to_file = False
            self._pdf_file = None
            logging.error(&#39;Cannot create PDF, file path is invalid&#39;)
    else:
        logging.error(&#39;No valid PDf file is specified&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.place_cell_plots"><code class="name flex">
<span>def <span class="ident">place_cell_plots</span></span>(<span>self, directory, dpi=400)</span>
</code></dt>
<dd>
<section class="desc"><p>Plot png images of place cell figures, looping over a directory.</p>
<p>Currently only works for axona files, but can be extended.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dir</code></strong> :&ensp;<code>str</code></dt>
<dd>The directory to get files from.</dd>
<dt><strong><code>dpi</code></strong> :&ensp;<code>int</code></dt>
<dd>The desired dpi of the pngs.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def place_cell_plots(self, directory, dpi=400):
    &#34;&#34;&#34;
    Plot png images of place cell figures, looping over a directory.

    Currently only works for axona files, but can be extended.

    Parameters
    ----------
    dir : str
        The directory to get files from.
    dpi : int
        The desired dpi of the pngs.

    &#34;&#34;&#34;
    try:
        container = NDataContainer(load_on_fly=True)
        container.add_axona_files_from_dir(
            directory, tetrode_list = [i for i in range(1, 17)])
        nca.place_cell_summary(
            container, dpi=dpi, 
            filter_place_cells=False, filter_low_freq=False)
    except Exception as ex:
        log_exception(ex, &#34;In walking a directory for place cell summaries&#34;)
    return </code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.plot_data_to_hdf"><code class="name flex">
<span>def <span class="ident">plot_data_to_hdf</span></span>(<span>self, name=None, graph_data=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Stores plot data to the HDF5 file in the '/analysis/' path</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Unit ID which is also the name of the group in the
'/analysis/' path</dd>
<dt><strong><code>graph_data</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary of data that are plotted</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def plot_data_to_hdf(self, name=None, graph_data=None):
    &#34;&#34;&#34;
    Stores plot data to the HDF5 file in the &#39;/analysis/&#39; path
    
    Parameters
    ----------
    name : str
        Unit ID which is also the name of the group in the  &#39;/analysis/&#39; path
    graph_data : dict
        Dictionary of data that are plotted
    
    Returns
    -------
    None

    &#34;&#34;&#34;
    
    self.hdf.save_dict_recursive(path=&#39;/analysis/&#39;, \
         name=name, data=graph_data)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Reset NeuroChaT's internal attributes and prepares it for another set of
analysis or new session.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def reset(self):
    &#34;&#34;&#34;
    Reset NeuroChaT&#39;s internal attributes and prepares it for another set of
    analysis or new session.
    
    Parameters
    ----------
    None
    
    Returns
    -------
    None
    
    &#34;&#34;&#34;
    
    self.__count = 0
    self.nwb_files = []
    self.graphic_files = []
    self.cellid = []
    self.results = []
    self.save_to_file = False
    self._pdf_file = None
    
    if not self.get_graphic_format():
        self.set_graphic_format(&#39;PDF&#39;)
    nc_plot.set_backend(self.get_graphic_format())</code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>After calling start(), the NeuroChaT thread calls this function. It
verifies the input specifications and calls the mode() method.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def run(self):
    &#34;&#34;&#34;
    After calling start(), the NeuroChaT thread calls this function. It
    verifies the input specifications and calls the mode() method.
    
    Parameters
    ----------
    None
    
    Returns
    -------
    None
        
    &#34;&#34;&#34;


    self.reset()
    verified = True
    # Deduce the configuration
    # Same filename for spike, lfp and spatial will go for NWB
    if not any(self.get_analysis(&#39;all&#39;)):
        verified = False
        # Handle menu functions
        special_analysis = self.get_special_analysis()
        if special_analysis:
            key = special_analysis[&#34;key&#34;]
            logging.info(&#34;Starting special analysis {}&#34;.format(
                key))
            if key == &#34;place_cell_plots&#34;:
                self.place_cell_plots(
                    special_analysis[&#34;directory&#34;],
                    special_analysis[&#34;dpi&#34;])
            elif key == &#34;angle_calculation&#34;:
                self.open_pdf(special_analysis[&#34;pdf_name&#34;])
                self.angle_calculation(special_analysis[&#34;excel_file&#34;])
                self.close_pdf()
            else:
                logging.error(&#39;No analysis method has been selected&#39;)
    else:
        # Could take this to mode, 
        # but replication would occur for each data format
        mode_id = self.get_analysis_mode()[1]
        if (mode_id == 0 or mode_id == 1) and \
            (self.get_data_format() == &#39;Axona&#39; or self.get_data_format() == &#39;Neuralynx&#39;):
            if not os.path.isfile(self.get_spike_file()):
                verified = False
                logging.error(&#39;Spike file does not exist&#39;)

            if not os.path.isfile(self.get_spatial_file()):
                logging.warning(&#39;Position file does not exist&#39;)
        elif mode_id == 2:
            if not os.path.isfile(self.get_excel_file()):
                verified = False
                logging.error(&#39;Excel file does not exist&#39;)

    if verified:
        self.__count = 0
        self.ndata.set_data_format(self.get_data_format())
        self.mode()
    self.finished.emit()</code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.set_configuration"><code class="name flex">
<span>def <span class="ident">set_configuration</span></span>(<span>self, config)</span>
</code></dt>
<dd>
<section class="desc"><p>Sets a new Configuration() object or its subclass object.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>config</code></strong> :&ensp;<code>Configuration</code></dt>
<dd>Object of Configuration class or its subclass.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_configuration(self, config):
    &#34;&#34;&#34;
    Sets a new Configuration() object or its subclass object.
    
    Parameters
    ----------
    config : Configuration
        Object of Configuration class or its subclass.
    
    Returns
    -------
    None

    &#34;&#34;&#34;
    
    if inspect.isclass(config):
        config = config()
    if isinstance(config, Configuration):
        self.config = config
    else:
        logging.warning(&#39;Inappropriate Configuration object or class&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.set_neuro_data"><code class="name flex">
<span>def <span class="ident">set_neuro_data</span></span>(<span>self, ndata)</span>
</code></dt>
<dd>
<section class="desc"><p>Sets a new NData() object or its subclass object.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ndata</code></strong> :&ensp;<code>NData</code></dt>
<dd>Object of NData class or its subclass.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_neuro_data(self, ndata):
    &#34;&#34;&#34;
    Sets a new NData() object or its subclass object.
    
    Parameters
    ----------
    ndata : NData
        Object of NData class or its subclass.
    
    Returns
    -------
    None

    &#34;&#34;&#34;
    
    if inspect.isclass(ndata):
        ndata = ndata()
    if isinstance(ndata, NData):
        self.ndata = ndata
    else:
        logging.warning(&#39;Inappropriate NeuroData object or class&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.update_results"><code class="name flex">
<span>def <span class="ident">update_results</span></span>(<span>self, _results)</span>
</code></dt>
<dd>
<section class="desc"><p>Updates the results with new analysis results.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>_results</code></strong> :&ensp;<code>OrderedDict</code></dt>
<dd>Dictionary of the new results</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def update_results(self, _results):
    &#34;&#34;&#34;
    Updates the results with new analysis results.
    
    Parameters
    ----------
    _results : OrderedDict
        Dictionary of the new results
    
    Returns
    -------
    None
    
    &#34;&#34;&#34;
    
    self.results.append(_results.copy()) # without copy, list contains a reference to the original dictionary, and old results are replaced by the new one</code></pre>
</details>
</dd>
<dt id="neurochat.nc_control.NeuroChaT.verify_units"><code class="name flex">
<span>def <span class="ident">verify_units</span></span>(<span>self, excel_file=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Takes a list of datasets and verify the specifications of the units.
The verification tool is useful for prescreening of units before the
batch-mode analysis using 'Listed Units' mode of NeuroChaT</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>excel_file</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the excel file that contains data specifications</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def verify_units(self, excel_file=None):
    &#34;&#34;&#34;
    Takes a list of datasets and verify the specifications of the units.
    The verification tool is useful for prescreening of units before the 
    batch-mode analysis using &#39;Listed Units&#39; mode of NeuroChaT
    
    Parameters
    ----------
    excel_file : str
        Name of the excel file that contains data specifications
    
    Returns
    -------
    None

    &#34;&#34;&#34;
    
    info = {&#39;spike&#39;: [], &#39;unit&#39;: []}
    if os.path.exists(excel_file):
        excel_info = pd.read_excel(excel_file)
        for row in excel_info.itertuples():
            spike_file = row[1]+ os.sep+ row[3]
            unit_no = int(row[4])
            if self.get_data_format() == &#39;NWB&#39;:
            # excel list: directory| spike group| unit_no
                hdf_name = row[1] + os.sep+ row[3]+ &#39;.hdf5&#39;
                spike_file = hdf_name+ &#39;/processing/Shank&#39;+ &#39;/&#39;+ row[4]
            info[&#39;spike&#39;].append(spike_file)
            info[&#39;unit&#39;].append(unit_no)
        n_units = excel_info.shape[0]

        excel_info = excel_info.assign(fileExists=pd.Series(np.zeros(n_units, dtype=bool)))
        excel_info = excel_info.assign(unitExists=pd.Series(np.zeros(n_units, dtype=bool)))

        if info[&#39;spike&#39;]:
            for i, spike_file in enumerate(info[&#39;spike&#39;]):

                logging.info(&#39;Verifying unit: &#39;+ str(i+ 1))
                if os.path.exists(spike_file):
                    excel_info.loc[i, &#39;fileExists&#39;] = True
                    self.ndata.set_spike_file(spike_file)
                    self.ndata.load_spike()
                    units = self.ndata.get_unit_list()

                    if info[&#39;unit&#39;][i] in units:
                        excel_info.loc[i, &#39;unitExists&#39;] = True

        excel_info.to_excel(excel_file, index=False)
        logging.info(&#39;Verification process completed!&#39;)
    else:
        logging.error(&#39;Excel  file does not exist!&#39;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="neurochat" href="index.html">neurochat</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="neurochat.nc_control.NeuroChaT" href="#neurochat.nc_control.NeuroChaT">NeuroChaT</a></code></h4>
<ul class="two-column">
<li><code><a title="neurochat.nc_control.NeuroChaT.angle_calculation" href="#neurochat.nc_control.NeuroChaT.angle_calculation">angle_calculation</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.close_fig" href="#neurochat.nc_control.NeuroChaT.close_fig">close_fig</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.close_hdf_file" href="#neurochat.nc_control.NeuroChaT.close_hdf_file">close_hdf_file</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.close_pdf" href="#neurochat.nc_control.NeuroChaT.close_pdf">close_pdf</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.cluster_evaluate" href="#neurochat.nc_control.NeuroChaT.cluster_evaluate">cluster_evaluate</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.cluster_similarity" href="#neurochat.nc_control.NeuroChaT.cluster_similarity">cluster_similarity</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.convert_to_nwb" href="#neurochat.nc_control.NeuroChaT.convert_to_nwb">convert_to_nwb</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.execute" href="#neurochat.nc_control.NeuroChaT.execute">execute</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.exist_hdf_path" href="#neurochat.nc_control.NeuroChaT.exist_hdf_path">exist_hdf_path</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.finished" href="#neurochat.nc_control.NeuroChaT.finished">finished</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.get_configuration" href="#neurochat.nc_control.NeuroChaT.get_configuration">get_configuration</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.get_hdf_groups" href="#neurochat.nc_control.NeuroChaT.get_hdf_groups">get_hdf_groups</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.get_neuro_data" href="#neurochat.nc_control.NeuroChaT.get_neuro_data">get_neuro_data</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.get_output_files" href="#neurochat.nc_control.NeuroChaT.get_output_files">get_output_files</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.get_results" href="#neurochat.nc_control.NeuroChaT.get_results">get_results</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.mode" href="#neurochat.nc_control.NeuroChaT.mode">mode</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.open_hdf_file" href="#neurochat.nc_control.NeuroChaT.open_hdf_file">open_hdf_file</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.open_pdf" href="#neurochat.nc_control.NeuroChaT.open_pdf">open_pdf</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.place_cell_plots" href="#neurochat.nc_control.NeuroChaT.place_cell_plots">place_cell_plots</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.plot_data_to_hdf" href="#neurochat.nc_control.NeuroChaT.plot_data_to_hdf">plot_data_to_hdf</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.reset" href="#neurochat.nc_control.NeuroChaT.reset">reset</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.run" href="#neurochat.nc_control.NeuroChaT.run">run</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.set_configuration" href="#neurochat.nc_control.NeuroChaT.set_configuration">set_configuration</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.set_neuro_data" href="#neurochat.nc_control.NeuroChaT.set_neuro_data">set_neuro_data</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.update_results" href="#neurochat.nc_control.NeuroChaT.update_results">update_results</a></code></li>
<li><code><a title="neurochat.nc_control.NeuroChaT.verify_units" href="#neurochat.nc_control.NeuroChaT.verify_units">verify_units</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.6.3</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>