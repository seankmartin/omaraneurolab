<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.6.3" />
<title>neurochat.nc_spike API documentation</title>
<meta name="description" content="This module implements NSpike Class for NeuroChaT software â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>neurochat.nc_spike</code></h1>
</header>
<section id="section-intro">
<p>This module implements NSpike Class for NeuroChaT software</p>
<p>@author: Md Nurul Islam; islammn at tcd dot ie</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
&#34;&#34;&#34;
This module implements NSpike Class for NeuroChaT software

@author: Md Nurul Islam; islammn at tcd dot ie
&#34;&#34;&#34;

import os

import re
#from imp import reload

import logging
from collections import OrderedDict as oDict
from copy import deepcopy

import numpy as np

#import nc_utils
#reload(nc_utils)
from neurochat.nc_utils import extrema, find, residual_stat

#from nc_lfp import NLfp
from neurochat.nc_hdf import Nhdf
from neurochat.nc_base import NBase

from scipy.optimize import curve_fit


class NSpike(NBase):
    &#34;&#34;&#34;
    This data class is the placeholder for the dataset that contains information
    about the neural spikes. It decodes data from different formats and analyses
    single units in the recording.
     
    &#34;&#34;&#34;
     
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._unit_no = kwargs.get(&#39;unit_no&#39;, 0)
        self._unit_stamp = []
        self._timestamp = []
        self._unit_list = []
        self._unit_Tags = []
        self._waveform = []
        self.set_record_info({&#39;Timebase&#39;: 1,
                              &#39;Samples per spike&#39;: 1,
                              &#39;No of spikes&#39;: 0,
                              &#39;Channel IDs&#39;: None})
        
        self.__type = &#39;spike&#39;
        
    def get_type(self):
        &#34;&#34;&#34;
        Returns the type of object. For NSpike, this is always `spike` type
        
        Parameters
        ----------
        None
        
        Returns
        -------
        str

        &#34;&#34;&#34;
        
        return self.__type 
    
    def get_unit_tags(self):
        
        &#34;&#34;&#34;
        Returns the unit number or tags of the clustered units
        
        Parameters
        ----------
        None
        
        Returns
        -------
        list ot ndarray

        &#34;&#34;&#34;
        
        return self._unit_Tags
    
    def set_unit_tags(self, new_tags):
        
        &#34;&#34;&#34;
        Sets the number or tags of the clustered units
        
        Parameters
        ----------
        new_tags : list or ndarray
            Tags for each spiking wave 
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if len(new_tags) == len(self._timestamp):
            self._unit_Tags = new_tags
            self._set_unit_list()
        else:
            logging.error(&#39;No of tags spikes does not match with no of spikes&#39;)

    def get_unit_list(self):
        &#34;&#34;&#34;
        Gets the list of the units
        
        Parameters
        ----------
        None
        
        Returns
        -------
        list
            List of the unique tags of spiking-waveforms from clustering

        &#34;&#34;&#34;
        
        return self._unit_list
    
    def _set_unit_list(self):
        &#34;&#34;&#34;
        Sets the list of units from the list of unit tags
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        self._unit_list = list(map(int, set(self._unit_Tags)))
        if 0 in self._unit_list:
            self._unit_list.remove(0)

    def set_unit_no(self, unit_no=None, spike_name=None):
        &#34;&#34;&#34;
        Sets the unit number of the spike dataset to analyse
        
        Parameters
        ----------
        unit_no : int
            Unit or cell number to analyse
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if isinstance(unit_no, int):
            if unit_no in self.get_unit_list():
                self._unit_no = unit_no
                self._set_unit_stamp()
        else:
            if spike_name is None:
                spike_name = self.get_spike_names()
            if len(unit_no) == len(spike_name):
                spikes = self.get_spike(spike_name)
                for i, num in enumerate(unit_no):
                    if num in spikes[i].get_unit_list():
                        spikes[i].set_unit_no(num)
            else:
                logging.error(&#39;Unit no. to set are not as many as child spikes!&#39;)

    def get_unit_no(self, spike_name=None):
        &#34;&#34;&#34;
        Gets currently set unit number of the spike dataset to analyse
        
        Parameters
        ----------
        None
        
        Returns
        -------
        int
            Unit or cell number set to analyse

        &#34;&#34;&#34;
        if spike_name is None:
            unit_no = self._unit_no
        else:
            unit_no = []
            spikes = self.get_spike(spike_name)
            for spike in spikes:
                unit_no.append(spike._unit_no)
        return unit_no

    def get_timestamp(self, unit_no=None):
        &#34;&#34;&#34;
        Returns the timestamps of the spike-waveforms of spefied unit
        
        Parameters
        ----------
        None
        
        Returns
        -------
        ndarray
            Timestamps of the spiking waveforms
        &#34;&#34;&#34;
        
        if unit_no is None:
            return self._timestamp
        else:
            if unit_no in self._unit_list:
                return self._timestamp[self._unit_Tags == unit_no]
            else:
                logging.warning(&#39;Unit &#39; + str(unit_no) + &#39; is not present in the spike data&#39;)

    def _set_timestamp(self, timestamp=None):
        &#34;&#34;&#34;
        Sets the timestamps for all spiking waveforms in the recording
        
        Parameters
        ----------
        timestamp : list or ndarray
            Timestamps of all spiking waveforms
            
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if timestamp is not None:
            self._timestamp = timestamp

    def get_unit_stamp(self):
        &#34;&#34;&#34;
        Gets the timestamps for currently set unit to analyse
        
        Parameters
        ----------
        None
        
        Returns
        -------
        list or ndarray
            Timestamps for currently set unit

        &#34;&#34;&#34;
        
        return self.get_timestamp(self._unit_no)

    def _set_unit_stamp(self):
        &#34;&#34;&#34;
        Sets timestamps of the unit currently set to analyse
        
        Parameters
        ----------
        None
        
        Returns
        -------
        int
            Unit or cell number set to analyse

        &#34;&#34;&#34;
        
        self._unit_stamp = self.get_unit_stamp()

    def get_unit_spikes_count(self, unit_no=None):
        &#34;&#34;&#34;
        Returns the number of spikes in a unit
        
        Parameters
        ----------
        unit_no : int
            Units whose spike count is returned
        
        Returns
        -------
        int
            Number of units spikes of a unit in a recording session

        &#34;&#34;&#34;
        
        if unit_no is None:
            unit_no = self._unit_no
        if unit_no in self._unit_list:
            return sum(self._unit_Tags == unit_no)

    def get_waveform(self):
        &#34;&#34;&#34;
        Returns spike-waveforms
        
        Parameters
        ----------
        None
        
        Returns
        -------
        OrderedDict
            Dictionary of spiking waveforms where keys represent the channel number

        &#34;&#34;&#34;
        
        return self._waveform

    def _set_waveform(self, spike_waves=[]):
        &#34;&#34;&#34;
        Sets spike waveform to the NSpike() object
        
        Parameters
        ----------
        spike_waves : OrderedDict
            Spike waveforms where each key represents one channel
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if spike_waves:
            self._waveform = spike_waves

    def get_unit_waves(self, unit_no=None):
        &#34;&#34;&#34;
        Returns spike waveform of a specified unit
        
        Parameters
        ----------
        unit_no : int
            Unit whose waveforms are to be returned
        
        Returns
        -------
        OrderedDict
            Waveforms of the specified unit. If None, waveforms of currently set
            unit are returned

        &#34;&#34;&#34;
        
        if unit_no is None:
            unit_no = self._unit_no
        _waves = oDict()
        for chan, wave in self._waveform.items():
            _waves[chan] = wave[self._unit_Tags == unit_no, :]
        
        return _waves

    def get_unit_stamps_in_ranges(self, ranges):
        &#34;&#34;&#34;
        Return the unit timestamps in a list of ranges.

        Parameters
        ----------
        ranges : list
            A list of tuples indicating time ranges to get stamps in
        
        Returns
        -------
        list
            The timestamps
        &#34;&#34;&#34;
        stamps = self.get_unit_stamp()
        new_stamps = [
            val for val in stamps
            if any(lower &lt;= val &lt;= upper for (lower, upper) in ranges)
        ]
        return new_stamps

    def subsample(self, sample_range=None):
        &#34;&#34;&#34;
        Extract a time range from the spikes.
        
        Parameters
        ----------
        sample_range : tuple
            the time in seconds to extract from the spikes
        
        Returns
        -------
        NSpike
            subsampled version of initial spike object
        &#34;&#34;&#34;
        if sample_range is None:
            return self
        new_spike = deepcopy(self)
        stamps = self.get_timestamp()
        lower, upper = sample_range
        sample_spike_idxs = (
            (stamps &lt;= upper) &amp; (stamps &gt;= lower)).nonzero()
        new_spike_times = stamps[sample_spike_idxs]
        new_tags = self.get_unit_tags()[sample_spike_idxs]
        new_waveform = new_spike.get_waveform()
        for ch in new_waveform.keys():
            new_waveform[ch] = new_waveform[ch][sample_spike_idxs, :].squeeze()
        new_spike._set_timestamp(new_spike_times)
        new_spike.set_unit_tags(new_tags)
        new_spike._set_waveform(new_waveform)
        new_spike._set_duration(upper - lower)
        return new_spike

    def load(self, filename=None, system=None):
        &#34;&#34;&#34;
        Loads spike datasets
        
        Parameters
        ----------
        filename : str
            Name of the spike datafile
        system : str
            Recording system or format of the spike data file
        
        Returns
        -------
        None
        
        See also
        --------
        load_spike_axona(), load_spike_NLX(), load_spike_NWB()
            
        &#34;&#34;&#34;
        
        if system is None:
            system = self._system
        else:
            self._system = system
        if filename is None:
            filename = self._filename
        else:
            self._filename = filename
        loader = getattr(self, &#39;load_spike_&#39;+ system)
        loader(filename)

    def add_spike(self, spike=None, **kwargs):
        &#34;&#34;&#34;
        Adds new spike node to current NSpike() object
        
        Parameters
        ----------
        spike : NSpike
            NSPike object. If None, new object is created
        
        Returns
        -------
        `:obj:NSpike`
            A new NSpike() object

        &#34;&#34;&#34;
        new_spike = self._add_node(self.__class__, spike, &#39;spike&#39;, **kwargs)
        
        return new_spike

    def load_spike(self, names=None):
        &#34;&#34;&#34;
        Loads datasets of the spike nodes. Name of each node is used for obtaining the
        filenames
        
        Parameters
        ----------
        names : list of str
            Names of the nodes to load. If None, current NSpike() object is loaded
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if names is None:
            self.load()
        elif names == &#39;all&#39;:
            for spike in self._spikes:
                spike.load()
        else:
            logging.error(&#34;Spikes by name has yet to be implemented&#34;)
            # for name in names:
            #     spike = self.get_spikes_by_name(name)
            #     spike.load()

    def add_lfp(self, lfp=None, **kwargs):
        &#34;&#34;&#34;
        Adds new LFP node to current NSpike() object
        
        Parameters
        ----------
        lfp : NLfp
            NLfp object. If None, new object is created
        
        Returns
        -------
        `:obj:Nlfp`
            A new NLfp() object

        &#34;&#34;&#34;
        
        try:
            data_type = lfp.get_type()
        except:
            logging.error(&#39;The data type of the added object cannot be determined!&#39;)

        if data_type == &#39;lfp&#39;:
                cls= lfp.___class__ 
        else:
            cls = None

        new_lfp = self._add_node(cls, lfp, &#39;lfp&#39;, **kwargs)
        
        return new_lfp

    def load_lfp(self, names=&#39;all&#39;):
        &#34;&#34;&#34;
        Loads datasets of the LFP nodes. Name of each node is used for obtaining the
        filenames
        
        Parameters
        ----------
        names : list of str
            Names of the nodes to load. If `all`, all LFP nodes are loaded
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if names == &#39;all&#39;:
            for lfp in self._lfp:
                lfp.load()
        else:
            logging.error(&#34;Lfp by name has yet to be implemented&#34;)
            # for name in names:
            #     lfp = self.get_lfp_by_name(name)
            #     lfp.load()

    def wave_property(self):
        &#34;&#34;&#34;
        Claulates different waveform properties for currently set unit
        
        Parameters
        ----------
        None
 
        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;

        _result = oDict()
        graph_data = {}

        def argpeak(data):
            data = np.array(data)
            peak_loc = [j for j in range(7, len(data)) \
                        if data[j] &lt;= 0 and data[j - 1] &gt; 0]
            return peak_loc[0] if peak_loc else 0

        def argtrough1(data, peak_loc):
            data = data.tolist()
            trough_loc = [peak_loc - j for j in range(peak_loc - 2) \
                        if data[peak_loc - j] &gt;= 0 and data[peak_loc - j - 1] &lt;= 0]
            return trough_loc[0] if trough_loc else 0

        def wave_width(wave, peak, thresh=0.25):
            p_loc, p_val = peak
            Len = wave.size
            if p_loc:
                w_start = find(wave[:p_loc] &lt;= thresh*p_val, 1, &#39;last&#39;)
                w_start = w_start[0] if w_start.size else 0
                w_end = find(wave[p_loc:] &lt;= thresh*p_val, 1, &#39;first&#39;)
                w_end = p_loc + w_end[0] if w_end.size else Len
            else:
                w_start = 1
                w_end = Len

            return w_end- w_start

        num_spikes = self.get_unit_spikes_count()
        _result[&#39;Number of Spikes&#39;] = num_spikes
        _result[&#39;Mean Spiking Freq&#39;] = num_spikes/ self.get_duration()
        _waves = self.get_unit_waves()
        samples_per_spike = self.get_samples_per_spike()
        tot_chans = self.get_total_channels()
        meanWave = np.empty([samples_per_spike, tot_chans])
        stdWave = np.empty([samples_per_spike, tot_chans])

        width = np.empty([num_spikes, tot_chans])
        amp = np.empty([num_spikes, tot_chans])
        height = np.empty([num_spikes, tot_chans])
        for i, (chan, wave) in enumerate(_waves.items()):
            meanWave[:, i] = np.mean(wave, 0)
            stdWave[:, i] = np.std(wave, 0)
            slope = np.gradient(wave)[1][:, :-1]
            max_val = wave.max(1)

            peak_val, trough1_val = 0, 0
            if max_val.max() &gt; 0:
                peak_loc = [argpeak(slope[I, :]) for I in range(num_spikes)]
                peak_val = [wave[I, peak_loc[I]] for I in range(num_spikes)]
                trough1_loc = [argtrough1(slope[I, :], peak_loc[I]) for I in range(num_spikes)]
                trough1_val = [wave[I, trough1_loc[I]] for I in range(num_spikes)]
                peak_loc = np.array(peak_loc)
                peak_val = np.array(peak_val)
                trough1_loc = np.array(trough1_loc)
                trough1_val = np.array(trough1_val)
                width[:, i] = np.array([wave_width(wave[I, :], (peak_loc[I], peak_val[I]), 0.25) \
                             for I in range(num_spikes)])

            amp[:, i] = peak_val - trough1_val
            height[:, i] = peak_val - wave.min(1)
        max_chan = amp.mean(0).argmax()
        width = width[:, max_chan]* 10**6/self.get_sampling_rate()
        amp = amp[:, max_chan]
        height = height[:, max_chan]

        graph_data = {&#39;Mean wave&#39;: meanWave, &#39;Std wave&#39;: stdWave,
                      &#39;Amplitude&#39;: amp, &#39;Width&#39;: width, &#39;Height&#39;: height,
                      &#39;Max channel&#39;: max_chan}

        _result.update({&#39;Mean amplitude&#39;: amp.mean(), &#39;Std amplitude&#39;: amp.std(),
                        &#39;Mean height&#39;: height.mean(), &#39;Std height&#39;: height.std(),
                        &#39;Mean width&#39;: width.mean(), &#39;Std width&#39;: width.std()})

        self.update_result(_result)
        
        return graph_data

    def isi(self, bins=&#39;auto&#39;, bound=None, density=False, 
            refractory_threshold=2):
        &#34;&#34;&#34;
        Calulates the ISI histogram of the spike train
        
        Parameters
        ----------
        bins : str or int
            Number of ISI histogram bins. If &#39;auto&#39;, NumPy default is used
            
        bound : int
            Length of the ISI histogram in msec
        density : bool
            If true, normalized historagm is calcultaed
        refractory_threshold : int
            Length of the refractory period in msec

        Returns
        -------
        dict
            Graphical data of the analysis
    
        &#34;&#34;&#34;
        graph_data = oDict()
        _results = oDict()

        unitStamp = self.get_unit_stamp()
        isi = 1000*np.diff(unitStamp)

        below_refractory = isi[isi &lt; refractory_threshold]

        graph_data[&#39;isiHist&#39;], edges = np.histogram(isi, bins=bins, range=bound, density=density)
        graph_data[&#39;isiBins&#39;] = edges[:-1]
        graph_data[&#39;isiBinCentres&#39;] = edges[:-1] + np.mean(np.diff(edges))
        graph_data[&#39;isi&#39;] = isi
        graph_data[&#39;maxCount&#39;] = graph_data[&#39;isiHist&#39;].max()
        graph_data[&#39;isiBefore&#39;] = isi[:-1]
        graph_data[&#39;isiAfter&#39;] = isi[1:]

        _results[&#34;Mean ISI&#34;] = isi.mean()
        _results[&#34;Std ISI&#34;] = isi.std()
        _results[&#34;Refractory violation&#34;] = (
            below_refractory.size / unitStamp.size)

        self.update_result(_results)
        return graph_data

    def isi_corr(self, spike=None, **kwargs):
        &#34;&#34;&#34;
        Calculates the correlation of ISI histogram.
        
        Parameters
        ----------
        spike : NSpike()
            If specified, it calulates cross-correlation.
            
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
    
        &#34;&#34;&#34;
        
        graph_data = oDict()
        if spike is None:
            _unit_stamp = np.copy(self.get_unit_stamp())
        elif isinstance(spike, int):
            if spike in self.get_unit_list():
                _unit_stamp = self.get_timestamp(spike)
        else:
            if isinstance(spike, str):
                spike = self.get_spike(spike)
            if isinstance(spike, self.__class__):
                _unit_stamp = spike.get_unit_stamp()
            else:
                logging.error(&#39;No valid spike specified&#39;)

        _corr = self.psth(_unit_stamp, **kwargs)
        graph_data[&#39;isiCorrBins&#39;] = _corr[&#39;bins&#39;]
        graph_data[&#39;isiAllCorrBins&#39;] = _corr[&#39;all_bins&#39;]
        center = find(_corr[&#39;bins&#39;] == 0, 1, &#39;first&#39;)[0]
        graph_data[&#39;isiCorr&#39;] = _corr[&#39;psth&#39;]
        graph_data[&#39;isiCorr&#39;][center] = graph_data[&#39;isiCorr&#39;][center] \
                                    - np.min([self.get_unit_stamp().size, _unit_stamp.size])

        return graph_data

    def psth(self, event_stamp, **kwargs):
        &#34;&#34;&#34;
        Calculates peri-stimulus time histogram (PSTH)
        
        Parameters
        ----------
        event_stamp : ndarray
            Event timestamps
            
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
    
        &#34;&#34;&#34;
        
        graph_data = oDict()
        bins = kwargs.get(&#39;bins&#39;, 1)
        if isinstance(bins, int):
            bound = np.array(kwargs.get(&#39;bound&#39;, [-500, 500]))
            bins = np.hstack((np.arange(bound[0], 0, bins), np.arange(0, bound[1] + bins, bins)))
        bins = bins/1000 # converted to sec
        n_bins = len(bins) - 1

        hist_count = np.zeros([n_bins, ])
        unitStamp = self.get_unit_stamp()
        for it in range(event_stamp.size):
            tmp_count, edges = np.histogram(unitStamp - event_stamp[it], bins=bins)
            hist_count = hist_count + tmp_count

        graph_data[&#39;psth&#39;] = hist_count
        graph_data[&#39;bins&#39;] = 1000*edges[:-1]

        # Included in case the last point is needed
        graph_data[&#39;all_bins&#39;] = 1000*edges

        return graph_data

    def burst(self, burst_thresh=5, ibi_thresh=50):
        &#34;&#34;&#34;
        Analysis of bursting properties of the spiking train
        
        Parameters
        ----------
        burst_thresh : int
            Minimum ISI between consecutive spikes in a burst
            
        ibi_thresh : int
            Minimum inter-burst interval between two bursting groups of spikes
 
        Returns
        -------
        None
    
        &#34;&#34;&#34;

        _results = oDict()

        unitStamp = self.get_unit_stamp()
        isi = 1000*np.diff(unitStamp)

        burst_start = []
        burst_end = []
        burst_duration = []
        spikesInBurst = []
        bursting_isi = []
        num_burst = 0
        ibi = []
        duty_cycle = []
        k = 0
        while k &lt; isi.size:
            if isi[k] &lt;= burst_thresh:
                burst_start.append(k)
                spikesInBurst.append(2)
                bursting_isi.append(isi[k])
                burst_duration.append(isi[k])
                m = k+1
                while m &lt; isi.size and isi[m] &lt;= burst_thresh:
                    spikesInBurst[num_burst] += 1
                    bursting_isi.append(isi[m])
                    burst_duration[num_burst] += isi[m]
                    m += 1
                burst_duration[num_burst] += 1 # to compensate for the span of the last spike
                burst_end.append(m)
                k = m+1
                num_burst += 1
            else:
                k += 1
        if num_burst:
            for j in range(0, num_burst-1):
                ibi.append(unitStamp[burst_start[j+1]]- unitStamp[burst_end[j]])
            duty_cycle = np.divide(burst_duration[1:], ibi)/1000 # ibi in sec, burst_duration in ms
        else:
            logging.warning(
                &#39;No burst detected in {}&#39;.format(self.get_filename()))

        spikesInBurst = np.array(spikesInBurst) if spikesInBurst else np.array([])
        bursting_isi = np.array(bursting_isi) if bursting_isi else np.array([])
        ibi = 1000*np.array(ibi) if ibi else np.array([]) # in sec unit, so converted to ms
        burst_duration = np.array(burst_duration) if burst_duration else np.array([])
        duty_cycle = np.array(duty_cycle) if len(duty_cycle) else np.array([])

        _results[&#39;Total burst&#39;] = num_burst
        _results[&#39;Total bursting spikes&#39;] = spikesInBurst.sum()
        _results[&#39;Mean bursting ISI ms&#39;] = bursting_isi.mean() if bursting_isi.any() else None
        _results[&#39;Std bursting ISI ms&#39;] = bursting_isi.std() if bursting_isi.any() else None
        _results[&#39;Mean spikes per burst&#39;] = spikesInBurst.mean() if spikesInBurst.any() else None
        _results[&#39;Std spikes per burst&#39;] = spikesInBurst.std() if spikesInBurst.any() else None
        _results[&#39;Mean burst duration ms&#39;] = burst_duration.mean() if burst_duration.any() else None
        _results[&#39;Std burst duration&#39;] = burst_duration.std() if burst_duration.any() else None
        _results[&#39;Mean duty cycle&#39;] = duty_cycle.mean() if duty_cycle.any() else None
        _results[&#39;Std duty cycle&#39;] = duty_cycle.std() if duty_cycle.any() else None
        _results[&#39;Mean IBI&#39;] = ibi.mean() if ibi.any() else None
        _results[&#39;Std IBI&#39;] = ibi.std() if ibi.any() else None
        _results[&#39;Propensity to burst&#39;] = spikesInBurst.sum()/ unitStamp.size
        
        self.update_result(_results)

    def theta_index(self, **kwargs):
        &#34;&#34;&#34;
        Analysis of theta-modulation of a unit
        
        Parameters
        ----------
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
    
        &#34;&#34;&#34;
        
        p_0 = kwargs.get(&#39;start&#39;, [6, 0.1, 0.05])
        lb = kwargs.get(&#39;lower&#39;, [4, 0, 0])
        ub = kwargs.get(&#39;upper&#39;, [14, 5, 0.1])

        _results = oDict()
        graph_data = self.isi_corr(**kwargs)
        corrBins = graph_data[&#39;isiCorrBins&#39;]
        corrCount = graph_data[&#39;isiCorr&#39;]
        m = corrCount.max()
        center = find(corrBins == 0, 1, &#39;first&#39;)[0]
        x = corrBins[center:]/1000
        y = corrCount[center:]
        y_fit = np.empty([corrBins.size,])

        ## This is for the double-exponent dip model
        # def fit_func(x, a, f, tau1, b, c1, tau2, c2, tau3):
        #     return  a*np.cos(2*np.pi*f*x)*np.exp(-np.abs(x)/tau1)+ b+ \
        #         c1*np.exp(-np.abs(x)/tau2)- c2*np.exp(-np.abs(x)/tau3)
        
        # popt, pcov = curve_fit(fit_func, x, y, \
        #                         p0=[m, p_0[0], p_0[1], m, m, p_0[2], m, 0.005], \
        #                         bounds=([0, lb[0], lb[1], 0, 0, lb[2], 0, 0], \
        #                         [m, ub[0], ub[1], m, m, ub[2], m, 0.01]),
        #                         max_nfev=100000)
        # a, f, tau1, b, c1, tau2, c2, tau3 = popt

        # This is for the single-exponent dip model
        def fit_func(x, a, f, tau1, b, c, tau2):
            return  a*np.cos(2*np.pi*f*x)*np.exp(-np.abs(x)/tau1)+ b+ \
                c*np.exp(-(x/tau2)**2)

        try:
            popt, pcov = curve_fit(
                fit_func, x, y,
                p0=[m, p_0[0], p_0[1], m, m, p_0[2]], 
                bounds=([0, lb[0], lb[1], 0, -m, lb[2]],
                [m, ub[0], ub[1], m, m, ub[2]]),
                max_nfev=100000)
        except Exception as e:
            logging.error(&#34;Failed curve_fit in theta_index: {} &#34;.format(e))
            _results[&#39;Theta Index&#39;] = None
            _results[&#39;TI fit freq Hz&#39;] = None
            _results[&#39;TI fit tau1 sec&#39;] = None
            _results[&#39;TI adj Rsq&#39;] = None
            _results[&#39;TI Pearse R&#39;] = None
            _results[&#39;TI Pearse P&#39;] = None

            self.update_result(_results)
            return None

        a, f, tau1, b, c, tau2 = popt

        y_fit[center:] = fit_func(x, *popt)
        y_fit[:center] = np.flipud(y_fit[center:])

        gof = residual_stat(y, y_fit[center:], 6)

        graph_data[&#39;corrFit&#39;] = y_fit
        _results[&#39;Theta Index&#39;] = a/b
        _results[&#39;TI fit freq Hz&#39;] = f
        _results[&#39;TI fit tau1 sec&#39;] = tau1
        _results[&#39;TI adj Rsq&#39;] = gof[&#39;adj Rsq&#39;]
        _results[&#39;TI Pearse R&#39;] = gof[&#39;Pearson R&#39;]
        _results[&#39;TI Pearse P&#39;] = gof[&#39;Pearson P&#39;]

        self.update_result(_results)

        return graph_data

    def theta_skip_index(self, **kwargs):
        &#34;&#34;&#34;
        Analysis of theta-skipping of a unit
        
        Parameters
        ----------
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;
        
        p_0 = kwargs.get(&#39;start&#39;, [6, 0.1, 0.05])
        lb = kwargs.get(&#39;lower&#39;, [4, 0, 0])
        ub = kwargs.get(&#39;upper&#39;, [14, 5, 0.1])

        _results = oDict()
        graph_data = self.isi_corr(**kwargs)
        corrBins = graph_data[&#39;isiCorrBins&#39;]
        corrCount = graph_data[&#39;isiCorr&#39;]
        m = corrCount.max()
        center = find(corrBins == 0, 1, &#39;first&#39;)[0]
        x = corrBins[center:]/1000
        y = corrCount[center:]
        y_fit = np.empty([corrBins.size,])

        # This is for the double-exponent dip model
        def fit_func(x, a1, f1, a2, f2, tau1, b, c1, tau2, c2, tau3):
            return  (a1*np.cos(2*np.pi*f1*x)+ a2*np.cos(2*np.pi*f2*x))*np.exp(-np.abs(x)/tau1)+ b+ \
                c1*np.exp(-np.abs(x)/tau2)- c2*np.exp(-np.abs(x)/tau3)

        popt, pcov = curve_fit(fit_func, x, y, \
                                p0=[m, p_0[0], m, p_0[0]/2, p_0[1], m, m, p_0[2], m, 0.005], \
                                bounds=([0, lb[0], 0, lb[0]/2, lb[1], 0, 0, lb[2], 0, 0], \
                                [m, ub[0], m, ub[0]/2, ub[1], m, m, ub[2], m, 0.01]),\
                                max_nfev=100000)
        a1, f1, a2, f2, tau1, b, c1, tau2, c2, tau3 = popt

        ## This is for the single-exponent dip model
        # def fit_func(x, a1, f1, a2, f2, tau1, b, c, tau2):
        #     return  (a1*np.cos(2*np.pi*f1*x)+ a2*np.cos(2*np.pi*f2*x))*np.exp(-np.abs(x)/tau1)+ b+ \
        #         c*np.exp(-(x/tau2)**2)
        
        # popt, pcov = curve_fit(fit_func, x, y, \
        #                         p0=[m, p_0[0], m, p_0[0]/2, p_0[1], m, m, p_0[2]], \
        #                         bounds=([0, lb[0], 0, lb[0]/2, lb[1], 0, -m, lb[2]], \
        #                         [m, ub[0], m, ub[0]/2, ub[1], m, m, ub[2]]),
        #                         max_nfev=100000)
        # a1, f1, a2, f2, tau1, b, c, tau2 = popt

        temp_fit = fit_func(x, *popt)
        y_fit[center:] = temp_fit
        y_fit[:center] = np.flipud(temp_fit)

        peak_val, peak_loc = extrema(temp_fit[find(x &gt;= 50/1000)])[0:2]

        if len(peak_val) &gt;= 2:
            skipIndex = (peak_val[1]- peak_val[0])/np.max(np.array([peak_val[1], peak_val[0]]))
        else:
            skipIndex = None
        gof = residual_stat(y, temp_fit, 6)

        graph_data[&#39;corrFit&#39;] = y_fit
        _results[&#39;Theta Skip Index&#39;] = skipIndex
        _results[&#39;TS jump factor&#39;] = a2/(a1+ a2) if skipIndex else None
        _results[&#39;TS f1 freq Hz&#39;] = f1 if skipIndex else None
        _results[&#39;TS f2 freq Hz&#39;] = f2 if skipIndex else None
        _results[&#39;TS freq ratio&#39;] = f1/f2 if skipIndex else None
        _results[&#39;TS tau1 sec&#39;] = tau1 if skipIndex else None
        _results[&#39;TS adj Rsq&#39;] = gof[&#39;adj Rsq&#39;]
        _results[&#39;TS Pearse R&#39;] = gof[&#39;Pearson R&#39;]
        _results[&#39;TS Pearse P&#39;] = gof[&#39;Pearson P&#39;]

        self.update_result(_results)

        return graph_data

    def phase_dist(self, lfp = None, **kwargs):
        &#34;&#34;&#34;
        Analysis of spike to LFP phase distribution
        
        Delegates to NLfp().phase_dist()
        
        Parameters
        ----------
        lfp : NLfp
            LFP object which contains the LFP data
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
    
        See also
        --------
        nc_lfp.NLfp().phase_dist()

        &#34;&#34;&#34;
        
        if lfp is None:
            logging.error(&#39;LFP data not specified!&#39;)
        else:
            try:
                lfp.phase_dist(self.get_unit_stamp(), **kwargs)
            except:
                logging.error(&#39;No phase_dist() method in lfp data specified!&#39;)

    def plv(self, lfp=None, **kwargs):
        &#34;&#34;&#34;
        Calculates phase-locking value of spike train to underlying LFP signal.
        
        Delegates to NLfp().plv()
        
        Parameters
        ----------
        lfp : NLfp
            LFP object which contains the LFP data
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
    
        See also
        --------
        nc_lfp.NLfp().plv()

        &#34;&#34;&#34;
        
        if lfp is None:
            logging.error(&#39;LFP data not specified!&#39;)
        else:
            try:
                lfp.plv(self.get_unit_stamp(), **kwargs)
            except:
                logging.error(&#39;No plv() method in lfp data specified!&#39;)
            
    def spike_lfp_causality(self, lfp=None, **kwargs):
        &#34;&#34;&#34;
        Analyses spike to underlying LFP causality
        
        Delegates to NLfp().spike_lfp_causality()
        
        Parameters
        ----------
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
    
        See also
        --------
        nc_lfp.NLfp().spike_lfp_causality()

        &#34;&#34;&#34;
        
        if lfp is None:
            logging.error(&#39;LFP data not specified!&#39;)
        else:
            try:
                lfp.spike_lfp_causality(self.get_unit_stamp(), **kwargs)
            except:
                logging.error(&#39;No sfc() method in lfp data specified!&#39;)

    def _set_total_spikes(self, spike_count=1):
        &#34;&#34;&#34;
        Sets the total number of spikes as part of storing the recording information
                
        Parameters
        ----------
        spike_count : int
            Total number of spikes
 
        Returns
        -------
        None    

        &#34;&#34;&#34;
        
        self._record_info[&#39;No of spikes&#39;] = spike_count
        self.spike_count = spike_count
        
    def _set_total_channels(self, tot_channels=1):
        &#34;&#34;&#34;
        Sets the value of number of channels as part of storing the recording information
                
        Parameters
        ----------
        tot_channels : int
            Total number of channels
 
        Returns
        -------
        None    

        &#34;&#34;&#34;

        self._record_info[&#39;No of channels&#39;] = tot_channels
        
    def _set_channel_ids(self, channel_ids):
        &#34;&#34;&#34;
        Sets identity of the channels as part of storing the recording information
                
        Parameters
        ----------
        channel_ids : int
            Total number of channels
 
        Returns
        -------
        None    

        &#34;&#34;&#34;
        
        self._record_info[&#39;Channel IDs&#39;] = channel_ids
        
    def _set_timestamp_bytes(self, bytes_per_timestamp):
        &#34;&#34;&#34;
        Sets `bytes per timestamp` value as part of storing the recording information
                
        Parameters
        ----------
        bytes_per_timestamp : int
            Total number of bytes to represent timestamp in the binary file
 
        Returns
        -------
        None    

        &#34;&#34;&#34;
        self._record_info[&#39;Bytes per timestamp&#39;] = bytes_per_timestamp
        
    def _set_timebase(self, timebase=1):
        &#34;&#34;&#34;
        Sets timbase for spike event timestamps as part of storing the recording information
                
        Parameters
        ----------
        timebase : int
            Timebase for the spike event timestamps
 
        Returns
        -------
        None    

        &#34;&#34;&#34;
        self._record_info[&#39;Timebase&#39;] = timebase
        
    def _set_sampling_rate(self, sampling_rate=1):
        &#34;&#34;&#34;
        Sets the sampling rate of the spike waveform as part of storing the recording information
                
        Parameters
        ----------
        sampling_rate : int
            Sampling rate of the spike waveforms
 
        Returns
        -------
        None    

        &#34;&#34;&#34;
        self._record_info[&#39;Sampling rate&#39;] = sampling_rate
        
    def _set_bytes_per_sample(self, bytes_per_sample=1):
        &#34;&#34;&#34;
        Sets `bytes per sample` value as part of storing the recording information
                
        Parameters
        ----------
        bytes_per_sample : int
            Total number of bytes to represent each waveform sample in the binary file
 
        Returns
        -------
        None    

        &#34;&#34;&#34;
        self._record_info[&#39;Bytes per sample&#39;] = bytes_per_sample
        
    def _set_samples_per_spike(self, samples_per_spike=1):
        &#34;&#34;&#34;
        Sets `samples per spike` value as part of storing the recording information
                
        Parameters
        ----------
        samples_per_spike : int
            Total number of samples to represent a spike waveform
 
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        self._record_info[&#39;Samples per spike&#39;] = samples_per_spike
        
    def _set_fullscale_mv(self, adc_fullscale_mv=1):
        &#34;&#34;&#34;
        Sets fullscale value of ADC value in mV as part of storing the recording information
                
        Parameters
        ----------
        adc_fullscale_mv : int
            Fullscale voltage of ADC signal in mV
 
        Returns
        -------
        None    

        &#34;&#34;&#34;
        self._record_info[&#39;ADC Fullscale mv&#39;] = adc_fullscale_mv

    def get_total_spikes(self):
        &#34;&#34;&#34;
        Returns total number of spikes in the recording
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Total number of spikes

        &#34;&#34;&#34;

        return self._record_info[&#39;No of spikes&#39;]
    
    def get_total_channels(self):
        &#34;&#34;&#34;
        Returns total number of electrode channels in the spike data file
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Total number of electrode channels

        &#34;&#34;&#34;
        
        return self._record_info[&#39;No of channels&#39;]
    
    def get_channel_ids(self):
        &#34;&#34;&#34;
        Returns the identities of individual channels
                
        Parameters
        ----------
        None
 
        Returns
        -------
        list
            Identities of individual channels 

        &#34;&#34;&#34;
        
        return self._record_info[&#39;Channel IDs&#39;]
    
    def get_timestamp_bytes(self):
        &#34;&#34;&#34;
        Returns the number of bytes to represent each timestamp in the binary file
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Number of bytes to represent timestamps

        &#34;&#34;&#34;
        return self._record_info[&#39;Bytes per timestamp&#39;]
    
    def get_timebase(self):
        &#34;&#34;&#34;
        Returns the timebase for spike event timestamps
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Timebase for spike event timestamps

        &#34;&#34;&#34;
    
        return self._record_info[&#39;Timebase&#39;]
    
    def get_sampling_rate(self):
        &#34;&#34;&#34;
        Returns the sampling rate of spike waveforms
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Sampling rate for spike waveforms

        &#34;&#34;&#34;
        return self._record_info[&#39;Sampling rate&#39;]
    
    def get_bytes_per_sample(self):
        &#34;&#34;&#34;
        Returns the number of bytes to represent each spike waveform sample
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Number of bytes to represent each sample of the spike waveforms

        &#34;&#34;&#34;
    
        return self._record_info[&#39;Bytes per sample&#39;]
    
    def get_samples_per_spike(self):
        &#34;&#34;&#34;
        Returns the number of bytes to represent each timestamp in the binary file
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Number of bytes to represent timestamps

        &#34;&#34;&#34;
        
        return self._record_info[&#39;Samples per spike&#39;]
    
    def get_fullscale_mv(self):
        &#34;&#34;&#34;
        Returns the fullscale value of the ADC in mV
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Fullscale ADC value in mV

        &#34;&#34;&#34;
        
        return self._record_info[&#39;ADC Fullscale mv&#39;]

    def save_to_hdf5(self, file_name=None, system=None):
        &#34;&#34;&#34;
        Stores NSpike() object to HDF5 file
        
        Parameters
        ----------
        file_name : str
            Full file directory for the spike data
        system : str
            Recoring system or data format
        
        Returns
        -------
        None
        
        Also see
        --------
        nc_hdf.Nhdf().save_spike()
        
        &#34;&#34;&#34;
        hdf = Nhdf()
        if file_name and system:
            if os.path.exists(file_name):
                self.set_filename(file_name)
                self.set_system(system)
                self.load()
            else:
                logging.error(&#39;Specified file cannot be found!&#39;)

        hdf.save_spike(spike=self)
        hdf.close()

    def load_spike_NWB(self, file_name):
        &#34;&#34;&#34;
        Decodes spike data from NWB (HDF5) file format
        
        Parameters
        ----------
        file_name : str
            Full file directory for the spike data
        
        Returns
        -------
        None
        
        &#34;&#34;&#34;
        
        file_name, path = file_name.split(&#39;+&#39;)
        
        if os.path.exists(file_name):
            hdf = Nhdf()
            hdf.set_filename(file_name)

            _record_info = {}
    
            if path in hdf.f:
                g = hdf.f[path]
            elif &#39;/processing/Shank/&#39;+ path in hdf.f:
                path = &#39;/processing/Shank/&#39;+ path
                g = hdf.f[path]
            else:
                logging.error(&#39;Specified shank datapath does not exist!&#39;)
    
            for key, value in g.attrs.items():
                _record_info[key] = value
            self.set_record_info(_record_info)
    
            path_clust = &#39;Clustering&#39;
            path_wave = &#39;EventWaveForm/WaveForm&#39;
    
            if path_clust in g:
                g_clust = g[path_clust]
                self._set_timestamp(hdf.get_dataset(group=g_clust, name=&#39;times&#39;))
                self.set_unit_tags(hdf.get_dataset(group=g_clust, name=&#39;num&#39;))
                self._set_unit_list()
            else:
                logging.error(&#39;There is no /Clustering in the :&#39; +path)
    
            if path_wave in g:
                g_wave = g[path_wave]
                self._set_total_spikes(hdf.get_dataset(group=g_wave, name=&#39;num_events&#39;))
                chanIDs = hdf.get_dataset(group=g_wave, name=&#39;electrode_idx&#39;)
                self._set_channel_ids(chanIDs)
    
                spike_wave = oDict()
                data = hdf.get_dataset(group=g_wave, name=&#39;data&#39;)
                if len(data.shape) == 2:
                    num_events, num_samples = data.shape
                    tot_chans = 1
                elif len(data.shape) == 3:
                    num_events, num_samples, tot_chans = data.shape
                else:
                    logging.error(path_wave+ &#39;/data contains for more than 3 dimensions!&#39;)
    
                if num_events != hdf.get_dataset(group=g_wave, name=&#39;num_events&#39;):
                    logging.error(&#39;Mismatch between num_events and 1st dimension of &#39;+ path_wave+ &#39;/data&#39;)
                if num_samples != hdf.get_dataset(group=g_wave, name=&#39;num_samples&#39;):
                    logging.error(&#39;Mismatch between num_samples and 2nd dimension of &#39;+ path_wave+ &#39;/data&#39;)
                for i in np.arange(tot_chans):
                    spike_wave[&#39;ch&#39;+ str(i+1)] = data[:, :, i]
                self._set_waveform(spike_wave)
            else:
                logging.error(&#39;There is no /EventWaveForm/WaveForm in the :&#39; +path)
            
            hdf.close()
        else:
            logging.error(file_name + &#39; does not exist!&#39;)
    
    def load_spike_Axona(self, file_name):
        &#34;&#34;&#34;
        Decodes spike data from Axona file format
        
        Parameters
        ----------
        file_name : str
            Full file directory for the spike data
        
        Returns
        -------
        None
        
        &#34;&#34;&#34;
        words = file_name.split(sep=os.sep)
        file_directory = os.sep.join(words[0:-1])
        file_tag = words[-1].split(sep=&#39;.&#39;)[0]
        tet_no = words[-1].split(sep=&#39;.&#39;)[1]
        set_file = file_directory + os.sep + file_tag + &#39;.set&#39;
        cut_file = file_directory + os.sep + file_tag + &#39;_&#39; + tet_no + &#39;.cut&#39;
        clu_file = file_directory + os.sep + file_tag + &#39;.clu.&#39; + tet_no

        self._set_data_source(file_name)
        self._set_source_format(&#39;Axona&#39;)

        with open(file_name, &#39;rb&#39;) as f:
            while True:
                line = f.readline()
                try:
                    line = line.decode(&#39;latin-1&#39;)
                except:
                    break

                if line == &#39;&#39;:
                    break
                if line.startswith(&#39;trial_date&#39;):
                    self._set_date(&#39; &#39;.join(line.replace(&#39;,&#39;, &#39; &#39;).split()[1:]))
                if line.startswith(&#39;trial_time&#39;):
                    self._set_time(line.split()[1])
                if line.startswith(&#39;experimenter&#39;):
                    self._set_experiemnter(&#39; &#39;.join(line.split()[1:]))
                if line.startswith(&#39;comments&#39;):
                    self._set_comments(&#39; &#39;.join(line.split()[1:]))
                if line.startswith(&#39;duration&#39;):
                    self._set_duration(float(&#39;&#39;.join(line.split()[1:])))
                if line.startswith(&#39;sw_version&#39;):
                    self._set_file_version(line.split()[1])
                if line.startswith(&#39;num_chans&#39;):
                    self._set_total_channels(int(&#39;&#39;.join(line.split()[1:])))
                if line.startswith(&#39;timebase&#39;):
                    self._set_timebase(int(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line))))
                if line.startswith(&#39;bytes_per_timestamp&#39;):
                    self._set_timestamp_bytes(int(&#39;&#39;.join(line.split()[1:])))
                if line.startswith(&#39;samples_per_spike&#39;):
                    self._set_samples_per_spike(int(&#39;&#39;.join(line.split()[1:])))
                if line.startswith(&#39;sample_rate&#39;):
                    self._set_sampling_rate(int(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line))))
                if line.startswith(&#39;bytes_per_sample&#39;):
                    self._set_bytes_per_sample(int(&#39;&#39;.join(line.split()[1:])))
                if line.startswith(&#39;num_spikes&#39;):
                    self._set_total_spikes(int(&#39;&#39;.join(line.split()[1:])))
                if line.startswith(&#39;data_start&#39;):
                    break

            num_spikes = self.get_total_spikes()
            bytes_per_timestamp = self.get_timestamp_bytes()
            bytes_per_sample = self.get_bytes_per_sample()
            samples_per_spike = self.get_samples_per_spike()

            f.seek(0, 0)
            header_offset = []
            while True:
                try:
                    buff = f.read(10).decode(&#39;UTF-8&#39;)
                    if buff == &#39;data_start&#39;:
                        header_offset = f.tell()
                        break
                    else:
                        f.seek(-9, 1)
                except:
                    break

            tot_channels = self.get_total_channels()
            self._set_channel_ids([(int(tet_no) - 1)*tot_channels + x for x in range(tot_channels)])
            max_ADC_count = 2**(8*bytes_per_sample - 1) - 1
            max_byte_value = 2**(8*bytes_per_sample)

            with open(set_file, &#39;r&#39;, encoding=&#39;latin-1&#39;) as f_set:
                lines = f_set.readlines()
                gain_lines = dict([tuple(map(int, re.findall(r&#39;\d+.\d+|\d+&#39;, line)[0].split()))\
                            for line in lines if &#39;gain_ch_&#39; in line])
                gains = np.array([gain_lines[ch_id] for ch_id in self.get_channel_ids()])
                for line in lines:
                    if line.startswith(&#39;ADC_fullscale_mv&#39;):
                        self._set_fullscale_mv(int(re.findall(r&#39;\d+.\d+|d+&#39;, line)[0]))
                        break
                AD_bit_uvolts = 2*self.get_fullscale_mv()*10**3/ \
                                 (gains*(2**(8*bytes_per_sample)))

            record_size = tot_channels*(bytes_per_timestamp + \
                            bytes_per_sample * samples_per_spike)
            time_be = 256**(np.arange(bytes_per_timestamp, 0, -1)-1)
            sample_le = 256**(np.arange(0, bytes_per_sample, 1))

            if not header_offset:
                print(&#39;Error: data_start marker not found!&#39;)
            else:
                f.seek(header_offset, 0)
                byte_buffer = np.fromfile(f, dtype=&#39;uint8&#39;)
                spike_time = np.zeros([num_spikes, ], dtype=&#39;uint32&#39;)
                for i in list(range(0, bytes_per_timestamp)):
                    byte = byte_buffer[i:len(byte_buffer):record_size]
                    byte = byte[:num_spikes]
                    spike_time = spike_time + time_be[i]*byte
                spike_time = spike_time/ self.get_timebase()
                spike_time = spike_time.reshape((num_spikes, ))

                spike_wave = oDict()


                for i in np.arange(tot_channels):
                    chan_offset = (i+1)*bytes_per_timestamp+ i*bytes_per_sample*samples_per_spike
                    chan_wave = np.zeros([num_spikes, samples_per_spike], dtype=np.float64)
                    for j in np.arange(0, samples_per_spike, 1):
                        sample_offset = j*bytes_per_sample + chan_offset
                        for k in np.arange(0, bytes_per_sample, 1):
                            byte_offset = k + sample_offset
                            sample_value = sample_le[k]* byte_buffer[byte_offset \
                                          : len(byte_buffer)+ byte_offset-record_size\
                                          :record_size]
                            sample_value = sample_value.astype(np.float64, casting=&#39;unsafe&#39;, copy=False)
                            np.add(chan_wave[:, j], sample_value, out=chan_wave[:, j])
                        np.putmask(chan_wave[:, j], chan_wave[:, j] &gt; max_ADC_count, chan_wave[:, j]- max_byte_value)
                    spike_wave[&#39;ch&#39;+ str(i+1)] = chan_wave*AD_bit_uvolts[i]

            if os.path.isfile(cut_file):
                with open(cut_file, &#39;r&#39;) as f_cut:
                    while True:
                        line = f_cut.readline()
                        if line == &#39;&#39;:
                            break
                        if line.startswith(&#39;Exact_cut&#39;):
                            unit_ID = np.fromfile(f_cut, dtype=&#39;uint8&#39;, sep=&#39; &#39;)
            
            elif os.path.isfile(clu_file):
                data = np.loadtxt(clu_file)
                unit_ID = data[1:].flatten() - 1
                    
            else:
                logging.error(
                    &#34;No cluster file found for spike file {} please make one at {} or {}&#34;.format(
                        file_name, cut_file, clu_file))
                return
            self._set_timestamp(spike_time)
            self._set_waveform(spike_wave)
            self.set_unit_tags(unit_ID)

    def load_spike_Neuralynx(self, file_name):
        &#34;&#34;&#34;
        Decodes spike data from Neuralynx file format
        
        Parameters
        ----------
        file_name : str
            Full file directory for the spike data
        
        Returns
        -------
        None
        
        &#34;&#34;&#34;
        self._set_data_source(file_name)
        self._set_source_format(&#39;Neuralynx&#39;)

        # Format description for the NLX file:
        file_ext = file_name[-3:]
        if file_ext == &#39;ntt&#39;:
            tot_channels = 4
        elif file_ext == &#39;nst&#39;:
            tot_channels = 2
        elif file_ext == &#39;nse&#39;:
            tot_channels = 1
        header_offset = 16*1024 # fixed for NLX files

        bytes_per_timestamp = 8
        bytes_chan_no = 4
        bytes_cell_no = 4
        bytes_per_feature = 4
        num_features = 8
        bytes_features = bytes_per_feature*num_features
        bytes_per_sample = 2
        samples_per_record = 32
        channel_pack_size = bytes_per_sample*tot_channels# ch1|ch2|ch3|ch4 each with 2 bytes

        max_byte_value = np.power(2, bytes_per_sample*8)
        max_ADC_count = np.power(2, bytes_per_sample*8- 1)-1
        AD_bit_uvolts = np.ones([tot_channels, ])*10**-6 # Default value

        record_size = None
        with open(file_name, &#39;rb&#39;) as f:
            while True:
                line = f.readline()
                try:
                    line = line.decode(&#39;UTF-8&#39;)
                except:
                    break

                if line == &#39;&#39;:
                    break
                if &#39;SamplingFrequency&#39; in line:
                    self._set_sampling_rate(float(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line))))
                if &#39;RecordSize&#39; in line:
                    record_size = int(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line)))
                if &#39;Time Opened&#39; in line:
                    self._set_date(re.search(r&#39;\d+/\d+/\d+&#39;, line).group())
                    self._set_time(re.search(r&#39;\d+:\d+:\d+&#39;, line).group())
                if &#39;FileVersion&#39; in line:
                    self._set_file_version(line.split()[1])
                if &#39;ADMaxValue&#39; in line:
                    max_ADC_count = float(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line)))
                if &#39;ADBitVolts&#39; in line:
                    AD_bit_uvolts = np.array([float(x)*(10**6) for x in re.findall(r&#39;\d+.\d+|\d+&#39;, line)])
                if &#39;ADChannel&#39; in line:
                    self._set_channel_ids(np.array([int(x) for x in re.findall(r&#39;\d+&#39;, line)]))
                if &#39;NumADChannels&#39; in line:
                    tot_channels = int(&#39;&#39;.join(re.findall(r&#39;\d+&#39;, line)))

            self._set_fullscale_mv((max_byte_value/2)*AD_bit_uvolts) # gain = 1 assumed to keep in similarity to Axona
            self._set_bytes_per_sample(bytes_per_sample)
            self._set_samples_per_spike(samples_per_record)
            self._set_timestamp_bytes(bytes_per_timestamp)
            self._set_total_channels(tot_channels)

            if not record_size:
                record_size = bytes_per_timestamp+ \
                             bytes_chan_no+ \
                             bytes_cell_no+ \
                             bytes_features+ \
                             bytes_per_sample*samples_per_record*tot_channels

            time_offset = 0
            unitID_offset = bytes_per_timestamp+ \
                           bytes_chan_no
            sample_offset = bytes_per_timestamp+ \
                           bytes_chan_no+ \
                           bytes_cell_no+ \
                           bytes_features
            f.seek(0, 2)
            num_spikes = int((f.tell()- header_offset)/record_size)
            self._set_total_spikes(num_spikes)

            f.seek(header_offset, 0)
            spike_time = np.zeros([num_spikes, ])
            unit_ID = np.zeros([num_spikes, ], dtype=int)
            spike_wave = oDict()
            sample_le = 256**(np.arange(bytes_per_sample))
            for i in np.arange(tot_channels):
                spike_wave[&#39;ch&#39;+ str(i+1)] = np.zeros([num_spikes, samples_per_record])

            for i in np.arange(num_spikes):
                sample_bytes = np.fromfile(f, dtype=&#39;uint8&#39;, count=record_size)
                spike_time[i] = int.from_bytes(sample_bytes[time_offset+ np.arange(bytes_per_timestamp)], byteorder=&#39;little&#39;, signed=False)/10**6
                unit_ID[i] = int.from_bytes(sample_bytes[unitID_offset+ np.arange(bytes_cell_no)], byteorder=&#39;little&#39;, signed=False)

                for j in range(tot_channels):
                    sample_value = np.zeros([samples_per_record, bytes_per_sample])
                    ind = sample_offset+ j*bytes_per_sample+ np.arange(samples_per_record)*channel_pack_size
                    for k in np.arange(bytes_per_sample):
                        sample_value[:, k] = sample_bytes[ind+ k]
                    sample_value = sample_value.dot(sample_le)
                    np.putmask(sample_value, sample_value &gt; max_ADC_count, sample_value- max_byte_value)
                    spike_wave[&#39;ch&#39;+ str(j+1)][i, :] = sample_value*AD_bit_uvolts[j]
            spike_time -= spike_time.min()
            self._set_duration(spike_time.max())
            self._set_timestamp(spike_time)
            self._set_waveform(spike_wave)
            self.set_unit_tags(unit_ID)

    # def sfc(self, lfp=None, **kwargs):
    #     &#34;&#34;&#34;
    #     Calculates spike-field coherence of spike train with underlying LFP signal.

    #     Delegates to NLfp().sfc()

    #     Parameters
    #     ----------
    #     lfp : NLfp
    #         LFP object which contains the LFP data
    #     **kwargs
    #         Keyword arguments

    #     Returns
    #     -------
    #     dict
    #         Graphical data of the analysis

    #     See also
    #     --------
    #     nc_lfp.NLfp().sfc()

    #     &#34;&#34;&#34;

    #     if lfp is None:
    #         logging.error(&#39;LFP data not specified!&#39;)
    #     else:
    #         try:
    #             lfp.sfc(self.get_unit_stamp(), **kwargs)
    #         except:
    #             logging.error(&#39;No sfc() method in lfp data specified!&#39;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="neurochat.nc_spike.NSpike"><code class="flex name class">
<span>class <span class="ident">NSpike</span></span>
<span>(</span><span>**kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>This data class is the placeholder for the dataset that contains information
about the neural spikes. It decodes data from different formats and analyses
single units in the recording.</p>
<p>Instantiate the <code>NBase</code> class</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class NSpike(NBase):
    &#34;&#34;&#34;
    This data class is the placeholder for the dataset that contains information
    about the neural spikes. It decodes data from different formats and analyses
    single units in the recording.
     
    &#34;&#34;&#34;
     
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._unit_no = kwargs.get(&#39;unit_no&#39;, 0)
        self._unit_stamp = []
        self._timestamp = []
        self._unit_list = []
        self._unit_Tags = []
        self._waveform = []
        self.set_record_info({&#39;Timebase&#39;: 1,
                              &#39;Samples per spike&#39;: 1,
                              &#39;No of spikes&#39;: 0,
                              &#39;Channel IDs&#39;: None})
        
        self.__type = &#39;spike&#39;
        
    def get_type(self):
        &#34;&#34;&#34;
        Returns the type of object. For NSpike, this is always `spike` type
        
        Parameters
        ----------
        None
        
        Returns
        -------
        str

        &#34;&#34;&#34;
        
        return self.__type 
    
    def get_unit_tags(self):
        
        &#34;&#34;&#34;
        Returns the unit number or tags of the clustered units
        
        Parameters
        ----------
        None
        
        Returns
        -------
        list ot ndarray

        &#34;&#34;&#34;
        
        return self._unit_Tags
    
    def set_unit_tags(self, new_tags):
        
        &#34;&#34;&#34;
        Sets the number or tags of the clustered units
        
        Parameters
        ----------
        new_tags : list or ndarray
            Tags for each spiking wave 
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if len(new_tags) == len(self._timestamp):
            self._unit_Tags = new_tags
            self._set_unit_list()
        else:
            logging.error(&#39;No of tags spikes does not match with no of spikes&#39;)

    def get_unit_list(self):
        &#34;&#34;&#34;
        Gets the list of the units
        
        Parameters
        ----------
        None
        
        Returns
        -------
        list
            List of the unique tags of spiking-waveforms from clustering

        &#34;&#34;&#34;
        
        return self._unit_list
    
    def _set_unit_list(self):
        &#34;&#34;&#34;
        Sets the list of units from the list of unit tags
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        self._unit_list = list(map(int, set(self._unit_Tags)))
        if 0 in self._unit_list:
            self._unit_list.remove(0)

    def set_unit_no(self, unit_no=None, spike_name=None):
        &#34;&#34;&#34;
        Sets the unit number of the spike dataset to analyse
        
        Parameters
        ----------
        unit_no : int
            Unit or cell number to analyse
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if isinstance(unit_no, int):
            if unit_no in self.get_unit_list():
                self._unit_no = unit_no
                self._set_unit_stamp()
        else:
            if spike_name is None:
                spike_name = self.get_spike_names()
            if len(unit_no) == len(spike_name):
                spikes = self.get_spike(spike_name)
                for i, num in enumerate(unit_no):
                    if num in spikes[i].get_unit_list():
                        spikes[i].set_unit_no(num)
            else:
                logging.error(&#39;Unit no. to set are not as many as child spikes!&#39;)

    def get_unit_no(self, spike_name=None):
        &#34;&#34;&#34;
        Gets currently set unit number of the spike dataset to analyse
        
        Parameters
        ----------
        None
        
        Returns
        -------
        int
            Unit or cell number set to analyse

        &#34;&#34;&#34;
        if spike_name is None:
            unit_no = self._unit_no
        else:
            unit_no = []
            spikes = self.get_spike(spike_name)
            for spike in spikes:
                unit_no.append(spike._unit_no)
        return unit_no

    def get_timestamp(self, unit_no=None):
        &#34;&#34;&#34;
        Returns the timestamps of the spike-waveforms of spefied unit
        
        Parameters
        ----------
        None
        
        Returns
        -------
        ndarray
            Timestamps of the spiking waveforms
        &#34;&#34;&#34;
        
        if unit_no is None:
            return self._timestamp
        else:
            if unit_no in self._unit_list:
                return self._timestamp[self._unit_Tags == unit_no]
            else:
                logging.warning(&#39;Unit &#39; + str(unit_no) + &#39; is not present in the spike data&#39;)

    def _set_timestamp(self, timestamp=None):
        &#34;&#34;&#34;
        Sets the timestamps for all spiking waveforms in the recording
        
        Parameters
        ----------
        timestamp : list or ndarray
            Timestamps of all spiking waveforms
            
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if timestamp is not None:
            self._timestamp = timestamp

    def get_unit_stamp(self):
        &#34;&#34;&#34;
        Gets the timestamps for currently set unit to analyse
        
        Parameters
        ----------
        None
        
        Returns
        -------
        list or ndarray
            Timestamps for currently set unit

        &#34;&#34;&#34;
        
        return self.get_timestamp(self._unit_no)

    def _set_unit_stamp(self):
        &#34;&#34;&#34;
        Sets timestamps of the unit currently set to analyse
        
        Parameters
        ----------
        None
        
        Returns
        -------
        int
            Unit or cell number set to analyse

        &#34;&#34;&#34;
        
        self._unit_stamp = self.get_unit_stamp()

    def get_unit_spikes_count(self, unit_no=None):
        &#34;&#34;&#34;
        Returns the number of spikes in a unit
        
        Parameters
        ----------
        unit_no : int
            Units whose spike count is returned
        
        Returns
        -------
        int
            Number of units spikes of a unit in a recording session

        &#34;&#34;&#34;
        
        if unit_no is None:
            unit_no = self._unit_no
        if unit_no in self._unit_list:
            return sum(self._unit_Tags == unit_no)

    def get_waveform(self):
        &#34;&#34;&#34;
        Returns spike-waveforms
        
        Parameters
        ----------
        None
        
        Returns
        -------
        OrderedDict
            Dictionary of spiking waveforms where keys represent the channel number

        &#34;&#34;&#34;
        
        return self._waveform

    def _set_waveform(self, spike_waves=[]):
        &#34;&#34;&#34;
        Sets spike waveform to the NSpike() object
        
        Parameters
        ----------
        spike_waves : OrderedDict
            Spike waveforms where each key represents one channel
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if spike_waves:
            self._waveform = spike_waves

    def get_unit_waves(self, unit_no=None):
        &#34;&#34;&#34;
        Returns spike waveform of a specified unit
        
        Parameters
        ----------
        unit_no : int
            Unit whose waveforms are to be returned
        
        Returns
        -------
        OrderedDict
            Waveforms of the specified unit. If None, waveforms of currently set
            unit are returned

        &#34;&#34;&#34;
        
        if unit_no is None:
            unit_no = self._unit_no
        _waves = oDict()
        for chan, wave in self._waveform.items():
            _waves[chan] = wave[self._unit_Tags == unit_no, :]
        
        return _waves

    def get_unit_stamps_in_ranges(self, ranges):
        &#34;&#34;&#34;
        Return the unit timestamps in a list of ranges.

        Parameters
        ----------
        ranges : list
            A list of tuples indicating time ranges to get stamps in
        
        Returns
        -------
        list
            The timestamps
        &#34;&#34;&#34;
        stamps = self.get_unit_stamp()
        new_stamps = [
            val for val in stamps
            if any(lower &lt;= val &lt;= upper for (lower, upper) in ranges)
        ]
        return new_stamps

    def subsample(self, sample_range=None):
        &#34;&#34;&#34;
        Extract a time range from the spikes.
        
        Parameters
        ----------
        sample_range : tuple
            the time in seconds to extract from the spikes
        
        Returns
        -------
        NSpike
            subsampled version of initial spike object
        &#34;&#34;&#34;
        if sample_range is None:
            return self
        new_spike = deepcopy(self)
        stamps = self.get_timestamp()
        lower, upper = sample_range
        sample_spike_idxs = (
            (stamps &lt;= upper) &amp; (stamps &gt;= lower)).nonzero()
        new_spike_times = stamps[sample_spike_idxs]
        new_tags = self.get_unit_tags()[sample_spike_idxs]
        new_waveform = new_spike.get_waveform()
        for ch in new_waveform.keys():
            new_waveform[ch] = new_waveform[ch][sample_spike_idxs, :].squeeze()
        new_spike._set_timestamp(new_spike_times)
        new_spike.set_unit_tags(new_tags)
        new_spike._set_waveform(new_waveform)
        new_spike._set_duration(upper - lower)
        return new_spike

    def load(self, filename=None, system=None):
        &#34;&#34;&#34;
        Loads spike datasets
        
        Parameters
        ----------
        filename : str
            Name of the spike datafile
        system : str
            Recording system or format of the spike data file
        
        Returns
        -------
        None
        
        See also
        --------
        load_spike_axona(), load_spike_NLX(), load_spike_NWB()
            
        &#34;&#34;&#34;
        
        if system is None:
            system = self._system
        else:
            self._system = system
        if filename is None:
            filename = self._filename
        else:
            self._filename = filename
        loader = getattr(self, &#39;load_spike_&#39;+ system)
        loader(filename)

    def add_spike(self, spike=None, **kwargs):
        &#34;&#34;&#34;
        Adds new spike node to current NSpike() object
        
        Parameters
        ----------
        spike : NSpike
            NSPike object. If None, new object is created
        
        Returns
        -------
        `:obj:NSpike`
            A new NSpike() object

        &#34;&#34;&#34;
        new_spike = self._add_node(self.__class__, spike, &#39;spike&#39;, **kwargs)
        
        return new_spike

    def load_spike(self, names=None):
        &#34;&#34;&#34;
        Loads datasets of the spike nodes. Name of each node is used for obtaining the
        filenames
        
        Parameters
        ----------
        names : list of str
            Names of the nodes to load. If None, current NSpike() object is loaded
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if names is None:
            self.load()
        elif names == &#39;all&#39;:
            for spike in self._spikes:
                spike.load()
        else:
            logging.error(&#34;Spikes by name has yet to be implemented&#34;)
            # for name in names:
            #     spike = self.get_spikes_by_name(name)
            #     spike.load()

    def add_lfp(self, lfp=None, **kwargs):
        &#34;&#34;&#34;
        Adds new LFP node to current NSpike() object
        
        Parameters
        ----------
        lfp : NLfp
            NLfp object. If None, new object is created
        
        Returns
        -------
        `:obj:Nlfp`
            A new NLfp() object

        &#34;&#34;&#34;
        
        try:
            data_type = lfp.get_type()
        except:
            logging.error(&#39;The data type of the added object cannot be determined!&#39;)

        if data_type == &#39;lfp&#39;:
                cls= lfp.___class__ 
        else:
            cls = None

        new_lfp = self._add_node(cls, lfp, &#39;lfp&#39;, **kwargs)
        
        return new_lfp

    def load_lfp(self, names=&#39;all&#39;):
        &#34;&#34;&#34;
        Loads datasets of the LFP nodes. Name of each node is used for obtaining the
        filenames
        
        Parameters
        ----------
        names : list of str
            Names of the nodes to load. If `all`, all LFP nodes are loaded
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if names == &#39;all&#39;:
            for lfp in self._lfp:
                lfp.load()
        else:
            logging.error(&#34;Lfp by name has yet to be implemented&#34;)
            # for name in names:
            #     lfp = self.get_lfp_by_name(name)
            #     lfp.load()

    def wave_property(self):
        &#34;&#34;&#34;
        Claulates different waveform properties for currently set unit
        
        Parameters
        ----------
        None
 
        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;

        _result = oDict()
        graph_data = {}

        def argpeak(data):
            data = np.array(data)
            peak_loc = [j for j in range(7, len(data)) \
                        if data[j] &lt;= 0 and data[j - 1] &gt; 0]
            return peak_loc[0] if peak_loc else 0

        def argtrough1(data, peak_loc):
            data = data.tolist()
            trough_loc = [peak_loc - j for j in range(peak_loc - 2) \
                        if data[peak_loc - j] &gt;= 0 and data[peak_loc - j - 1] &lt;= 0]
            return trough_loc[0] if trough_loc else 0

        def wave_width(wave, peak, thresh=0.25):
            p_loc, p_val = peak
            Len = wave.size
            if p_loc:
                w_start = find(wave[:p_loc] &lt;= thresh*p_val, 1, &#39;last&#39;)
                w_start = w_start[0] if w_start.size else 0
                w_end = find(wave[p_loc:] &lt;= thresh*p_val, 1, &#39;first&#39;)
                w_end = p_loc + w_end[0] if w_end.size else Len
            else:
                w_start = 1
                w_end = Len

            return w_end- w_start

        num_spikes = self.get_unit_spikes_count()
        _result[&#39;Number of Spikes&#39;] = num_spikes
        _result[&#39;Mean Spiking Freq&#39;] = num_spikes/ self.get_duration()
        _waves = self.get_unit_waves()
        samples_per_spike = self.get_samples_per_spike()
        tot_chans = self.get_total_channels()
        meanWave = np.empty([samples_per_spike, tot_chans])
        stdWave = np.empty([samples_per_spike, tot_chans])

        width = np.empty([num_spikes, tot_chans])
        amp = np.empty([num_spikes, tot_chans])
        height = np.empty([num_spikes, tot_chans])
        for i, (chan, wave) in enumerate(_waves.items()):
            meanWave[:, i] = np.mean(wave, 0)
            stdWave[:, i] = np.std(wave, 0)
            slope = np.gradient(wave)[1][:, :-1]
            max_val = wave.max(1)

            peak_val, trough1_val = 0, 0
            if max_val.max() &gt; 0:
                peak_loc = [argpeak(slope[I, :]) for I in range(num_spikes)]
                peak_val = [wave[I, peak_loc[I]] for I in range(num_spikes)]
                trough1_loc = [argtrough1(slope[I, :], peak_loc[I]) for I in range(num_spikes)]
                trough1_val = [wave[I, trough1_loc[I]] for I in range(num_spikes)]
                peak_loc = np.array(peak_loc)
                peak_val = np.array(peak_val)
                trough1_loc = np.array(trough1_loc)
                trough1_val = np.array(trough1_val)
                width[:, i] = np.array([wave_width(wave[I, :], (peak_loc[I], peak_val[I]), 0.25) \
                             for I in range(num_spikes)])

            amp[:, i] = peak_val - trough1_val
            height[:, i] = peak_val - wave.min(1)
        max_chan = amp.mean(0).argmax()
        width = width[:, max_chan]* 10**6/self.get_sampling_rate()
        amp = amp[:, max_chan]
        height = height[:, max_chan]

        graph_data = {&#39;Mean wave&#39;: meanWave, &#39;Std wave&#39;: stdWave,
                      &#39;Amplitude&#39;: amp, &#39;Width&#39;: width, &#39;Height&#39;: height,
                      &#39;Max channel&#39;: max_chan}

        _result.update({&#39;Mean amplitude&#39;: amp.mean(), &#39;Std amplitude&#39;: amp.std(),
                        &#39;Mean height&#39;: height.mean(), &#39;Std height&#39;: height.std(),
                        &#39;Mean width&#39;: width.mean(), &#39;Std width&#39;: width.std()})

        self.update_result(_result)
        
        return graph_data

    def isi(self, bins=&#39;auto&#39;, bound=None, density=False, 
            refractory_threshold=2):
        &#34;&#34;&#34;
        Calulates the ISI histogram of the spike train
        
        Parameters
        ----------
        bins : str or int
            Number of ISI histogram bins. If &#39;auto&#39;, NumPy default is used
            
        bound : int
            Length of the ISI histogram in msec
        density : bool
            If true, normalized historagm is calcultaed
        refractory_threshold : int
            Length of the refractory period in msec

        Returns
        -------
        dict
            Graphical data of the analysis
    
        &#34;&#34;&#34;
        graph_data = oDict()
        _results = oDict()

        unitStamp = self.get_unit_stamp()
        isi = 1000*np.diff(unitStamp)

        below_refractory = isi[isi &lt; refractory_threshold]

        graph_data[&#39;isiHist&#39;], edges = np.histogram(isi, bins=bins, range=bound, density=density)
        graph_data[&#39;isiBins&#39;] = edges[:-1]
        graph_data[&#39;isiBinCentres&#39;] = edges[:-1] + np.mean(np.diff(edges))
        graph_data[&#39;isi&#39;] = isi
        graph_data[&#39;maxCount&#39;] = graph_data[&#39;isiHist&#39;].max()
        graph_data[&#39;isiBefore&#39;] = isi[:-1]
        graph_data[&#39;isiAfter&#39;] = isi[1:]

        _results[&#34;Mean ISI&#34;] = isi.mean()
        _results[&#34;Std ISI&#34;] = isi.std()
        _results[&#34;Refractory violation&#34;] = (
            below_refractory.size / unitStamp.size)

        self.update_result(_results)
        return graph_data

    def isi_corr(self, spike=None, **kwargs):
        &#34;&#34;&#34;
        Calculates the correlation of ISI histogram.
        
        Parameters
        ----------
        spike : NSpike()
            If specified, it calulates cross-correlation.
            
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
    
        &#34;&#34;&#34;
        
        graph_data = oDict()
        if spike is None:
            _unit_stamp = np.copy(self.get_unit_stamp())
        elif isinstance(spike, int):
            if spike in self.get_unit_list():
                _unit_stamp = self.get_timestamp(spike)
        else:
            if isinstance(spike, str):
                spike = self.get_spike(spike)
            if isinstance(spike, self.__class__):
                _unit_stamp = spike.get_unit_stamp()
            else:
                logging.error(&#39;No valid spike specified&#39;)

        _corr = self.psth(_unit_stamp, **kwargs)
        graph_data[&#39;isiCorrBins&#39;] = _corr[&#39;bins&#39;]
        graph_data[&#39;isiAllCorrBins&#39;] = _corr[&#39;all_bins&#39;]
        center = find(_corr[&#39;bins&#39;] == 0, 1, &#39;first&#39;)[0]
        graph_data[&#39;isiCorr&#39;] = _corr[&#39;psth&#39;]
        graph_data[&#39;isiCorr&#39;][center] = graph_data[&#39;isiCorr&#39;][center] \
                                    - np.min([self.get_unit_stamp().size, _unit_stamp.size])

        return graph_data

    def psth(self, event_stamp, **kwargs):
        &#34;&#34;&#34;
        Calculates peri-stimulus time histogram (PSTH)
        
        Parameters
        ----------
        event_stamp : ndarray
            Event timestamps
            
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
    
        &#34;&#34;&#34;
        
        graph_data = oDict()
        bins = kwargs.get(&#39;bins&#39;, 1)
        if isinstance(bins, int):
            bound = np.array(kwargs.get(&#39;bound&#39;, [-500, 500]))
            bins = np.hstack((np.arange(bound[0], 0, bins), np.arange(0, bound[1] + bins, bins)))
        bins = bins/1000 # converted to sec
        n_bins = len(bins) - 1

        hist_count = np.zeros([n_bins, ])
        unitStamp = self.get_unit_stamp()
        for it in range(event_stamp.size):
            tmp_count, edges = np.histogram(unitStamp - event_stamp[it], bins=bins)
            hist_count = hist_count + tmp_count

        graph_data[&#39;psth&#39;] = hist_count
        graph_data[&#39;bins&#39;] = 1000*edges[:-1]

        # Included in case the last point is needed
        graph_data[&#39;all_bins&#39;] = 1000*edges

        return graph_data

    def burst(self, burst_thresh=5, ibi_thresh=50):
        &#34;&#34;&#34;
        Analysis of bursting properties of the spiking train
        
        Parameters
        ----------
        burst_thresh : int
            Minimum ISI between consecutive spikes in a burst
            
        ibi_thresh : int
            Minimum inter-burst interval between two bursting groups of spikes
 
        Returns
        -------
        None
    
        &#34;&#34;&#34;

        _results = oDict()

        unitStamp = self.get_unit_stamp()
        isi = 1000*np.diff(unitStamp)

        burst_start = []
        burst_end = []
        burst_duration = []
        spikesInBurst = []
        bursting_isi = []
        num_burst = 0
        ibi = []
        duty_cycle = []
        k = 0
        while k &lt; isi.size:
            if isi[k] &lt;= burst_thresh:
                burst_start.append(k)
                spikesInBurst.append(2)
                bursting_isi.append(isi[k])
                burst_duration.append(isi[k])
                m = k+1
                while m &lt; isi.size and isi[m] &lt;= burst_thresh:
                    spikesInBurst[num_burst] += 1
                    bursting_isi.append(isi[m])
                    burst_duration[num_burst] += isi[m]
                    m += 1
                burst_duration[num_burst] += 1 # to compensate for the span of the last spike
                burst_end.append(m)
                k = m+1
                num_burst += 1
            else:
                k += 1
        if num_burst:
            for j in range(0, num_burst-1):
                ibi.append(unitStamp[burst_start[j+1]]- unitStamp[burst_end[j]])
            duty_cycle = np.divide(burst_duration[1:], ibi)/1000 # ibi in sec, burst_duration in ms
        else:
            logging.warning(
                &#39;No burst detected in {}&#39;.format(self.get_filename()))

        spikesInBurst = np.array(spikesInBurst) if spikesInBurst else np.array([])
        bursting_isi = np.array(bursting_isi) if bursting_isi else np.array([])
        ibi = 1000*np.array(ibi) if ibi else np.array([]) # in sec unit, so converted to ms
        burst_duration = np.array(burst_duration) if burst_duration else np.array([])
        duty_cycle = np.array(duty_cycle) if len(duty_cycle) else np.array([])

        _results[&#39;Total burst&#39;] = num_burst
        _results[&#39;Total bursting spikes&#39;] = spikesInBurst.sum()
        _results[&#39;Mean bursting ISI ms&#39;] = bursting_isi.mean() if bursting_isi.any() else None
        _results[&#39;Std bursting ISI ms&#39;] = bursting_isi.std() if bursting_isi.any() else None
        _results[&#39;Mean spikes per burst&#39;] = spikesInBurst.mean() if spikesInBurst.any() else None
        _results[&#39;Std spikes per burst&#39;] = spikesInBurst.std() if spikesInBurst.any() else None
        _results[&#39;Mean burst duration ms&#39;] = burst_duration.mean() if burst_duration.any() else None
        _results[&#39;Std burst duration&#39;] = burst_duration.std() if burst_duration.any() else None
        _results[&#39;Mean duty cycle&#39;] = duty_cycle.mean() if duty_cycle.any() else None
        _results[&#39;Std duty cycle&#39;] = duty_cycle.std() if duty_cycle.any() else None
        _results[&#39;Mean IBI&#39;] = ibi.mean() if ibi.any() else None
        _results[&#39;Std IBI&#39;] = ibi.std() if ibi.any() else None
        _results[&#39;Propensity to burst&#39;] = spikesInBurst.sum()/ unitStamp.size
        
        self.update_result(_results)

    def theta_index(self, **kwargs):
        &#34;&#34;&#34;
        Analysis of theta-modulation of a unit
        
        Parameters
        ----------
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
    
        &#34;&#34;&#34;
        
        p_0 = kwargs.get(&#39;start&#39;, [6, 0.1, 0.05])
        lb = kwargs.get(&#39;lower&#39;, [4, 0, 0])
        ub = kwargs.get(&#39;upper&#39;, [14, 5, 0.1])

        _results = oDict()
        graph_data = self.isi_corr(**kwargs)
        corrBins = graph_data[&#39;isiCorrBins&#39;]
        corrCount = graph_data[&#39;isiCorr&#39;]
        m = corrCount.max()
        center = find(corrBins == 0, 1, &#39;first&#39;)[0]
        x = corrBins[center:]/1000
        y = corrCount[center:]
        y_fit = np.empty([corrBins.size,])

        ## This is for the double-exponent dip model
        # def fit_func(x, a, f, tau1, b, c1, tau2, c2, tau3):
        #     return  a*np.cos(2*np.pi*f*x)*np.exp(-np.abs(x)/tau1)+ b+ \
        #         c1*np.exp(-np.abs(x)/tau2)- c2*np.exp(-np.abs(x)/tau3)
        
        # popt, pcov = curve_fit(fit_func, x, y, \
        #                         p0=[m, p_0[0], p_0[1], m, m, p_0[2], m, 0.005], \
        #                         bounds=([0, lb[0], lb[1], 0, 0, lb[2], 0, 0], \
        #                         [m, ub[0], ub[1], m, m, ub[2], m, 0.01]),
        #                         max_nfev=100000)
        # a, f, tau1, b, c1, tau2, c2, tau3 = popt

        # This is for the single-exponent dip model
        def fit_func(x, a, f, tau1, b, c, tau2):
            return  a*np.cos(2*np.pi*f*x)*np.exp(-np.abs(x)/tau1)+ b+ \
                c*np.exp(-(x/tau2)**2)

        try:
            popt, pcov = curve_fit(
                fit_func, x, y,
                p0=[m, p_0[0], p_0[1], m, m, p_0[2]], 
                bounds=([0, lb[0], lb[1], 0, -m, lb[2]],
                [m, ub[0], ub[1], m, m, ub[2]]),
                max_nfev=100000)
        except Exception as e:
            logging.error(&#34;Failed curve_fit in theta_index: {} &#34;.format(e))
            _results[&#39;Theta Index&#39;] = None
            _results[&#39;TI fit freq Hz&#39;] = None
            _results[&#39;TI fit tau1 sec&#39;] = None
            _results[&#39;TI adj Rsq&#39;] = None
            _results[&#39;TI Pearse R&#39;] = None
            _results[&#39;TI Pearse P&#39;] = None

            self.update_result(_results)
            return None

        a, f, tau1, b, c, tau2 = popt

        y_fit[center:] = fit_func(x, *popt)
        y_fit[:center] = np.flipud(y_fit[center:])

        gof = residual_stat(y, y_fit[center:], 6)

        graph_data[&#39;corrFit&#39;] = y_fit
        _results[&#39;Theta Index&#39;] = a/b
        _results[&#39;TI fit freq Hz&#39;] = f
        _results[&#39;TI fit tau1 sec&#39;] = tau1
        _results[&#39;TI adj Rsq&#39;] = gof[&#39;adj Rsq&#39;]
        _results[&#39;TI Pearse R&#39;] = gof[&#39;Pearson R&#39;]
        _results[&#39;TI Pearse P&#39;] = gof[&#39;Pearson P&#39;]

        self.update_result(_results)

        return graph_data

    def theta_skip_index(self, **kwargs):
        &#34;&#34;&#34;
        Analysis of theta-skipping of a unit
        
        Parameters
        ----------
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;
        
        p_0 = kwargs.get(&#39;start&#39;, [6, 0.1, 0.05])
        lb = kwargs.get(&#39;lower&#39;, [4, 0, 0])
        ub = kwargs.get(&#39;upper&#39;, [14, 5, 0.1])

        _results = oDict()
        graph_data = self.isi_corr(**kwargs)
        corrBins = graph_data[&#39;isiCorrBins&#39;]
        corrCount = graph_data[&#39;isiCorr&#39;]
        m = corrCount.max()
        center = find(corrBins == 0, 1, &#39;first&#39;)[0]
        x = corrBins[center:]/1000
        y = corrCount[center:]
        y_fit = np.empty([corrBins.size,])

        # This is for the double-exponent dip model
        def fit_func(x, a1, f1, a2, f2, tau1, b, c1, tau2, c2, tau3):
            return  (a1*np.cos(2*np.pi*f1*x)+ a2*np.cos(2*np.pi*f2*x))*np.exp(-np.abs(x)/tau1)+ b+ \
                c1*np.exp(-np.abs(x)/tau2)- c2*np.exp(-np.abs(x)/tau3)

        popt, pcov = curve_fit(fit_func, x, y, \
                                p0=[m, p_0[0], m, p_0[0]/2, p_0[1], m, m, p_0[2], m, 0.005], \
                                bounds=([0, lb[0], 0, lb[0]/2, lb[1], 0, 0, lb[2], 0, 0], \
                                [m, ub[0], m, ub[0]/2, ub[1], m, m, ub[2], m, 0.01]),\
                                max_nfev=100000)
        a1, f1, a2, f2, tau1, b, c1, tau2, c2, tau3 = popt

        ## This is for the single-exponent dip model
        # def fit_func(x, a1, f1, a2, f2, tau1, b, c, tau2):
        #     return  (a1*np.cos(2*np.pi*f1*x)+ a2*np.cos(2*np.pi*f2*x))*np.exp(-np.abs(x)/tau1)+ b+ \
        #         c*np.exp(-(x/tau2)**2)
        
        # popt, pcov = curve_fit(fit_func, x, y, \
        #                         p0=[m, p_0[0], m, p_0[0]/2, p_0[1], m, m, p_0[2]], \
        #                         bounds=([0, lb[0], 0, lb[0]/2, lb[1], 0, -m, lb[2]], \
        #                         [m, ub[0], m, ub[0]/2, ub[1], m, m, ub[2]]),
        #                         max_nfev=100000)
        # a1, f1, a2, f2, tau1, b, c, tau2 = popt

        temp_fit = fit_func(x, *popt)
        y_fit[center:] = temp_fit
        y_fit[:center] = np.flipud(temp_fit)

        peak_val, peak_loc = extrema(temp_fit[find(x &gt;= 50/1000)])[0:2]

        if len(peak_val) &gt;= 2:
            skipIndex = (peak_val[1]- peak_val[0])/np.max(np.array([peak_val[1], peak_val[0]]))
        else:
            skipIndex = None
        gof = residual_stat(y, temp_fit, 6)

        graph_data[&#39;corrFit&#39;] = y_fit
        _results[&#39;Theta Skip Index&#39;] = skipIndex
        _results[&#39;TS jump factor&#39;] = a2/(a1+ a2) if skipIndex else None
        _results[&#39;TS f1 freq Hz&#39;] = f1 if skipIndex else None
        _results[&#39;TS f2 freq Hz&#39;] = f2 if skipIndex else None
        _results[&#39;TS freq ratio&#39;] = f1/f2 if skipIndex else None
        _results[&#39;TS tau1 sec&#39;] = tau1 if skipIndex else None
        _results[&#39;TS adj Rsq&#39;] = gof[&#39;adj Rsq&#39;]
        _results[&#39;TS Pearse R&#39;] = gof[&#39;Pearson R&#39;]
        _results[&#39;TS Pearse P&#39;] = gof[&#39;Pearson P&#39;]

        self.update_result(_results)

        return graph_data

    def phase_dist(self, lfp = None, **kwargs):
        &#34;&#34;&#34;
        Analysis of spike to LFP phase distribution
        
        Delegates to NLfp().phase_dist()
        
        Parameters
        ----------
        lfp : NLfp
            LFP object which contains the LFP data
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
    
        See also
        --------
        nc_lfp.NLfp().phase_dist()

        &#34;&#34;&#34;
        
        if lfp is None:
            logging.error(&#39;LFP data not specified!&#39;)
        else:
            try:
                lfp.phase_dist(self.get_unit_stamp(), **kwargs)
            except:
                logging.error(&#39;No phase_dist() method in lfp data specified!&#39;)

    def plv(self, lfp=None, **kwargs):
        &#34;&#34;&#34;
        Calculates phase-locking value of spike train to underlying LFP signal.
        
        Delegates to NLfp().plv()
        
        Parameters
        ----------
        lfp : NLfp
            LFP object which contains the LFP data
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
    
        See also
        --------
        nc_lfp.NLfp().plv()

        &#34;&#34;&#34;
        
        if lfp is None:
            logging.error(&#39;LFP data not specified!&#39;)
        else:
            try:
                lfp.plv(self.get_unit_stamp(), **kwargs)
            except:
                logging.error(&#39;No plv() method in lfp data specified!&#39;)
            
    def spike_lfp_causality(self, lfp=None, **kwargs):
        &#34;&#34;&#34;
        Analyses spike to underlying LFP causality
        
        Delegates to NLfp().spike_lfp_causality()
        
        Parameters
        ----------
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
    
        See also
        --------
        nc_lfp.NLfp().spike_lfp_causality()

        &#34;&#34;&#34;
        
        if lfp is None:
            logging.error(&#39;LFP data not specified!&#39;)
        else:
            try:
                lfp.spike_lfp_causality(self.get_unit_stamp(), **kwargs)
            except:
                logging.error(&#39;No sfc() method in lfp data specified!&#39;)

    def _set_total_spikes(self, spike_count=1):
        &#34;&#34;&#34;
        Sets the total number of spikes as part of storing the recording information
                
        Parameters
        ----------
        spike_count : int
            Total number of spikes
 
        Returns
        -------
        None    

        &#34;&#34;&#34;
        
        self._record_info[&#39;No of spikes&#39;] = spike_count
        self.spike_count = spike_count
        
    def _set_total_channels(self, tot_channels=1):
        &#34;&#34;&#34;
        Sets the value of number of channels as part of storing the recording information
                
        Parameters
        ----------
        tot_channels : int
            Total number of channels
 
        Returns
        -------
        None    

        &#34;&#34;&#34;

        self._record_info[&#39;No of channels&#39;] = tot_channels
        
    def _set_channel_ids(self, channel_ids):
        &#34;&#34;&#34;
        Sets identity of the channels as part of storing the recording information
                
        Parameters
        ----------
        channel_ids : int
            Total number of channels
 
        Returns
        -------
        None    

        &#34;&#34;&#34;
        
        self._record_info[&#39;Channel IDs&#39;] = channel_ids
        
    def _set_timestamp_bytes(self, bytes_per_timestamp):
        &#34;&#34;&#34;
        Sets `bytes per timestamp` value as part of storing the recording information
                
        Parameters
        ----------
        bytes_per_timestamp : int
            Total number of bytes to represent timestamp in the binary file
 
        Returns
        -------
        None    

        &#34;&#34;&#34;
        self._record_info[&#39;Bytes per timestamp&#39;] = bytes_per_timestamp
        
    def _set_timebase(self, timebase=1):
        &#34;&#34;&#34;
        Sets timbase for spike event timestamps as part of storing the recording information
                
        Parameters
        ----------
        timebase : int
            Timebase for the spike event timestamps
 
        Returns
        -------
        None    

        &#34;&#34;&#34;
        self._record_info[&#39;Timebase&#39;] = timebase
        
    def _set_sampling_rate(self, sampling_rate=1):
        &#34;&#34;&#34;
        Sets the sampling rate of the spike waveform as part of storing the recording information
                
        Parameters
        ----------
        sampling_rate : int
            Sampling rate of the spike waveforms
 
        Returns
        -------
        None    

        &#34;&#34;&#34;
        self._record_info[&#39;Sampling rate&#39;] = sampling_rate
        
    def _set_bytes_per_sample(self, bytes_per_sample=1):
        &#34;&#34;&#34;
        Sets `bytes per sample` value as part of storing the recording information
                
        Parameters
        ----------
        bytes_per_sample : int
            Total number of bytes to represent each waveform sample in the binary file
 
        Returns
        -------
        None    

        &#34;&#34;&#34;
        self._record_info[&#39;Bytes per sample&#39;] = bytes_per_sample
        
    def _set_samples_per_spike(self, samples_per_spike=1):
        &#34;&#34;&#34;
        Sets `samples per spike` value as part of storing the recording information
                
        Parameters
        ----------
        samples_per_spike : int
            Total number of samples to represent a spike waveform
 
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        self._record_info[&#39;Samples per spike&#39;] = samples_per_spike
        
    def _set_fullscale_mv(self, adc_fullscale_mv=1):
        &#34;&#34;&#34;
        Sets fullscale value of ADC value in mV as part of storing the recording information
                
        Parameters
        ----------
        adc_fullscale_mv : int
            Fullscale voltage of ADC signal in mV
 
        Returns
        -------
        None    

        &#34;&#34;&#34;
        self._record_info[&#39;ADC Fullscale mv&#39;] = adc_fullscale_mv

    def get_total_spikes(self):
        &#34;&#34;&#34;
        Returns total number of spikes in the recording
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Total number of spikes

        &#34;&#34;&#34;

        return self._record_info[&#39;No of spikes&#39;]
    
    def get_total_channels(self):
        &#34;&#34;&#34;
        Returns total number of electrode channels in the spike data file
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Total number of electrode channels

        &#34;&#34;&#34;
        
        return self._record_info[&#39;No of channels&#39;]
    
    def get_channel_ids(self):
        &#34;&#34;&#34;
        Returns the identities of individual channels
                
        Parameters
        ----------
        None
 
        Returns
        -------
        list
            Identities of individual channels 

        &#34;&#34;&#34;
        
        return self._record_info[&#39;Channel IDs&#39;]
    
    def get_timestamp_bytes(self):
        &#34;&#34;&#34;
        Returns the number of bytes to represent each timestamp in the binary file
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Number of bytes to represent timestamps

        &#34;&#34;&#34;
        return self._record_info[&#39;Bytes per timestamp&#39;]
    
    def get_timebase(self):
        &#34;&#34;&#34;
        Returns the timebase for spike event timestamps
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Timebase for spike event timestamps

        &#34;&#34;&#34;
    
        return self._record_info[&#39;Timebase&#39;]
    
    def get_sampling_rate(self):
        &#34;&#34;&#34;
        Returns the sampling rate of spike waveforms
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Sampling rate for spike waveforms

        &#34;&#34;&#34;
        return self._record_info[&#39;Sampling rate&#39;]
    
    def get_bytes_per_sample(self):
        &#34;&#34;&#34;
        Returns the number of bytes to represent each spike waveform sample
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Number of bytes to represent each sample of the spike waveforms

        &#34;&#34;&#34;
    
        return self._record_info[&#39;Bytes per sample&#39;]
    
    def get_samples_per_spike(self):
        &#34;&#34;&#34;
        Returns the number of bytes to represent each timestamp in the binary file
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Number of bytes to represent timestamps

        &#34;&#34;&#34;
        
        return self._record_info[&#39;Samples per spike&#39;]
    
    def get_fullscale_mv(self):
        &#34;&#34;&#34;
        Returns the fullscale value of the ADC in mV
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Fullscale ADC value in mV

        &#34;&#34;&#34;
        
        return self._record_info[&#39;ADC Fullscale mv&#39;]

    def save_to_hdf5(self, file_name=None, system=None):
        &#34;&#34;&#34;
        Stores NSpike() object to HDF5 file
        
        Parameters
        ----------
        file_name : str
            Full file directory for the spike data
        system : str
            Recoring system or data format
        
        Returns
        -------
        None
        
        Also see
        --------
        nc_hdf.Nhdf().save_spike()
        
        &#34;&#34;&#34;
        hdf = Nhdf()
        if file_name and system:
            if os.path.exists(file_name):
                self.set_filename(file_name)
                self.set_system(system)
                self.load()
            else:
                logging.error(&#39;Specified file cannot be found!&#39;)

        hdf.save_spike(spike=self)
        hdf.close()

    def load_spike_NWB(self, file_name):
        &#34;&#34;&#34;
        Decodes spike data from NWB (HDF5) file format
        
        Parameters
        ----------
        file_name : str
            Full file directory for the spike data
        
        Returns
        -------
        None
        
        &#34;&#34;&#34;
        
        file_name, path = file_name.split(&#39;+&#39;)
        
        if os.path.exists(file_name):
            hdf = Nhdf()
            hdf.set_filename(file_name)

            _record_info = {}
    
            if path in hdf.f:
                g = hdf.f[path]
            elif &#39;/processing/Shank/&#39;+ path in hdf.f:
                path = &#39;/processing/Shank/&#39;+ path
                g = hdf.f[path]
            else:
                logging.error(&#39;Specified shank datapath does not exist!&#39;)
    
            for key, value in g.attrs.items():
                _record_info[key] = value
            self.set_record_info(_record_info)
    
            path_clust = &#39;Clustering&#39;
            path_wave = &#39;EventWaveForm/WaveForm&#39;
    
            if path_clust in g:
                g_clust = g[path_clust]
                self._set_timestamp(hdf.get_dataset(group=g_clust, name=&#39;times&#39;))
                self.set_unit_tags(hdf.get_dataset(group=g_clust, name=&#39;num&#39;))
                self._set_unit_list()
            else:
                logging.error(&#39;There is no /Clustering in the :&#39; +path)
    
            if path_wave in g:
                g_wave = g[path_wave]
                self._set_total_spikes(hdf.get_dataset(group=g_wave, name=&#39;num_events&#39;))
                chanIDs = hdf.get_dataset(group=g_wave, name=&#39;electrode_idx&#39;)
                self._set_channel_ids(chanIDs)
    
                spike_wave = oDict()
                data = hdf.get_dataset(group=g_wave, name=&#39;data&#39;)
                if len(data.shape) == 2:
                    num_events, num_samples = data.shape
                    tot_chans = 1
                elif len(data.shape) == 3:
                    num_events, num_samples, tot_chans = data.shape
                else:
                    logging.error(path_wave+ &#39;/data contains for more than 3 dimensions!&#39;)
    
                if num_events != hdf.get_dataset(group=g_wave, name=&#39;num_events&#39;):
                    logging.error(&#39;Mismatch between num_events and 1st dimension of &#39;+ path_wave+ &#39;/data&#39;)
                if num_samples != hdf.get_dataset(group=g_wave, name=&#39;num_samples&#39;):
                    logging.error(&#39;Mismatch between num_samples and 2nd dimension of &#39;+ path_wave+ &#39;/data&#39;)
                for i in np.arange(tot_chans):
                    spike_wave[&#39;ch&#39;+ str(i+1)] = data[:, :, i]
                self._set_waveform(spike_wave)
            else:
                logging.error(&#39;There is no /EventWaveForm/WaveForm in the :&#39; +path)
            
            hdf.close()
        else:
            logging.error(file_name + &#39; does not exist!&#39;)
    
    def load_spike_Axona(self, file_name):
        &#34;&#34;&#34;
        Decodes spike data from Axona file format
        
        Parameters
        ----------
        file_name : str
            Full file directory for the spike data
        
        Returns
        -------
        None
        
        &#34;&#34;&#34;
        words = file_name.split(sep=os.sep)
        file_directory = os.sep.join(words[0:-1])
        file_tag = words[-1].split(sep=&#39;.&#39;)[0]
        tet_no = words[-1].split(sep=&#39;.&#39;)[1]
        set_file = file_directory + os.sep + file_tag + &#39;.set&#39;
        cut_file = file_directory + os.sep + file_tag + &#39;_&#39; + tet_no + &#39;.cut&#39;
        clu_file = file_directory + os.sep + file_tag + &#39;.clu.&#39; + tet_no

        self._set_data_source(file_name)
        self._set_source_format(&#39;Axona&#39;)

        with open(file_name, &#39;rb&#39;) as f:
            while True:
                line = f.readline()
                try:
                    line = line.decode(&#39;latin-1&#39;)
                except:
                    break

                if line == &#39;&#39;:
                    break
                if line.startswith(&#39;trial_date&#39;):
                    self._set_date(&#39; &#39;.join(line.replace(&#39;,&#39;, &#39; &#39;).split()[1:]))
                if line.startswith(&#39;trial_time&#39;):
                    self._set_time(line.split()[1])
                if line.startswith(&#39;experimenter&#39;):
                    self._set_experiemnter(&#39; &#39;.join(line.split()[1:]))
                if line.startswith(&#39;comments&#39;):
                    self._set_comments(&#39; &#39;.join(line.split()[1:]))
                if line.startswith(&#39;duration&#39;):
                    self._set_duration(float(&#39;&#39;.join(line.split()[1:])))
                if line.startswith(&#39;sw_version&#39;):
                    self._set_file_version(line.split()[1])
                if line.startswith(&#39;num_chans&#39;):
                    self._set_total_channels(int(&#39;&#39;.join(line.split()[1:])))
                if line.startswith(&#39;timebase&#39;):
                    self._set_timebase(int(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line))))
                if line.startswith(&#39;bytes_per_timestamp&#39;):
                    self._set_timestamp_bytes(int(&#39;&#39;.join(line.split()[1:])))
                if line.startswith(&#39;samples_per_spike&#39;):
                    self._set_samples_per_spike(int(&#39;&#39;.join(line.split()[1:])))
                if line.startswith(&#39;sample_rate&#39;):
                    self._set_sampling_rate(int(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line))))
                if line.startswith(&#39;bytes_per_sample&#39;):
                    self._set_bytes_per_sample(int(&#39;&#39;.join(line.split()[1:])))
                if line.startswith(&#39;num_spikes&#39;):
                    self._set_total_spikes(int(&#39;&#39;.join(line.split()[1:])))
                if line.startswith(&#39;data_start&#39;):
                    break

            num_spikes = self.get_total_spikes()
            bytes_per_timestamp = self.get_timestamp_bytes()
            bytes_per_sample = self.get_bytes_per_sample()
            samples_per_spike = self.get_samples_per_spike()

            f.seek(0, 0)
            header_offset = []
            while True:
                try:
                    buff = f.read(10).decode(&#39;UTF-8&#39;)
                    if buff == &#39;data_start&#39;:
                        header_offset = f.tell()
                        break
                    else:
                        f.seek(-9, 1)
                except:
                    break

            tot_channels = self.get_total_channels()
            self._set_channel_ids([(int(tet_no) - 1)*tot_channels + x for x in range(tot_channels)])
            max_ADC_count = 2**(8*bytes_per_sample - 1) - 1
            max_byte_value = 2**(8*bytes_per_sample)

            with open(set_file, &#39;r&#39;, encoding=&#39;latin-1&#39;) as f_set:
                lines = f_set.readlines()
                gain_lines = dict([tuple(map(int, re.findall(r&#39;\d+.\d+|\d+&#39;, line)[0].split()))\
                            for line in lines if &#39;gain_ch_&#39; in line])
                gains = np.array([gain_lines[ch_id] for ch_id in self.get_channel_ids()])
                for line in lines:
                    if line.startswith(&#39;ADC_fullscale_mv&#39;):
                        self._set_fullscale_mv(int(re.findall(r&#39;\d+.\d+|d+&#39;, line)[0]))
                        break
                AD_bit_uvolts = 2*self.get_fullscale_mv()*10**3/ \
                                 (gains*(2**(8*bytes_per_sample)))

            record_size = tot_channels*(bytes_per_timestamp + \
                            bytes_per_sample * samples_per_spike)
            time_be = 256**(np.arange(bytes_per_timestamp, 0, -1)-1)
            sample_le = 256**(np.arange(0, bytes_per_sample, 1))

            if not header_offset:
                print(&#39;Error: data_start marker not found!&#39;)
            else:
                f.seek(header_offset, 0)
                byte_buffer = np.fromfile(f, dtype=&#39;uint8&#39;)
                spike_time = np.zeros([num_spikes, ], dtype=&#39;uint32&#39;)
                for i in list(range(0, bytes_per_timestamp)):
                    byte = byte_buffer[i:len(byte_buffer):record_size]
                    byte = byte[:num_spikes]
                    spike_time = spike_time + time_be[i]*byte
                spike_time = spike_time/ self.get_timebase()
                spike_time = spike_time.reshape((num_spikes, ))

                spike_wave = oDict()


                for i in np.arange(tot_channels):
                    chan_offset = (i+1)*bytes_per_timestamp+ i*bytes_per_sample*samples_per_spike
                    chan_wave = np.zeros([num_spikes, samples_per_spike], dtype=np.float64)
                    for j in np.arange(0, samples_per_spike, 1):
                        sample_offset = j*bytes_per_sample + chan_offset
                        for k in np.arange(0, bytes_per_sample, 1):
                            byte_offset = k + sample_offset
                            sample_value = sample_le[k]* byte_buffer[byte_offset \
                                          : len(byte_buffer)+ byte_offset-record_size\
                                          :record_size]
                            sample_value = sample_value.astype(np.float64, casting=&#39;unsafe&#39;, copy=False)
                            np.add(chan_wave[:, j], sample_value, out=chan_wave[:, j])
                        np.putmask(chan_wave[:, j], chan_wave[:, j] &gt; max_ADC_count, chan_wave[:, j]- max_byte_value)
                    spike_wave[&#39;ch&#39;+ str(i+1)] = chan_wave*AD_bit_uvolts[i]

            if os.path.isfile(cut_file):
                with open(cut_file, &#39;r&#39;) as f_cut:
                    while True:
                        line = f_cut.readline()
                        if line == &#39;&#39;:
                            break
                        if line.startswith(&#39;Exact_cut&#39;):
                            unit_ID = np.fromfile(f_cut, dtype=&#39;uint8&#39;, sep=&#39; &#39;)
            
            elif os.path.isfile(clu_file):
                data = np.loadtxt(clu_file)
                unit_ID = data[1:].flatten() - 1
                    
            else:
                logging.error(
                    &#34;No cluster file found for spike file {} please make one at {} or {}&#34;.format(
                        file_name, cut_file, clu_file))
                return
            self._set_timestamp(spike_time)
            self._set_waveform(spike_wave)
            self.set_unit_tags(unit_ID)

    def load_spike_Neuralynx(self, file_name):
        &#34;&#34;&#34;
        Decodes spike data from Neuralynx file format
        
        Parameters
        ----------
        file_name : str
            Full file directory for the spike data
        
        Returns
        -------
        None
        
        &#34;&#34;&#34;
        self._set_data_source(file_name)
        self._set_source_format(&#39;Neuralynx&#39;)

        # Format description for the NLX file:
        file_ext = file_name[-3:]
        if file_ext == &#39;ntt&#39;:
            tot_channels = 4
        elif file_ext == &#39;nst&#39;:
            tot_channels = 2
        elif file_ext == &#39;nse&#39;:
            tot_channels = 1
        header_offset = 16*1024 # fixed for NLX files

        bytes_per_timestamp = 8
        bytes_chan_no = 4
        bytes_cell_no = 4
        bytes_per_feature = 4
        num_features = 8
        bytes_features = bytes_per_feature*num_features
        bytes_per_sample = 2
        samples_per_record = 32
        channel_pack_size = bytes_per_sample*tot_channels# ch1|ch2|ch3|ch4 each with 2 bytes

        max_byte_value = np.power(2, bytes_per_sample*8)
        max_ADC_count = np.power(2, bytes_per_sample*8- 1)-1
        AD_bit_uvolts = np.ones([tot_channels, ])*10**-6 # Default value

        record_size = None
        with open(file_name, &#39;rb&#39;) as f:
            while True:
                line = f.readline()
                try:
                    line = line.decode(&#39;UTF-8&#39;)
                except:
                    break

                if line == &#39;&#39;:
                    break
                if &#39;SamplingFrequency&#39; in line:
                    self._set_sampling_rate(float(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line))))
                if &#39;RecordSize&#39; in line:
                    record_size = int(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line)))
                if &#39;Time Opened&#39; in line:
                    self._set_date(re.search(r&#39;\d+/\d+/\d+&#39;, line).group())
                    self._set_time(re.search(r&#39;\d+:\d+:\d+&#39;, line).group())
                if &#39;FileVersion&#39; in line:
                    self._set_file_version(line.split()[1])
                if &#39;ADMaxValue&#39; in line:
                    max_ADC_count = float(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line)))
                if &#39;ADBitVolts&#39; in line:
                    AD_bit_uvolts = np.array([float(x)*(10**6) for x in re.findall(r&#39;\d+.\d+|\d+&#39;, line)])
                if &#39;ADChannel&#39; in line:
                    self._set_channel_ids(np.array([int(x) for x in re.findall(r&#39;\d+&#39;, line)]))
                if &#39;NumADChannels&#39; in line:
                    tot_channels = int(&#39;&#39;.join(re.findall(r&#39;\d+&#39;, line)))

            self._set_fullscale_mv((max_byte_value/2)*AD_bit_uvolts) # gain = 1 assumed to keep in similarity to Axona
            self._set_bytes_per_sample(bytes_per_sample)
            self._set_samples_per_spike(samples_per_record)
            self._set_timestamp_bytes(bytes_per_timestamp)
            self._set_total_channels(tot_channels)

            if not record_size:
                record_size = bytes_per_timestamp+ \
                             bytes_chan_no+ \
                             bytes_cell_no+ \
                             bytes_features+ \
                             bytes_per_sample*samples_per_record*tot_channels

            time_offset = 0
            unitID_offset = bytes_per_timestamp+ \
                           bytes_chan_no
            sample_offset = bytes_per_timestamp+ \
                           bytes_chan_no+ \
                           bytes_cell_no+ \
                           bytes_features
            f.seek(0, 2)
            num_spikes = int((f.tell()- header_offset)/record_size)
            self._set_total_spikes(num_spikes)

            f.seek(header_offset, 0)
            spike_time = np.zeros([num_spikes, ])
            unit_ID = np.zeros([num_spikes, ], dtype=int)
            spike_wave = oDict()
            sample_le = 256**(np.arange(bytes_per_sample))
            for i in np.arange(tot_channels):
                spike_wave[&#39;ch&#39;+ str(i+1)] = np.zeros([num_spikes, samples_per_record])

            for i in np.arange(num_spikes):
                sample_bytes = np.fromfile(f, dtype=&#39;uint8&#39;, count=record_size)
                spike_time[i] = int.from_bytes(sample_bytes[time_offset+ np.arange(bytes_per_timestamp)], byteorder=&#39;little&#39;, signed=False)/10**6
                unit_ID[i] = int.from_bytes(sample_bytes[unitID_offset+ np.arange(bytes_cell_no)], byteorder=&#39;little&#39;, signed=False)

                for j in range(tot_channels):
                    sample_value = np.zeros([samples_per_record, bytes_per_sample])
                    ind = sample_offset+ j*bytes_per_sample+ np.arange(samples_per_record)*channel_pack_size
                    for k in np.arange(bytes_per_sample):
                        sample_value[:, k] = sample_bytes[ind+ k]
                    sample_value = sample_value.dot(sample_le)
                    np.putmask(sample_value, sample_value &gt; max_ADC_count, sample_value- max_byte_value)
                    spike_wave[&#39;ch&#39;+ str(j+1)][i, :] = sample_value*AD_bit_uvolts[j]
            spike_time -= spike_time.min()
            self._set_duration(spike_time.max())
            self._set_timestamp(spike_time)
            self._set_waveform(spike_wave)
            self.set_unit_tags(unit_ID)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="neurochat.nc_base.NBase" href="nc_base.html#neurochat.nc_base.NBase">NBase</a></li>
<li><a title="neurochat.nc_base.NAbstract" href="nc_base.html#neurochat.nc_base.NAbstract">NAbstract</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="neurochat.nc_spike.NSpike.add_lfp"><code class="name flex">
<span>def <span class="ident">add_lfp</span></span>(<span>self, lfp=None, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Adds new LFP node to current NSpike() object</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>lfp</code></strong> :&ensp;<code>NLfp</code></dt>
<dd>NLfp object. If None, new object is created</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>:obj:Nlfp</code>
A new NLfp() object</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def add_lfp(self, lfp=None, **kwargs):
    &#34;&#34;&#34;
    Adds new LFP node to current NSpike() object
    
    Parameters
    ----------
    lfp : NLfp
        NLfp object. If None, new object is created
    
    Returns
    -------
    `:obj:Nlfp`
        A new NLfp() object

    &#34;&#34;&#34;
    
    try:
        data_type = lfp.get_type()
    except:
        logging.error(&#39;The data type of the added object cannot be determined!&#39;)

    if data_type == &#39;lfp&#39;:
            cls= lfp.___class__ 
    else:
        cls = None

    new_lfp = self._add_node(cls, lfp, &#39;lfp&#39;, **kwargs)
    
    return new_lfp</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.add_spike"><code class="name flex">
<span>def <span class="ident">add_spike</span></span>(<span>self, spike=None, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Adds new spike node to current NSpike() object</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>spike</code></strong> :&ensp;<a title="neurochat.nc_spike.NSpike" href="#neurochat.nc_spike.NSpike"><code>NSpike</code></a></dt>
<dd>NSPike object. If None, new object is created</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>:obj:NSpike</code>
A new NSpike() object</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def add_spike(self, spike=None, **kwargs):
    &#34;&#34;&#34;
    Adds new spike node to current NSpike() object
    
    Parameters
    ----------
    spike : NSpike
        NSPike object. If None, new object is created
    
    Returns
    -------
    `:obj:NSpike`
        A new NSpike() object

    &#34;&#34;&#34;
    new_spike = self._add_node(self.__class__, spike, &#39;spike&#39;, **kwargs)
    
    return new_spike</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.burst"><code class="name flex">
<span>def <span class="ident">burst</span></span>(<span>self, burst_thresh=5, ibi_thresh=50)</span>
</code></dt>
<dd>
<section class="desc"><p>Analysis of bursting properties of the spiking train</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>burst_thresh</code></strong> :&ensp;<code>int</code></dt>
<dd>Minimum ISI between consecutive spikes in a burst</dd>
<dt><strong><code>ibi_thresh</code></strong> :&ensp;<code>int</code></dt>
<dd>Minimum inter-burst interval between two bursting groups of spikes</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def burst(self, burst_thresh=5, ibi_thresh=50):
    &#34;&#34;&#34;
    Analysis of bursting properties of the spiking train
    
    Parameters
    ----------
    burst_thresh : int
        Minimum ISI between consecutive spikes in a burst
        
    ibi_thresh : int
        Minimum inter-burst interval between two bursting groups of spikes

    Returns
    -------
    None

    &#34;&#34;&#34;

    _results = oDict()

    unitStamp = self.get_unit_stamp()
    isi = 1000*np.diff(unitStamp)

    burst_start = []
    burst_end = []
    burst_duration = []
    spikesInBurst = []
    bursting_isi = []
    num_burst = 0
    ibi = []
    duty_cycle = []
    k = 0
    while k &lt; isi.size:
        if isi[k] &lt;= burst_thresh:
            burst_start.append(k)
            spikesInBurst.append(2)
            bursting_isi.append(isi[k])
            burst_duration.append(isi[k])
            m = k+1
            while m &lt; isi.size and isi[m] &lt;= burst_thresh:
                spikesInBurst[num_burst] += 1
                bursting_isi.append(isi[m])
                burst_duration[num_burst] += isi[m]
                m += 1
            burst_duration[num_burst] += 1 # to compensate for the span of the last spike
            burst_end.append(m)
            k = m+1
            num_burst += 1
        else:
            k += 1
    if num_burst:
        for j in range(0, num_burst-1):
            ibi.append(unitStamp[burst_start[j+1]]- unitStamp[burst_end[j]])
        duty_cycle = np.divide(burst_duration[1:], ibi)/1000 # ibi in sec, burst_duration in ms
    else:
        logging.warning(
            &#39;No burst detected in {}&#39;.format(self.get_filename()))

    spikesInBurst = np.array(spikesInBurst) if spikesInBurst else np.array([])
    bursting_isi = np.array(bursting_isi) if bursting_isi else np.array([])
    ibi = 1000*np.array(ibi) if ibi else np.array([]) # in sec unit, so converted to ms
    burst_duration = np.array(burst_duration) if burst_duration else np.array([])
    duty_cycle = np.array(duty_cycle) if len(duty_cycle) else np.array([])

    _results[&#39;Total burst&#39;] = num_burst
    _results[&#39;Total bursting spikes&#39;] = spikesInBurst.sum()
    _results[&#39;Mean bursting ISI ms&#39;] = bursting_isi.mean() if bursting_isi.any() else None
    _results[&#39;Std bursting ISI ms&#39;] = bursting_isi.std() if bursting_isi.any() else None
    _results[&#39;Mean spikes per burst&#39;] = spikesInBurst.mean() if spikesInBurst.any() else None
    _results[&#39;Std spikes per burst&#39;] = spikesInBurst.std() if spikesInBurst.any() else None
    _results[&#39;Mean burst duration ms&#39;] = burst_duration.mean() if burst_duration.any() else None
    _results[&#39;Std burst duration&#39;] = burst_duration.std() if burst_duration.any() else None
    _results[&#39;Mean duty cycle&#39;] = duty_cycle.mean() if duty_cycle.any() else None
    _results[&#39;Std duty cycle&#39;] = duty_cycle.std() if duty_cycle.any() else None
    _results[&#39;Mean IBI&#39;] = ibi.mean() if ibi.any() else None
    _results[&#39;Std IBI&#39;] = ibi.std() if ibi.any() else None
    _results[&#39;Propensity to burst&#39;] = spikesInBurst.sum()/ unitStamp.size
    
    self.update_result(_results)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.get_bytes_per_sample"><code class="name flex">
<span>def <span class="ident">get_bytes_per_sample</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the number of bytes to represent each spike waveform sample</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Number of bytes to represent each sample of the spike waveforms</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_bytes_per_sample(self):
    &#34;&#34;&#34;
    Returns the number of bytes to represent each spike waveform sample
            
    Parameters
    ----------
    None

    Returns
    -------
    int
        Number of bytes to represent each sample of the spike waveforms

    &#34;&#34;&#34;

    return self._record_info[&#39;Bytes per sample&#39;]</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.get_channel_ids"><code class="name flex">
<span>def <span class="ident">get_channel_ids</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the identities of individual channels</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>Identities of individual channels</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_channel_ids(self):
    &#34;&#34;&#34;
    Returns the identities of individual channels
            
    Parameters
    ----------
    None

    Returns
    -------
    list
        Identities of individual channels 

    &#34;&#34;&#34;
    
    return self._record_info[&#39;Channel IDs&#39;]</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.get_fullscale_mv"><code class="name flex">
<span>def <span class="ident">get_fullscale_mv</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the fullscale value of the ADC in mV</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Fullscale ADC value in mV</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_fullscale_mv(self):
    &#34;&#34;&#34;
    Returns the fullscale value of the ADC in mV
            
    Parameters
    ----------
    None

    Returns
    -------
    int
        Fullscale ADC value in mV

    &#34;&#34;&#34;
    
    return self._record_info[&#39;ADC Fullscale mv&#39;]</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.get_samples_per_spike"><code class="name flex">
<span>def <span class="ident">get_samples_per_spike</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the number of bytes to represent each timestamp in the binary file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Number of bytes to represent timestamps</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_samples_per_spike(self):
    &#34;&#34;&#34;
    Returns the number of bytes to represent each timestamp in the binary file
            
    Parameters
    ----------
    None

    Returns
    -------
    int
        Number of bytes to represent timestamps

    &#34;&#34;&#34;
    
    return self._record_info[&#39;Samples per spike&#39;]</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.get_sampling_rate"><code class="name flex">
<span>def <span class="ident">get_sampling_rate</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the sampling rate of spike waveforms</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Sampling rate for spike waveforms</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_sampling_rate(self):
    &#34;&#34;&#34;
    Returns the sampling rate of spike waveforms
            
    Parameters
    ----------
    None

    Returns
    -------
    int
        Sampling rate for spike waveforms

    &#34;&#34;&#34;
    return self._record_info[&#39;Sampling rate&#39;]</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.get_timebase"><code class="name flex">
<span>def <span class="ident">get_timebase</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the timebase for spike event timestamps</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Timebase for spike event timestamps</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_timebase(self):
    &#34;&#34;&#34;
    Returns the timebase for spike event timestamps
            
    Parameters
    ----------
    None

    Returns
    -------
    int
        Timebase for spike event timestamps

    &#34;&#34;&#34;

    return self._record_info[&#39;Timebase&#39;]</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.get_timestamp"><code class="name flex">
<span>def <span class="ident">get_timestamp</span></span>(<span>self, unit_no=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the timestamps of the spike-waveforms of spefied unit</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>Timestamps of the spiking waveforms</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_timestamp(self, unit_no=None):
    &#34;&#34;&#34;
    Returns the timestamps of the spike-waveforms of spefied unit
    
    Parameters
    ----------
    None
    
    Returns
    -------
    ndarray
        Timestamps of the spiking waveforms
    &#34;&#34;&#34;
    
    if unit_no is None:
        return self._timestamp
    else:
        if unit_no in self._unit_list:
            return self._timestamp[self._unit_Tags == unit_no]
        else:
            logging.warning(&#39;Unit &#39; + str(unit_no) + &#39; is not present in the spike data&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.get_timestamp_bytes"><code class="name flex">
<span>def <span class="ident">get_timestamp_bytes</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the number of bytes to represent each timestamp in the binary file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Number of bytes to represent timestamps</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_timestamp_bytes(self):
    &#34;&#34;&#34;
    Returns the number of bytes to represent each timestamp in the binary file
            
    Parameters
    ----------
    None

    Returns
    -------
    int
        Number of bytes to represent timestamps

    &#34;&#34;&#34;
    return self._record_info[&#39;Bytes per timestamp&#39;]</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.get_total_channels"><code class="name flex">
<span>def <span class="ident">get_total_channels</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns total number of electrode channels in the spike data file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Total number of electrode channels</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_total_channels(self):
    &#34;&#34;&#34;
    Returns total number of electrode channels in the spike data file
            
    Parameters
    ----------
    None

    Returns
    -------
    int
        Total number of electrode channels

    &#34;&#34;&#34;
    
    return self._record_info[&#39;No of channels&#39;]</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.get_total_spikes"><code class="name flex">
<span>def <span class="ident">get_total_spikes</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns total number of spikes in the recording</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Total number of spikes</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_total_spikes(self):
    &#34;&#34;&#34;
    Returns total number of spikes in the recording
            
    Parameters
    ----------
    None

    Returns
    -------
    int
        Total number of spikes

    &#34;&#34;&#34;

    return self._record_info[&#39;No of spikes&#39;]</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.get_type"><code class="name flex">
<span>def <span class="ident">get_type</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the type of object. For NSpike, this is always <code>spike</code> type</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_type(self):
    &#34;&#34;&#34;
    Returns the type of object. For NSpike, this is always `spike` type
    
    Parameters
    ----------
    None
    
    Returns
    -------
    str

    &#34;&#34;&#34;
    
    return self.__type </code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.get_unit_list"><code class="name flex">
<span>def <span class="ident">get_unit_list</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Gets the list of the units</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>List of the unique tags of spiking-waveforms from clustering</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_unit_list(self):
    &#34;&#34;&#34;
    Gets the list of the units
    
    Parameters
    ----------
    None
    
    Returns
    -------
    list
        List of the unique tags of spiking-waveforms from clustering

    &#34;&#34;&#34;
    
    return self._unit_list</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.get_unit_no"><code class="name flex">
<span>def <span class="ident">get_unit_no</span></span>(<span>self, spike_name=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Gets currently set unit number of the spike dataset to analyse</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Unit or cell number set to analyse</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_unit_no(self, spike_name=None):
    &#34;&#34;&#34;
    Gets currently set unit number of the spike dataset to analyse
    
    Parameters
    ----------
    None
    
    Returns
    -------
    int
        Unit or cell number set to analyse

    &#34;&#34;&#34;
    if spike_name is None:
        unit_no = self._unit_no
    else:
        unit_no = []
        spikes = self.get_spike(spike_name)
        for spike in spikes:
            unit_no.append(spike._unit_no)
    return unit_no</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.get_unit_spikes_count"><code class="name flex">
<span>def <span class="ident">get_unit_spikes_count</span></span>(<span>self, unit_no=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the number of spikes in a unit</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>unit_no</code></strong> :&ensp;<code>int</code></dt>
<dd>Units whose spike count is returned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Number of units spikes of a unit in a recording session</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_unit_spikes_count(self, unit_no=None):
    &#34;&#34;&#34;
    Returns the number of spikes in a unit
    
    Parameters
    ----------
    unit_no : int
        Units whose spike count is returned
    
    Returns
    -------
    int
        Number of units spikes of a unit in a recording session

    &#34;&#34;&#34;
    
    if unit_no is None:
        unit_no = self._unit_no
    if unit_no in self._unit_list:
        return sum(self._unit_Tags == unit_no)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.get_unit_stamp"><code class="name flex">
<span>def <span class="ident">get_unit_stamp</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Gets the timestamps for currently set unit to analyse</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code> or <code>ndarray</code></dt>
<dd>Timestamps for currently set unit</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_unit_stamp(self):
    &#34;&#34;&#34;
    Gets the timestamps for currently set unit to analyse
    
    Parameters
    ----------
    None
    
    Returns
    -------
    list or ndarray
        Timestamps for currently set unit

    &#34;&#34;&#34;
    
    return self.get_timestamp(self._unit_no)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.get_unit_stamps_in_ranges"><code class="name flex">
<span>def <span class="ident">get_unit_stamps_in_ranges</span></span>(<span>self, ranges)</span>
</code></dt>
<dd>
<section class="desc"><p>Return the unit timestamps in a list of ranges.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ranges</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of tuples indicating time ranges to get stamps in</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>The timestamps</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_unit_stamps_in_ranges(self, ranges):
    &#34;&#34;&#34;
    Return the unit timestamps in a list of ranges.

    Parameters
    ----------
    ranges : list
        A list of tuples indicating time ranges to get stamps in
    
    Returns
    -------
    list
        The timestamps
    &#34;&#34;&#34;
    stamps = self.get_unit_stamp()
    new_stamps = [
        val for val in stamps
        if any(lower &lt;= val &lt;= upper for (lower, upper) in ranges)
    ]
    return new_stamps</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.get_unit_tags"><code class="name flex">
<span>def <span class="ident">get_unit_tags</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the unit number or tags of the clustered units</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code> <code>ot</code> <code>ndarray</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_unit_tags(self):
    
    &#34;&#34;&#34;
    Returns the unit number or tags of the clustered units
    
    Parameters
    ----------
    None
    
    Returns
    -------
    list ot ndarray

    &#34;&#34;&#34;
    
    return self._unit_Tags</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.get_unit_waves"><code class="name flex">
<span>def <span class="ident">get_unit_waves</span></span>(<span>self, unit_no=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns spike waveform of a specified unit</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>unit_no</code></strong> :&ensp;<code>int</code></dt>
<dd>Unit whose waveforms are to be returned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>OrderedDict</code></dt>
<dd>Waveforms of the specified unit. If None, waveforms of currently set
unit are returned</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_unit_waves(self, unit_no=None):
    &#34;&#34;&#34;
    Returns spike waveform of a specified unit
    
    Parameters
    ----------
    unit_no : int
        Unit whose waveforms are to be returned
    
    Returns
    -------
    OrderedDict
        Waveforms of the specified unit. If None, waveforms of currently set
        unit are returned

    &#34;&#34;&#34;
    
    if unit_no is None:
        unit_no = self._unit_no
    _waves = oDict()
    for chan, wave in self._waveform.items():
        _waves[chan] = wave[self._unit_Tags == unit_no, :]
    
    return _waves</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.get_waveform"><code class="name flex">
<span>def <span class="ident">get_waveform</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns spike-waveforms</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>OrderedDict</code></dt>
<dd>Dictionary of spiking waveforms where keys represent the channel number</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_waveform(self):
    &#34;&#34;&#34;
    Returns spike-waveforms
    
    Parameters
    ----------
    None
    
    Returns
    -------
    OrderedDict
        Dictionary of spiking waveforms where keys represent the channel number

    &#34;&#34;&#34;
    
    return self._waveform</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.isi"><code class="name flex">
<span>def <span class="ident">isi</span></span>(<span>self, bins='auto', bound=None, density=False, refractory_threshold=2)</span>
</code></dt>
<dd>
<section class="desc"><p>Calulates the ISI histogram of the spike train</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>bins</code></strong> :&ensp;<code>str</code> or <code>int</code></dt>
<dd>Number of ISI histogram bins. If 'auto', NumPy default is used</dd>
<dt><strong><code>bound</code></strong> :&ensp;<code>int</code></dt>
<dd>Length of the ISI histogram in msec</dd>
<dt><strong><code>density</code></strong> :&ensp;<code>bool</code></dt>
<dd>If true, normalized historagm is calcultaed</dd>
<dt><strong><code>refractory_threshold</code></strong> :&ensp;<code>int</code></dt>
<dd>Length of the refractory period in msec</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def isi(self, bins=&#39;auto&#39;, bound=None, density=False, 
        refractory_threshold=2):
    &#34;&#34;&#34;
    Calulates the ISI histogram of the spike train
    
    Parameters
    ----------
    bins : str or int
        Number of ISI histogram bins. If &#39;auto&#39;, NumPy default is used
        
    bound : int
        Length of the ISI histogram in msec
    density : bool
        If true, normalized historagm is calcultaed
    refractory_threshold : int
        Length of the refractory period in msec

    Returns
    -------
    dict
        Graphical data of the analysis

    &#34;&#34;&#34;
    graph_data = oDict()
    _results = oDict()

    unitStamp = self.get_unit_stamp()
    isi = 1000*np.diff(unitStamp)

    below_refractory = isi[isi &lt; refractory_threshold]

    graph_data[&#39;isiHist&#39;], edges = np.histogram(isi, bins=bins, range=bound, density=density)
    graph_data[&#39;isiBins&#39;] = edges[:-1]
    graph_data[&#39;isiBinCentres&#39;] = edges[:-1] + np.mean(np.diff(edges))
    graph_data[&#39;isi&#39;] = isi
    graph_data[&#39;maxCount&#39;] = graph_data[&#39;isiHist&#39;].max()
    graph_data[&#39;isiBefore&#39;] = isi[:-1]
    graph_data[&#39;isiAfter&#39;] = isi[1:]

    _results[&#34;Mean ISI&#34;] = isi.mean()
    _results[&#34;Std ISI&#34;] = isi.std()
    _results[&#34;Refractory violation&#34;] = (
        below_refractory.size / unitStamp.size)

    self.update_result(_results)
    return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.isi_corr"><code class="name flex">
<span>def <span class="ident">isi_corr</span></span>(<span>self, spike=None, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates the correlation of ISI histogram.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>spike</code></strong> :&ensp;<a title="neurochat.nc_spike.NSpike" href="#neurochat.nc_spike.NSpike"><code>NSpike</code></a>()</dt>
<dd>If specified, it calulates cross-correlation.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def isi_corr(self, spike=None, **kwargs):
    &#34;&#34;&#34;
    Calculates the correlation of ISI histogram.
    
    Parameters
    ----------
    spike : NSpike()
        If specified, it calulates cross-correlation.
        
    **kwargs
        Keyword arguments

    Returns
    -------
    dict
        Graphical data of the analysis

    &#34;&#34;&#34;
    
    graph_data = oDict()
    if spike is None:
        _unit_stamp = np.copy(self.get_unit_stamp())
    elif isinstance(spike, int):
        if spike in self.get_unit_list():
            _unit_stamp = self.get_timestamp(spike)
    else:
        if isinstance(spike, str):
            spike = self.get_spike(spike)
        if isinstance(spike, self.__class__):
            _unit_stamp = spike.get_unit_stamp()
        else:
            logging.error(&#39;No valid spike specified&#39;)

    _corr = self.psth(_unit_stamp, **kwargs)
    graph_data[&#39;isiCorrBins&#39;] = _corr[&#39;bins&#39;]
    graph_data[&#39;isiAllCorrBins&#39;] = _corr[&#39;all_bins&#39;]
    center = find(_corr[&#39;bins&#39;] == 0, 1, &#39;first&#39;)[0]
    graph_data[&#39;isiCorr&#39;] = _corr[&#39;psth&#39;]
    graph_data[&#39;isiCorr&#39;][center] = graph_data[&#39;isiCorr&#39;][center] \
                                - np.min([self.get_unit_stamp().size, _unit_stamp.size])

    return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, filename=None, system=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Loads spike datasets</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the spike datafile</dd>
<dt><strong><code>system</code></strong> :&ensp;<code>str</code></dt>
<dd>Recording system or format of the spike data file</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="see-also">See also</h2>
<p><code>load_spike_axona()</code>, <code>load_spike_NLX()</code>, <code>load_spike_NWB()</code></p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def load(self, filename=None, system=None):
    &#34;&#34;&#34;
    Loads spike datasets
    
    Parameters
    ----------
    filename : str
        Name of the spike datafile
    system : str
        Recording system or format of the spike data file
    
    Returns
    -------
    None
    
    See also
    --------
    load_spike_axona(), load_spike_NLX(), load_spike_NWB()
        
    &#34;&#34;&#34;
    
    if system is None:
        system = self._system
    else:
        self._system = system
    if filename is None:
        filename = self._filename
    else:
        self._filename = filename
    loader = getattr(self, &#39;load_spike_&#39;+ system)
    loader(filename)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.load_lfp"><code class="name flex">
<span>def <span class="ident">load_lfp</span></span>(<span>self, names='all')</span>
</code></dt>
<dd>
<section class="desc"><p>Loads datasets of the LFP nodes. Name of each node is used for obtaining the
filenames</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>names</code></strong> :&ensp;<code>list</code> of <code>str</code></dt>
<dd>Names of the nodes to load. If <code>all</code>, all LFP nodes are loaded</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def load_lfp(self, names=&#39;all&#39;):
    &#34;&#34;&#34;
    Loads datasets of the LFP nodes. Name of each node is used for obtaining the
    filenames
    
    Parameters
    ----------
    names : list of str
        Names of the nodes to load. If `all`, all LFP nodes are loaded
    
    Returns
    -------
    None

    &#34;&#34;&#34;
    
    if names == &#39;all&#39;:
        for lfp in self._lfp:
            lfp.load()
    else:
        logging.error(&#34;Lfp by name has yet to be implemented&#34;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.load_spike"><code class="name flex">
<span>def <span class="ident">load_spike</span></span>(<span>self, names=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Loads datasets of the spike nodes. Name of each node is used for obtaining the
filenames</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>names</code></strong> :&ensp;<code>list</code> of <code>str</code></dt>
<dd>Names of the nodes to load. If None, current NSpike() object is loaded</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def load_spike(self, names=None):
    &#34;&#34;&#34;
    Loads datasets of the spike nodes. Name of each node is used for obtaining the
    filenames
    
    Parameters
    ----------
    names : list of str
        Names of the nodes to load. If None, current NSpike() object is loaded
    
    Returns
    -------
    None

    &#34;&#34;&#34;
    
    if names is None:
        self.load()
    elif names == &#39;all&#39;:
        for spike in self._spikes:
            spike.load()
    else:
        logging.error(&#34;Spikes by name has yet to be implemented&#34;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.load_spike_Axona"><code class="name flex">
<span>def <span class="ident">load_spike_Axona</span></span>(<span>self, file_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Decodes spike data from Axona file format</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Full file directory for the spike data</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def load_spike_Axona(self, file_name):
    &#34;&#34;&#34;
    Decodes spike data from Axona file format
    
    Parameters
    ----------
    file_name : str
        Full file directory for the spike data
    
    Returns
    -------
    None
    
    &#34;&#34;&#34;
    words = file_name.split(sep=os.sep)
    file_directory = os.sep.join(words[0:-1])
    file_tag = words[-1].split(sep=&#39;.&#39;)[0]
    tet_no = words[-1].split(sep=&#39;.&#39;)[1]
    set_file = file_directory + os.sep + file_tag + &#39;.set&#39;
    cut_file = file_directory + os.sep + file_tag + &#39;_&#39; + tet_no + &#39;.cut&#39;
    clu_file = file_directory + os.sep + file_tag + &#39;.clu.&#39; + tet_no

    self._set_data_source(file_name)
    self._set_source_format(&#39;Axona&#39;)

    with open(file_name, &#39;rb&#39;) as f:
        while True:
            line = f.readline()
            try:
                line = line.decode(&#39;latin-1&#39;)
            except:
                break

            if line == &#39;&#39;:
                break
            if line.startswith(&#39;trial_date&#39;):
                self._set_date(&#39; &#39;.join(line.replace(&#39;,&#39;, &#39; &#39;).split()[1:]))
            if line.startswith(&#39;trial_time&#39;):
                self._set_time(line.split()[1])
            if line.startswith(&#39;experimenter&#39;):
                self._set_experiemnter(&#39; &#39;.join(line.split()[1:]))
            if line.startswith(&#39;comments&#39;):
                self._set_comments(&#39; &#39;.join(line.split()[1:]))
            if line.startswith(&#39;duration&#39;):
                self._set_duration(float(&#39;&#39;.join(line.split()[1:])))
            if line.startswith(&#39;sw_version&#39;):
                self._set_file_version(line.split()[1])
            if line.startswith(&#39;num_chans&#39;):
                self._set_total_channels(int(&#39;&#39;.join(line.split()[1:])))
            if line.startswith(&#39;timebase&#39;):
                self._set_timebase(int(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line))))
            if line.startswith(&#39;bytes_per_timestamp&#39;):
                self._set_timestamp_bytes(int(&#39;&#39;.join(line.split()[1:])))
            if line.startswith(&#39;samples_per_spike&#39;):
                self._set_samples_per_spike(int(&#39;&#39;.join(line.split()[1:])))
            if line.startswith(&#39;sample_rate&#39;):
                self._set_sampling_rate(int(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line))))
            if line.startswith(&#39;bytes_per_sample&#39;):
                self._set_bytes_per_sample(int(&#39;&#39;.join(line.split()[1:])))
            if line.startswith(&#39;num_spikes&#39;):
                self._set_total_spikes(int(&#39;&#39;.join(line.split()[1:])))
            if line.startswith(&#39;data_start&#39;):
                break

        num_spikes = self.get_total_spikes()
        bytes_per_timestamp = self.get_timestamp_bytes()
        bytes_per_sample = self.get_bytes_per_sample()
        samples_per_spike = self.get_samples_per_spike()

        f.seek(0, 0)
        header_offset = []
        while True:
            try:
                buff = f.read(10).decode(&#39;UTF-8&#39;)
                if buff == &#39;data_start&#39;:
                    header_offset = f.tell()
                    break
                else:
                    f.seek(-9, 1)
            except:
                break

        tot_channels = self.get_total_channels()
        self._set_channel_ids([(int(tet_no) - 1)*tot_channels + x for x in range(tot_channels)])
        max_ADC_count = 2**(8*bytes_per_sample - 1) - 1
        max_byte_value = 2**(8*bytes_per_sample)

        with open(set_file, &#39;r&#39;, encoding=&#39;latin-1&#39;) as f_set:
            lines = f_set.readlines()
            gain_lines = dict([tuple(map(int, re.findall(r&#39;\d+.\d+|\d+&#39;, line)[0].split()))\
                        for line in lines if &#39;gain_ch_&#39; in line])
            gains = np.array([gain_lines[ch_id] for ch_id in self.get_channel_ids()])
            for line in lines:
                if line.startswith(&#39;ADC_fullscale_mv&#39;):
                    self._set_fullscale_mv(int(re.findall(r&#39;\d+.\d+|d+&#39;, line)[0]))
                    break
            AD_bit_uvolts = 2*self.get_fullscale_mv()*10**3/ \
                             (gains*(2**(8*bytes_per_sample)))

        record_size = tot_channels*(bytes_per_timestamp + \
                        bytes_per_sample * samples_per_spike)
        time_be = 256**(np.arange(bytes_per_timestamp, 0, -1)-1)
        sample_le = 256**(np.arange(0, bytes_per_sample, 1))

        if not header_offset:
            print(&#39;Error: data_start marker not found!&#39;)
        else:
            f.seek(header_offset, 0)
            byte_buffer = np.fromfile(f, dtype=&#39;uint8&#39;)
            spike_time = np.zeros([num_spikes, ], dtype=&#39;uint32&#39;)
            for i in list(range(0, bytes_per_timestamp)):
                byte = byte_buffer[i:len(byte_buffer):record_size]
                byte = byte[:num_spikes]
                spike_time = spike_time + time_be[i]*byte
            spike_time = spike_time/ self.get_timebase()
            spike_time = spike_time.reshape((num_spikes, ))

            spike_wave = oDict()


            for i in np.arange(tot_channels):
                chan_offset = (i+1)*bytes_per_timestamp+ i*bytes_per_sample*samples_per_spike
                chan_wave = np.zeros([num_spikes, samples_per_spike], dtype=np.float64)
                for j in np.arange(0, samples_per_spike, 1):
                    sample_offset = j*bytes_per_sample + chan_offset
                    for k in np.arange(0, bytes_per_sample, 1):
                        byte_offset = k + sample_offset
                        sample_value = sample_le[k]* byte_buffer[byte_offset \
                                      : len(byte_buffer)+ byte_offset-record_size\
                                      :record_size]
                        sample_value = sample_value.astype(np.float64, casting=&#39;unsafe&#39;, copy=False)
                        np.add(chan_wave[:, j], sample_value, out=chan_wave[:, j])
                    np.putmask(chan_wave[:, j], chan_wave[:, j] &gt; max_ADC_count, chan_wave[:, j]- max_byte_value)
                spike_wave[&#39;ch&#39;+ str(i+1)] = chan_wave*AD_bit_uvolts[i]

        if os.path.isfile(cut_file):
            with open(cut_file, &#39;r&#39;) as f_cut:
                while True:
                    line = f_cut.readline()
                    if line == &#39;&#39;:
                        break
                    if line.startswith(&#39;Exact_cut&#39;):
                        unit_ID = np.fromfile(f_cut, dtype=&#39;uint8&#39;, sep=&#39; &#39;)
        
        elif os.path.isfile(clu_file):
            data = np.loadtxt(clu_file)
            unit_ID = data[1:].flatten() - 1
                
        else:
            logging.error(
                &#34;No cluster file found for spike file {} please make one at {} or {}&#34;.format(
                    file_name, cut_file, clu_file))
            return
        self._set_timestamp(spike_time)
        self._set_waveform(spike_wave)
        self.set_unit_tags(unit_ID)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.load_spike_NWB"><code class="name flex">
<span>def <span class="ident">load_spike_NWB</span></span>(<span>self, file_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Decodes spike data from NWB (HDF5) file format</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Full file directory for the spike data</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def load_spike_NWB(self, file_name):
    &#34;&#34;&#34;
    Decodes spike data from NWB (HDF5) file format
    
    Parameters
    ----------
    file_name : str
        Full file directory for the spike data
    
    Returns
    -------
    None
    
    &#34;&#34;&#34;
    
    file_name, path = file_name.split(&#39;+&#39;)
    
    if os.path.exists(file_name):
        hdf = Nhdf()
        hdf.set_filename(file_name)

        _record_info = {}

        if path in hdf.f:
            g = hdf.f[path]
        elif &#39;/processing/Shank/&#39;+ path in hdf.f:
            path = &#39;/processing/Shank/&#39;+ path
            g = hdf.f[path]
        else:
            logging.error(&#39;Specified shank datapath does not exist!&#39;)

        for key, value in g.attrs.items():
            _record_info[key] = value
        self.set_record_info(_record_info)

        path_clust = &#39;Clustering&#39;
        path_wave = &#39;EventWaveForm/WaveForm&#39;

        if path_clust in g:
            g_clust = g[path_clust]
            self._set_timestamp(hdf.get_dataset(group=g_clust, name=&#39;times&#39;))
            self.set_unit_tags(hdf.get_dataset(group=g_clust, name=&#39;num&#39;))
            self._set_unit_list()
        else:
            logging.error(&#39;There is no /Clustering in the :&#39; +path)

        if path_wave in g:
            g_wave = g[path_wave]
            self._set_total_spikes(hdf.get_dataset(group=g_wave, name=&#39;num_events&#39;))
            chanIDs = hdf.get_dataset(group=g_wave, name=&#39;electrode_idx&#39;)
            self._set_channel_ids(chanIDs)

            spike_wave = oDict()
            data = hdf.get_dataset(group=g_wave, name=&#39;data&#39;)
            if len(data.shape) == 2:
                num_events, num_samples = data.shape
                tot_chans = 1
            elif len(data.shape) == 3:
                num_events, num_samples, tot_chans = data.shape
            else:
                logging.error(path_wave+ &#39;/data contains for more than 3 dimensions!&#39;)

            if num_events != hdf.get_dataset(group=g_wave, name=&#39;num_events&#39;):
                logging.error(&#39;Mismatch between num_events and 1st dimension of &#39;+ path_wave+ &#39;/data&#39;)
            if num_samples != hdf.get_dataset(group=g_wave, name=&#39;num_samples&#39;):
                logging.error(&#39;Mismatch between num_samples and 2nd dimension of &#39;+ path_wave+ &#39;/data&#39;)
            for i in np.arange(tot_chans):
                spike_wave[&#39;ch&#39;+ str(i+1)] = data[:, :, i]
            self._set_waveform(spike_wave)
        else:
            logging.error(&#39;There is no /EventWaveForm/WaveForm in the :&#39; +path)
        
        hdf.close()
    else:
        logging.error(file_name + &#39; does not exist!&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.load_spike_Neuralynx"><code class="name flex">
<span>def <span class="ident">load_spike_Neuralynx</span></span>(<span>self, file_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Decodes spike data from Neuralynx file format</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Full file directory for the spike data</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def load_spike_Neuralynx(self, file_name):
    &#34;&#34;&#34;
    Decodes spike data from Neuralynx file format
    
    Parameters
    ----------
    file_name : str
        Full file directory for the spike data
    
    Returns
    -------
    None
    
    &#34;&#34;&#34;
    self._set_data_source(file_name)
    self._set_source_format(&#39;Neuralynx&#39;)

    # Format description for the NLX file:
    file_ext = file_name[-3:]
    if file_ext == &#39;ntt&#39;:
        tot_channels = 4
    elif file_ext == &#39;nst&#39;:
        tot_channels = 2
    elif file_ext == &#39;nse&#39;:
        tot_channels = 1
    header_offset = 16*1024 # fixed for NLX files

    bytes_per_timestamp = 8
    bytes_chan_no = 4
    bytes_cell_no = 4
    bytes_per_feature = 4
    num_features = 8
    bytes_features = bytes_per_feature*num_features
    bytes_per_sample = 2
    samples_per_record = 32
    channel_pack_size = bytes_per_sample*tot_channels# ch1|ch2|ch3|ch4 each with 2 bytes

    max_byte_value = np.power(2, bytes_per_sample*8)
    max_ADC_count = np.power(2, bytes_per_sample*8- 1)-1
    AD_bit_uvolts = np.ones([tot_channels, ])*10**-6 # Default value

    record_size = None
    with open(file_name, &#39;rb&#39;) as f:
        while True:
            line = f.readline()
            try:
                line = line.decode(&#39;UTF-8&#39;)
            except:
                break

            if line == &#39;&#39;:
                break
            if &#39;SamplingFrequency&#39; in line:
                self._set_sampling_rate(float(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line))))
            if &#39;RecordSize&#39; in line:
                record_size = int(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line)))
            if &#39;Time Opened&#39; in line:
                self._set_date(re.search(r&#39;\d+/\d+/\d+&#39;, line).group())
                self._set_time(re.search(r&#39;\d+:\d+:\d+&#39;, line).group())
            if &#39;FileVersion&#39; in line:
                self._set_file_version(line.split()[1])
            if &#39;ADMaxValue&#39; in line:
                max_ADC_count = float(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line)))
            if &#39;ADBitVolts&#39; in line:
                AD_bit_uvolts = np.array([float(x)*(10**6) for x in re.findall(r&#39;\d+.\d+|\d+&#39;, line)])
            if &#39;ADChannel&#39; in line:
                self._set_channel_ids(np.array([int(x) for x in re.findall(r&#39;\d+&#39;, line)]))
            if &#39;NumADChannels&#39; in line:
                tot_channels = int(&#39;&#39;.join(re.findall(r&#39;\d+&#39;, line)))

        self._set_fullscale_mv((max_byte_value/2)*AD_bit_uvolts) # gain = 1 assumed to keep in similarity to Axona
        self._set_bytes_per_sample(bytes_per_sample)
        self._set_samples_per_spike(samples_per_record)
        self._set_timestamp_bytes(bytes_per_timestamp)
        self._set_total_channels(tot_channels)

        if not record_size:
            record_size = bytes_per_timestamp+ \
                         bytes_chan_no+ \
                         bytes_cell_no+ \
                         bytes_features+ \
                         bytes_per_sample*samples_per_record*tot_channels

        time_offset = 0
        unitID_offset = bytes_per_timestamp+ \
                       bytes_chan_no
        sample_offset = bytes_per_timestamp+ \
                       bytes_chan_no+ \
                       bytes_cell_no+ \
                       bytes_features
        f.seek(0, 2)
        num_spikes = int((f.tell()- header_offset)/record_size)
        self._set_total_spikes(num_spikes)

        f.seek(header_offset, 0)
        spike_time = np.zeros([num_spikes, ])
        unit_ID = np.zeros([num_spikes, ], dtype=int)
        spike_wave = oDict()
        sample_le = 256**(np.arange(bytes_per_sample))
        for i in np.arange(tot_channels):
            spike_wave[&#39;ch&#39;+ str(i+1)] = np.zeros([num_spikes, samples_per_record])

        for i in np.arange(num_spikes):
            sample_bytes = np.fromfile(f, dtype=&#39;uint8&#39;, count=record_size)
            spike_time[i] = int.from_bytes(sample_bytes[time_offset+ np.arange(bytes_per_timestamp)], byteorder=&#39;little&#39;, signed=False)/10**6
            unit_ID[i] = int.from_bytes(sample_bytes[unitID_offset+ np.arange(bytes_cell_no)], byteorder=&#39;little&#39;, signed=False)

            for j in range(tot_channels):
                sample_value = np.zeros([samples_per_record, bytes_per_sample])
                ind = sample_offset+ j*bytes_per_sample+ np.arange(samples_per_record)*channel_pack_size
                for k in np.arange(bytes_per_sample):
                    sample_value[:, k] = sample_bytes[ind+ k]
                sample_value = sample_value.dot(sample_le)
                np.putmask(sample_value, sample_value &gt; max_ADC_count, sample_value- max_byte_value)
                spike_wave[&#39;ch&#39;+ str(j+1)][i, :] = sample_value*AD_bit_uvolts[j]
        spike_time -= spike_time.min()
        self._set_duration(spike_time.max())
        self._set_timestamp(spike_time)
        self._set_waveform(spike_wave)
        self.set_unit_tags(unit_ID)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.phase_dist"><code class="name flex">
<span>def <span class="ident">phase_dist</span></span>(<span>self, lfp=None, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Analysis of spike to LFP phase distribution</p>
<p>Delegates to NLfp().phase_dist()</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>lfp</code></strong> :&ensp;<code>NLfp</code></dt>
<dd>LFP object which contains the LFP data</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl>
<h2 id="see-also">See also</h2>
<p><code>nc_lfp.NLfp().phase_dist()</code></p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def phase_dist(self, lfp = None, **kwargs):
    &#34;&#34;&#34;
    Analysis of spike to LFP phase distribution
    
    Delegates to NLfp().phase_dist()
    
    Parameters
    ----------
    lfp : NLfp
        LFP object which contains the LFP data
    **kwargs
        Keyword arguments

    Returns
    -------
    dict
        Graphical data of the analysis

    See also
    --------
    nc_lfp.NLfp().phase_dist()

    &#34;&#34;&#34;
    
    if lfp is None:
        logging.error(&#39;LFP data not specified!&#39;)
    else:
        try:
            lfp.phase_dist(self.get_unit_stamp(), **kwargs)
        except:
            logging.error(&#39;No phase_dist() method in lfp data specified!&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.plv"><code class="name flex">
<span>def <span class="ident">plv</span></span>(<span>self, lfp=None, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates phase-locking value of spike train to underlying LFP signal.</p>
<p>Delegates to NLfp().plv()</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>lfp</code></strong> :&ensp;<code>NLfp</code></dt>
<dd>LFP object which contains the LFP data</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl>
<h2 id="see-also">See also</h2>
<p><code>nc_lfp.NLfp().plv()</code></p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def plv(self, lfp=None, **kwargs):
    &#34;&#34;&#34;
    Calculates phase-locking value of spike train to underlying LFP signal.
    
    Delegates to NLfp().plv()
    
    Parameters
    ----------
    lfp : NLfp
        LFP object which contains the LFP data
    **kwargs
        Keyword arguments

    Returns
    -------
    dict
        Graphical data of the analysis

    See also
    --------
    nc_lfp.NLfp().plv()

    &#34;&#34;&#34;
    
    if lfp is None:
        logging.error(&#39;LFP data not specified!&#39;)
    else:
        try:
            lfp.plv(self.get_unit_stamp(), **kwargs)
        except:
            logging.error(&#39;No plv() method in lfp data specified!&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.psth"><code class="name flex">
<span>def <span class="ident">psth</span></span>(<span>self, event_stamp, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates peri-stimulus time histogram (PSTH)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>event_stamp</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Event timestamps</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def psth(self, event_stamp, **kwargs):
    &#34;&#34;&#34;
    Calculates peri-stimulus time histogram (PSTH)
    
    Parameters
    ----------
    event_stamp : ndarray
        Event timestamps
        
    **kwargs
        Keyword arguments

    Returns
    -------
    dict
        Graphical data of the analysis

    &#34;&#34;&#34;
    
    graph_data = oDict()
    bins = kwargs.get(&#39;bins&#39;, 1)
    if isinstance(bins, int):
        bound = np.array(kwargs.get(&#39;bound&#39;, [-500, 500]))
        bins = np.hstack((np.arange(bound[0], 0, bins), np.arange(0, bound[1] + bins, bins)))
    bins = bins/1000 # converted to sec
    n_bins = len(bins) - 1

    hist_count = np.zeros([n_bins, ])
    unitStamp = self.get_unit_stamp()
    for it in range(event_stamp.size):
        tmp_count, edges = np.histogram(unitStamp - event_stamp[it], bins=bins)
        hist_count = hist_count + tmp_count

    graph_data[&#39;psth&#39;] = hist_count
    graph_data[&#39;bins&#39;] = 1000*edges[:-1]

    # Included in case the last point is needed
    graph_data[&#39;all_bins&#39;] = 1000*edges

    return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.save_to_hdf5"><code class="name flex">
<span>def <span class="ident">save_to_hdf5</span></span>(<span>self, file_name=None, system=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Stores NSpike() object to HDF5 file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Full file directory for the spike data</dd>
<dt><strong><code>system</code></strong> :&ensp;<code>str</code></dt>
<dd>Recoring system or data format</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="also-see">Also see</h2>
<p>nc_hdf.Nhdf().save_spike()</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def save_to_hdf5(self, file_name=None, system=None):
    &#34;&#34;&#34;
    Stores NSpike() object to HDF5 file
    
    Parameters
    ----------
    file_name : str
        Full file directory for the spike data
    system : str
        Recoring system or data format
    
    Returns
    -------
    None
    
    Also see
    --------
    nc_hdf.Nhdf().save_spike()
    
    &#34;&#34;&#34;
    hdf = Nhdf()
    if file_name and system:
        if os.path.exists(file_name):
            self.set_filename(file_name)
            self.set_system(system)
            self.load()
        else:
            logging.error(&#39;Specified file cannot be found!&#39;)

    hdf.save_spike(spike=self)
    hdf.close()</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.set_unit_no"><code class="name flex">
<span>def <span class="ident">set_unit_no</span></span>(<span>self, unit_no=None, spike_name=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Sets the unit number of the spike dataset to analyse</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>unit_no</code></strong> :&ensp;<code>int</code></dt>
<dd>Unit or cell number to analyse</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_unit_no(self, unit_no=None, spike_name=None):
    &#34;&#34;&#34;
    Sets the unit number of the spike dataset to analyse
    
    Parameters
    ----------
    unit_no : int
        Unit or cell number to analyse
    
    Returns
    -------
    None

    &#34;&#34;&#34;
    
    if isinstance(unit_no, int):
        if unit_no in self.get_unit_list():
            self._unit_no = unit_no
            self._set_unit_stamp()
    else:
        if spike_name is None:
            spike_name = self.get_spike_names()
        if len(unit_no) == len(spike_name):
            spikes = self.get_spike(spike_name)
            for i, num in enumerate(unit_no):
                if num in spikes[i].get_unit_list():
                    spikes[i].set_unit_no(num)
        else:
            logging.error(&#39;Unit no. to set are not as many as child spikes!&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.set_unit_tags"><code class="name flex">
<span>def <span class="ident">set_unit_tags</span></span>(<span>self, new_tags)</span>
</code></dt>
<dd>
<section class="desc"><p>Sets the number or tags of the clustered units</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>new_tags</code></strong> :&ensp;<code>list</code> or <code>ndarray</code></dt>
<dd>Tags for each spiking wave</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_unit_tags(self, new_tags):
    
    &#34;&#34;&#34;
    Sets the number or tags of the clustered units
    
    Parameters
    ----------
    new_tags : list or ndarray
        Tags for each spiking wave 
    
    Returns
    -------
    None

    &#34;&#34;&#34;
    
    if len(new_tags) == len(self._timestamp):
        self._unit_Tags = new_tags
        self._set_unit_list()
    else:
        logging.error(&#39;No of tags spikes does not match with no of spikes&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.spike_lfp_causality"><code class="name flex">
<span>def <span class="ident">spike_lfp_causality</span></span>(<span>self, lfp=None, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Analyses spike to underlying LFP causality</p>
<p>Delegates to NLfp().spike_lfp_causality()</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl>
<h2 id="see-also">See also</h2>
<p><code>nc_lfp.NLfp().spike_lfp_causality()</code></p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def spike_lfp_causality(self, lfp=None, **kwargs):
    &#34;&#34;&#34;
    Analyses spike to underlying LFP causality
    
    Delegates to NLfp().spike_lfp_causality()
    
    Parameters
    ----------
    **kwargs
        Keyword arguments

    Returns
    -------
    dict
        Graphical data of the analysis

    See also
    --------
    nc_lfp.NLfp().spike_lfp_causality()

    &#34;&#34;&#34;
    
    if lfp is None:
        logging.error(&#39;LFP data not specified!&#39;)
    else:
        try:
            lfp.spike_lfp_causality(self.get_unit_stamp(), **kwargs)
        except:
            logging.error(&#39;No sfc() method in lfp data specified!&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.subsample"><code class="name flex">
<span>def <span class="ident">subsample</span></span>(<span>self, sample_range=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract a time range from the spikes.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sample_range</code></strong> :&ensp;<code>tuple</code></dt>
<dd>the time in seconds to extract from the spikes</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><a title="neurochat.nc_spike.NSpike" href="#neurochat.nc_spike.NSpike"><code>NSpike</code></a></dt>
<dd>subsampled version of initial spike object</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def subsample(self, sample_range=None):
    &#34;&#34;&#34;
    Extract a time range from the spikes.
    
    Parameters
    ----------
    sample_range : tuple
        the time in seconds to extract from the spikes
    
    Returns
    -------
    NSpike
        subsampled version of initial spike object
    &#34;&#34;&#34;
    if sample_range is None:
        return self
    new_spike = deepcopy(self)
    stamps = self.get_timestamp()
    lower, upper = sample_range
    sample_spike_idxs = (
        (stamps &lt;= upper) &amp; (stamps &gt;= lower)).nonzero()
    new_spike_times = stamps[sample_spike_idxs]
    new_tags = self.get_unit_tags()[sample_spike_idxs]
    new_waveform = new_spike.get_waveform()
    for ch in new_waveform.keys():
        new_waveform[ch] = new_waveform[ch][sample_spike_idxs, :].squeeze()
    new_spike._set_timestamp(new_spike_times)
    new_spike.set_unit_tags(new_tags)
    new_spike._set_waveform(new_waveform)
    new_spike._set_duration(upper - lower)
    return new_spike</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.theta_index"><code class="name flex">
<span>def <span class="ident">theta_index</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Analysis of theta-modulation of a unit</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def theta_index(self, **kwargs):
    &#34;&#34;&#34;
    Analysis of theta-modulation of a unit
    
    Parameters
    ----------
    **kwargs
        Keyword arguments

    Returns
    -------
    dict
        Graphical data of the analysis

    &#34;&#34;&#34;
    
    p_0 = kwargs.get(&#39;start&#39;, [6, 0.1, 0.05])
    lb = kwargs.get(&#39;lower&#39;, [4, 0, 0])
    ub = kwargs.get(&#39;upper&#39;, [14, 5, 0.1])

    _results = oDict()
    graph_data = self.isi_corr(**kwargs)
    corrBins = graph_data[&#39;isiCorrBins&#39;]
    corrCount = graph_data[&#39;isiCorr&#39;]
    m = corrCount.max()
    center = find(corrBins == 0, 1, &#39;first&#39;)[0]
    x = corrBins[center:]/1000
    y = corrCount[center:]
    y_fit = np.empty([corrBins.size,])

    ## This is for the double-exponent dip model
    # def fit_func(x, a, f, tau1, b, c1, tau2, c2, tau3):
    #     return  a*np.cos(2*np.pi*f*x)*np.exp(-np.abs(x)/tau1)+ b+ \
    #         c1*np.exp(-np.abs(x)/tau2)- c2*np.exp(-np.abs(x)/tau3)
    
    # popt, pcov = curve_fit(fit_func, x, y, \
    #                         p0=[m, p_0[0], p_0[1], m, m, p_0[2], m, 0.005], \
    #                         bounds=([0, lb[0], lb[1], 0, 0, lb[2], 0, 0], \
    #                         [m, ub[0], ub[1], m, m, ub[2], m, 0.01]),
    #                         max_nfev=100000)
    # a, f, tau1, b, c1, tau2, c2, tau3 = popt

    # This is for the single-exponent dip model
    def fit_func(x, a, f, tau1, b, c, tau2):
        return  a*np.cos(2*np.pi*f*x)*np.exp(-np.abs(x)/tau1)+ b+ \
            c*np.exp(-(x/tau2)**2)

    try:
        popt, pcov = curve_fit(
            fit_func, x, y,
            p0=[m, p_0[0], p_0[1], m, m, p_0[2]], 
            bounds=([0, lb[0], lb[1], 0, -m, lb[2]],
            [m, ub[0], ub[1], m, m, ub[2]]),
            max_nfev=100000)
    except Exception as e:
        logging.error(&#34;Failed curve_fit in theta_index: {} &#34;.format(e))
        _results[&#39;Theta Index&#39;] = None
        _results[&#39;TI fit freq Hz&#39;] = None
        _results[&#39;TI fit tau1 sec&#39;] = None
        _results[&#39;TI adj Rsq&#39;] = None
        _results[&#39;TI Pearse R&#39;] = None
        _results[&#39;TI Pearse P&#39;] = None

        self.update_result(_results)
        return None

    a, f, tau1, b, c, tau2 = popt

    y_fit[center:] = fit_func(x, *popt)
    y_fit[:center] = np.flipud(y_fit[center:])

    gof = residual_stat(y, y_fit[center:], 6)

    graph_data[&#39;corrFit&#39;] = y_fit
    _results[&#39;Theta Index&#39;] = a/b
    _results[&#39;TI fit freq Hz&#39;] = f
    _results[&#39;TI fit tau1 sec&#39;] = tau1
    _results[&#39;TI adj Rsq&#39;] = gof[&#39;adj Rsq&#39;]
    _results[&#39;TI Pearse R&#39;] = gof[&#39;Pearson R&#39;]
    _results[&#39;TI Pearse P&#39;] = gof[&#39;Pearson P&#39;]

    self.update_result(_results)

    return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.theta_skip_index"><code class="name flex">
<span>def <span class="ident">theta_skip_index</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Analysis of theta-skipping of a unit</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def theta_skip_index(self, **kwargs):
    &#34;&#34;&#34;
    Analysis of theta-skipping of a unit
    
    Parameters
    ----------
    **kwargs
        Keyword arguments

    Returns
    -------
    dict
        Graphical data of the analysis

    &#34;&#34;&#34;
    
    p_0 = kwargs.get(&#39;start&#39;, [6, 0.1, 0.05])
    lb = kwargs.get(&#39;lower&#39;, [4, 0, 0])
    ub = kwargs.get(&#39;upper&#39;, [14, 5, 0.1])

    _results = oDict()
    graph_data = self.isi_corr(**kwargs)
    corrBins = graph_data[&#39;isiCorrBins&#39;]
    corrCount = graph_data[&#39;isiCorr&#39;]
    m = corrCount.max()
    center = find(corrBins == 0, 1, &#39;first&#39;)[0]
    x = corrBins[center:]/1000
    y = corrCount[center:]
    y_fit = np.empty([corrBins.size,])

    # This is for the double-exponent dip model
    def fit_func(x, a1, f1, a2, f2, tau1, b, c1, tau2, c2, tau3):
        return  (a1*np.cos(2*np.pi*f1*x)+ a2*np.cos(2*np.pi*f2*x))*np.exp(-np.abs(x)/tau1)+ b+ \
            c1*np.exp(-np.abs(x)/tau2)- c2*np.exp(-np.abs(x)/tau3)

    popt, pcov = curve_fit(fit_func, x, y, \
                            p0=[m, p_0[0], m, p_0[0]/2, p_0[1], m, m, p_0[2], m, 0.005], \
                            bounds=([0, lb[0], 0, lb[0]/2, lb[1], 0, 0, lb[2], 0, 0], \
                            [m, ub[0], m, ub[0]/2, ub[1], m, m, ub[2], m, 0.01]),\
                            max_nfev=100000)
    a1, f1, a2, f2, tau1, b, c1, tau2, c2, tau3 = popt

    ## This is for the single-exponent dip model
    # def fit_func(x, a1, f1, a2, f2, tau1, b, c, tau2):
    #     return  (a1*np.cos(2*np.pi*f1*x)+ a2*np.cos(2*np.pi*f2*x))*np.exp(-np.abs(x)/tau1)+ b+ \
    #         c*np.exp(-(x/tau2)**2)
    
    # popt, pcov = curve_fit(fit_func, x, y, \
    #                         p0=[m, p_0[0], m, p_0[0]/2, p_0[1], m, m, p_0[2]], \
    #                         bounds=([0, lb[0], 0, lb[0]/2, lb[1], 0, -m, lb[2]], \
    #                         [m, ub[0], m, ub[0]/2, ub[1], m, m, ub[2]]),
    #                         max_nfev=100000)
    # a1, f1, a2, f2, tau1, b, c, tau2 = popt

    temp_fit = fit_func(x, *popt)
    y_fit[center:] = temp_fit
    y_fit[:center] = np.flipud(temp_fit)

    peak_val, peak_loc = extrema(temp_fit[find(x &gt;= 50/1000)])[0:2]

    if len(peak_val) &gt;= 2:
        skipIndex = (peak_val[1]- peak_val[0])/np.max(np.array([peak_val[1], peak_val[0]]))
    else:
        skipIndex = None
    gof = residual_stat(y, temp_fit, 6)

    graph_data[&#39;corrFit&#39;] = y_fit
    _results[&#39;Theta Skip Index&#39;] = skipIndex
    _results[&#39;TS jump factor&#39;] = a2/(a1+ a2) if skipIndex else None
    _results[&#39;TS f1 freq Hz&#39;] = f1 if skipIndex else None
    _results[&#39;TS f2 freq Hz&#39;] = f2 if skipIndex else None
    _results[&#39;TS freq ratio&#39;] = f1/f2 if skipIndex else None
    _results[&#39;TS tau1 sec&#39;] = tau1 if skipIndex else None
    _results[&#39;TS adj Rsq&#39;] = gof[&#39;adj Rsq&#39;]
    _results[&#39;TS Pearse R&#39;] = gof[&#39;Pearson R&#39;]
    _results[&#39;TS Pearse P&#39;] = gof[&#39;Pearson P&#39;]

    self.update_result(_results)

    return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spike.NSpike.wave_property"><code class="name flex">
<span>def <span class="ident">wave_property</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Claulates different waveform properties for currently set unit</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def wave_property(self):
    &#34;&#34;&#34;
    Claulates different waveform properties for currently set unit
    
    Parameters
    ----------
    None

    Returns
    -------
    dict
        Graphical data of the analysis

    &#34;&#34;&#34;

    _result = oDict()
    graph_data = {}

    def argpeak(data):
        data = np.array(data)
        peak_loc = [j for j in range(7, len(data)) \
                    if data[j] &lt;= 0 and data[j - 1] &gt; 0]
        return peak_loc[0] if peak_loc else 0

    def argtrough1(data, peak_loc):
        data = data.tolist()
        trough_loc = [peak_loc - j for j in range(peak_loc - 2) \
                    if data[peak_loc - j] &gt;= 0 and data[peak_loc - j - 1] &lt;= 0]
        return trough_loc[0] if trough_loc else 0

    def wave_width(wave, peak, thresh=0.25):
        p_loc, p_val = peak
        Len = wave.size
        if p_loc:
            w_start = find(wave[:p_loc] &lt;= thresh*p_val, 1, &#39;last&#39;)
            w_start = w_start[0] if w_start.size else 0
            w_end = find(wave[p_loc:] &lt;= thresh*p_val, 1, &#39;first&#39;)
            w_end = p_loc + w_end[0] if w_end.size else Len
        else:
            w_start = 1
            w_end = Len

        return w_end- w_start

    num_spikes = self.get_unit_spikes_count()
    _result[&#39;Number of Spikes&#39;] = num_spikes
    _result[&#39;Mean Spiking Freq&#39;] = num_spikes/ self.get_duration()
    _waves = self.get_unit_waves()
    samples_per_spike = self.get_samples_per_spike()
    tot_chans = self.get_total_channels()
    meanWave = np.empty([samples_per_spike, tot_chans])
    stdWave = np.empty([samples_per_spike, tot_chans])

    width = np.empty([num_spikes, tot_chans])
    amp = np.empty([num_spikes, tot_chans])
    height = np.empty([num_spikes, tot_chans])
    for i, (chan, wave) in enumerate(_waves.items()):
        meanWave[:, i] = np.mean(wave, 0)
        stdWave[:, i] = np.std(wave, 0)
        slope = np.gradient(wave)[1][:, :-1]
        max_val = wave.max(1)

        peak_val, trough1_val = 0, 0
        if max_val.max() &gt; 0:
            peak_loc = [argpeak(slope[I, :]) for I in range(num_spikes)]
            peak_val = [wave[I, peak_loc[I]] for I in range(num_spikes)]
            trough1_loc = [argtrough1(slope[I, :], peak_loc[I]) for I in range(num_spikes)]
            trough1_val = [wave[I, trough1_loc[I]] for I in range(num_spikes)]
            peak_loc = np.array(peak_loc)
            peak_val = np.array(peak_val)
            trough1_loc = np.array(trough1_loc)
            trough1_val = np.array(trough1_val)
            width[:, i] = np.array([wave_width(wave[I, :], (peak_loc[I], peak_val[I]), 0.25) \
                         for I in range(num_spikes)])

        amp[:, i] = peak_val - trough1_val
        height[:, i] = peak_val - wave.min(1)
    max_chan = amp.mean(0).argmax()
    width = width[:, max_chan]* 10**6/self.get_sampling_rate()
    amp = amp[:, max_chan]
    height = height[:, max_chan]

    graph_data = {&#39;Mean wave&#39;: meanWave, &#39;Std wave&#39;: stdWave,
                  &#39;Amplitude&#39;: amp, &#39;Width&#39;: width, &#39;Height&#39;: height,
                  &#39;Max channel&#39;: max_chan}

    _result.update({&#39;Mean amplitude&#39;: amp.mean(), &#39;Std amplitude&#39;: amp.std(),
                    &#39;Mean height&#39;: height.mean(), &#39;Std height&#39;: height.std(),
                    &#39;Mean width&#39;: width.mean(), &#39;Std width&#39;: width.std()})

    self.update_result(_result)
    
    return graph_data</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="neurochat.nc_base.NBase" href="nc_base.html#neurochat.nc_base.NBase">NBase</a></b></code>:
<ul class="hlist">
<li><code><a title="neurochat.nc_base.NBase.add_node" href="nc_base.html#neurochat.nc_base.NBase.add_node">add_node</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.change_names" href="nc_base.html#neurochat.nc_base.NBase.change_names">change_names</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.count_lfp" href="nc_base.html#neurochat.nc_base.NBase.count_lfp">count_lfp</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.count_spike" href="nc_base.html#neurochat.nc_base.NBase.count_spike">count_spike</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.del_lfp" href="nc_base.html#neurochat.nc_base.NBase.del_lfp">del_lfp</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.del_node" href="nc_base.html#neurochat.nc_base.NBase.del_node">del_node</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.del_spike" href="nc_base.html#neurochat.nc_base.NBase.del_spike">del_spike</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_comments" href="nc_base.html#neurochat.nc_base.NAbstract.get_comments">get_comments</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_data_source" href="nc_base.html#neurochat.nc_base.NAbstract.get_data_source">get_data_source</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_date" href="nc_base.html#neurochat.nc_base.NAbstract.get_date">get_date</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_duration" href="nc_base.html#neurochat.nc_base.NAbstract.get_duration">get_duration</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_experimenter" href="nc_base.html#neurochat.nc_base.NAbstract.get_experimenter">get_experimenter</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_file_version" href="nc_base.html#neurochat.nc_base.NAbstract.get_file_version">get_file_version</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_filename" href="nc_base.html#neurochat.nc_base.NAbstract.get_filename">get_filename</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_lfp" href="nc_base.html#neurochat.nc_base.NBase.get_lfp">get_lfp</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_lfp_names" href="nc_base.html#neurochat.nc_base.NBase.get_lfp_names">get_lfp_names</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_name" href="nc_base.html#neurochat.nc_base.NAbstract.get_name">get_name</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_node" href="nc_base.html#neurochat.nc_base.NBase.get_node">get_node</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_record_info" href="nc_base.html#neurochat.nc_base.NAbstract.get_record_info">get_record_info</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_results" href="nc_base.html#neurochat.nc_base.NAbstract.get_results">get_results</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_source_format" href="nc_base.html#neurochat.nc_base.NAbstract.get_source_format">get_source_format</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_spike" href="nc_base.html#neurochat.nc_base.NBase.get_spike">get_spike</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_spike_names" href="nc_base.html#neurochat.nc_base.NBase.get_spike_names">get_spike_names</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_system" href="nc_base.html#neurochat.nc_base.NAbstract.get_system">get_system</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_time" href="nc_base.html#neurochat.nc_base.NAbstract.get_time">get_time</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.reset_results" href="nc_base.html#neurochat.nc_base.NAbstract.reset_results">reset_results</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.set_description" href="nc_base.html#neurochat.nc_base.NAbstract.set_description">set_description</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.set_filename" href="nc_base.html#neurochat.nc_base.NAbstract.set_filename">set_filename</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.set_lfp_file_names" href="nc_base.html#neurochat.nc_base.NBase.set_lfp_file_names">set_lfp_file_names</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.set_lfp_names" href="nc_base.html#neurochat.nc_base.NBase.set_lfp_names">set_lfp_names</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.set_name" href="nc_base.html#neurochat.nc_base.NAbstract.set_name">set_name</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.set_node_file_names" href="nc_base.html#neurochat.nc_base.NBase.set_node_file_names">set_node_file_names</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.set_record_info" href="nc_base.html#neurochat.nc_base.NAbstract.set_record_info">set_record_info</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.set_spike_file_names" href="nc_base.html#neurochat.nc_base.NBase.set_spike_file_names">set_spike_file_names</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.set_spike_names" href="nc_base.html#neurochat.nc_base.NBase.set_spike_names">set_spike_names</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.set_system" href="nc_base.html#neurochat.nc_base.NAbstract.set_system">set_system</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.update_result" href="nc_base.html#neurochat.nc_base.NAbstract.update_result">update_result</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="neurochat" href="index.html">neurochat</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="neurochat.nc_spike.NSpike" href="#neurochat.nc_spike.NSpike">NSpike</a></code></h4>
<ul class="">
<li><code><a title="neurochat.nc_spike.NSpike.add_lfp" href="#neurochat.nc_spike.NSpike.add_lfp">add_lfp</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.add_spike" href="#neurochat.nc_spike.NSpike.add_spike">add_spike</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.burst" href="#neurochat.nc_spike.NSpike.burst">burst</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.get_bytes_per_sample" href="#neurochat.nc_spike.NSpike.get_bytes_per_sample">get_bytes_per_sample</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.get_channel_ids" href="#neurochat.nc_spike.NSpike.get_channel_ids">get_channel_ids</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.get_fullscale_mv" href="#neurochat.nc_spike.NSpike.get_fullscale_mv">get_fullscale_mv</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.get_samples_per_spike" href="#neurochat.nc_spike.NSpike.get_samples_per_spike">get_samples_per_spike</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.get_sampling_rate" href="#neurochat.nc_spike.NSpike.get_sampling_rate">get_sampling_rate</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.get_timebase" href="#neurochat.nc_spike.NSpike.get_timebase">get_timebase</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.get_timestamp" href="#neurochat.nc_spike.NSpike.get_timestamp">get_timestamp</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.get_timestamp_bytes" href="#neurochat.nc_spike.NSpike.get_timestamp_bytes">get_timestamp_bytes</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.get_total_channels" href="#neurochat.nc_spike.NSpike.get_total_channels">get_total_channels</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.get_total_spikes" href="#neurochat.nc_spike.NSpike.get_total_spikes">get_total_spikes</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.get_type" href="#neurochat.nc_spike.NSpike.get_type">get_type</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.get_unit_list" href="#neurochat.nc_spike.NSpike.get_unit_list">get_unit_list</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.get_unit_no" href="#neurochat.nc_spike.NSpike.get_unit_no">get_unit_no</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.get_unit_spikes_count" href="#neurochat.nc_spike.NSpike.get_unit_spikes_count">get_unit_spikes_count</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.get_unit_stamp" href="#neurochat.nc_spike.NSpike.get_unit_stamp">get_unit_stamp</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.get_unit_stamps_in_ranges" href="#neurochat.nc_spike.NSpike.get_unit_stamps_in_ranges">get_unit_stamps_in_ranges</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.get_unit_tags" href="#neurochat.nc_spike.NSpike.get_unit_tags">get_unit_tags</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.get_unit_waves" href="#neurochat.nc_spike.NSpike.get_unit_waves">get_unit_waves</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.get_waveform" href="#neurochat.nc_spike.NSpike.get_waveform">get_waveform</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.isi" href="#neurochat.nc_spike.NSpike.isi">isi</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.isi_corr" href="#neurochat.nc_spike.NSpike.isi_corr">isi_corr</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.load" href="#neurochat.nc_spike.NSpike.load">load</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.load_lfp" href="#neurochat.nc_spike.NSpike.load_lfp">load_lfp</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.load_spike" href="#neurochat.nc_spike.NSpike.load_spike">load_spike</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.load_spike_Axona" href="#neurochat.nc_spike.NSpike.load_spike_Axona">load_spike_Axona</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.load_spike_NWB" href="#neurochat.nc_spike.NSpike.load_spike_NWB">load_spike_NWB</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.load_spike_Neuralynx" href="#neurochat.nc_spike.NSpike.load_spike_Neuralynx">load_spike_Neuralynx</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.phase_dist" href="#neurochat.nc_spike.NSpike.phase_dist">phase_dist</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.plv" href="#neurochat.nc_spike.NSpike.plv">plv</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.psth" href="#neurochat.nc_spike.NSpike.psth">psth</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.save_to_hdf5" href="#neurochat.nc_spike.NSpike.save_to_hdf5">save_to_hdf5</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.set_unit_no" href="#neurochat.nc_spike.NSpike.set_unit_no">set_unit_no</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.set_unit_tags" href="#neurochat.nc_spike.NSpike.set_unit_tags">set_unit_tags</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.spike_lfp_causality" href="#neurochat.nc_spike.NSpike.spike_lfp_causality">spike_lfp_causality</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.subsample" href="#neurochat.nc_spike.NSpike.subsample">subsample</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.theta_index" href="#neurochat.nc_spike.NSpike.theta_index">theta_index</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.theta_skip_index" href="#neurochat.nc_spike.NSpike.theta_skip_index">theta_skip_index</a></code></li>
<li><code><a title="neurochat.nc_spike.NSpike.wave_property" href="#neurochat.nc_spike.NSpike.wave_property">wave_property</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.6.3</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>