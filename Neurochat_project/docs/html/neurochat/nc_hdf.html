<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.6.3" />
<title>neurochat.nc_hdf API documentation</title>
<meta name="description" content="This module implements Nhdf Class for NeuroChaT software â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>neurochat.nc_hdf</code></h1>
</header>
<section id="section-intro">
<p>This module implements Nhdf Class for NeuroChaT software</p>
<p>@author: Md Nurul Islam; islammn at tcd dot ie</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
&#34;&#34;&#34;
This module implements Nhdf Class for NeuroChaT software

@author: Md Nurul Islam; islammn at tcd dot ie

&#34;&#34;&#34;
import os
import io

import logging

import numpy as np

import h5py


class Nhdf(object):
    &#34;&#34;&#34;
    The Nhdf class manages the import and export of various NeuroChaT dataset to
    a HDF5 file dataset. It also creates and manages the nomenclature for storage
    paths within the file.
    
    &#34;&#34;&#34;
        
    def __init__(self, **kwargs):
        self._filename = kwargs.get(&#39;filename&#39;, &#39;&#39;)
        self.f = None
        
        self.__type = &#39;hdf&#39;
        
        if os.path.exists(self._filename):
            self.file()
            
    def get_type(self):
        &#34;&#34;&#34;
        Returns the type of object. For Nhdf, this is always `hdf` type
        
        Parameters
        ----------
        None
        
        Returns
        -------
        str

        &#34;&#34;&#34;
        return self.__type
    
    def get_filename(self):
        &#34;&#34;&#34;
        Returns the full file of the HDF5 dataset
        
        Parameters
        ----------
        None
        
        Returns
        -------
        str

        &#34;&#34;&#34;
        
        return self._filename

    def set_filename(self, filename=None):
        &#34;&#34;&#34;
        Sets the full file of the HDF5 dataset
        
        Parameters
        ----------
        filename : str
            Filename of the HDF5 dataset
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if filename:
            self._filename = filename
        try:
            self.file()
        except:
            logging.error(&#39;Invalid file!&#39;)

    def get_file_object(self):
        &#34;&#34;&#34;
        Returns the file object that is opened using h5py
        
        Parameters
        ----------
        None
        
        Returns
        -------
        object
            h5py file object

        &#34;&#34;&#34;
        if isinstance(self.f, io.IOBase):
            return self.f
        else:
            logging.warning(&#39;The file Nhdf instance is not open yet, use Nhdf.File() method to open it!&#39;)

    def file(self):
        &#34;&#34;&#34;
        Opens the file, and returns the file object
        
        Parameters
        ----------
        None
        
        Returns
        -------
        object
            h5py file object

        &#34;&#34;&#34;
        self.close()
        try:
            self.f = h5py.File(self._filename, &#39;a&#39;)
            self.initialize()
        except:
            logging.error(&#39;Cannot open &#39;+ self._filename)
            
        return self.f

    def close(self):
        &#34;&#34;&#34;
        Closes the h5py file object
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if isinstance(self.f, h5py.File):
            self.f.close()
            self.f = None

    def initialize(self):
        &#34;&#34;&#34;
        Initializes the basic groups for the HDF5 file
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        groups = [&#39;acquisition&#39;, &#39;processing&#39;, &#39;analysis&#39;, &#39;epochs&#39;, &#39;general&#39;, &#39;stimulus&#39;]
        for g in groups:
            self.f.require_group(g)
    
    def get_groups_in_path(self, path=&#39;&#39;):
        &#34;&#34;&#34;
        Returns the names of groups or datasets in a path 
        
        Parameters
        ----------
        path : str
            path to HDF5 file group
        
        Returns
        -------
        list
            Names of the groups or datasets in the path

        &#34;&#34;&#34;
        items= []
        if path in self.f:
            items = list(self.f[path].keys())            
        else:
            logging.warning(&#39;No groups in the path:&#39;+ path)
        
        return items
    
    @staticmethod
    def resolve_hdfname(data=None):
        &#34;&#34;&#34;
        Resolves and returns the name of the HDF5 file from the filenames of the NeuroChaT data
        
        Parameters
        ----------
        data
            One of the NeuroChaT data objects
        
        Returns
        -------
        hdf_name : str
            Hdf5 file name

        &#34;&#34;&#34;
        try:
            data_type = data.get_type()
        except:
            logging.error(&#39;The type of the data cannot be extracted!&#39;)
            
        hdf_name = None
        file_name = data.get_filename()
        system = data.get_system()
        if system == &#39;NWB&#39;:
            hdf_name = file_name.split(&#39;+&#39;)[0]
        if os.path.exists(file_name):
            words = file_name.split(os.sep)
            f_path = os.sep.join(words[0:-1])
            f_name = words[-1]
            if system == &#39;Axona&#39;:
                if data_type == &#39;spike&#39; or data_type == &#39;lfp&#39;:
                    hdf_name = os.sep.join([f_path, f_name.split(&#39;.&#39;)[0]+&#39;.hdf5&#39;])
                elif data_type == &#39;spatial&#39;:
                    hdf_name = os.sep.join([f_path, &#39;_&#39;.join(f_name.split(&#39;.&#39;)[0].split(&#39;_&#39;)[:-1])+&#39;.hdf5&#39;])
            elif system == &#39;Neuralynx&#39;:
                hdf_name = os.sep.join([f_path, f_path.split(os.sep)[-1]+&#39;.hdf5&#39;])

        return hdf_name

    def resolve_datapath(self, data=None):
        &#34;&#34;&#34;
        Resolves and returns path of the dataset from NeuroChaT data objects
        
        Parameters
        ----------
        data
            NeuroChaT data objects
        
        Returns
        -------
        str
            Path of the NeuroChaT data

        &#34;&#34;&#34;
        
        # No resolution for NWB file, this function will not be called if the system == &#39;NWB&#39;
        try:
            data_type = data.get_type()
        except:
            logging.error(&#39;The type of the data cannot be extracted!&#39;)
        path = None
        tag = self.get_file_tag(data)

        if data_type == &#39;spatial&#39;:
            path = &#39;/processing/Behavioural/Position&#39;
        elif tag and data_type == &#39;spike&#39;:
            path = &#39;/processing/Shank/&#39;+ tag
        elif tag and data_type == &#39;lfp&#39;:
            path = &#39;/processing/Neural Continuous/LFP/&#39;+ tag

        return path

    @staticmethod
    def get_file_tag(data=None):
        &#34;&#34;&#34;
        Resolves and returns the file tag or extension to name the group of the
        neural data in the HDF5 file
        
        Parameters
        ----------
        data : NSpike or NLfp
            Neural data objects of NeuroChaT
                    
        Returns
        -------
        str
            File extention (Axona) or name (Neuralynx) of the neural datasets

        &#34;&#34;&#34;
        try:
            data_type = data.get_type()
        except:
            logging.error(&#39;The type of the data cannot be extracted!&#39;)
        # data is one of NSpike or Nlfp instance
        tag = None
        if data_type == &#39;spike&#39; or data_type == &#39;lfp&#39;:
            f_name = data.get_filename()
            system = data.get_system()
            if system == &#39;NWB&#39;:
                tag = f_name.split(&#39;+&#39;)[-1].split(&#39;/&#39;)[-1]
            else:
                name, ext = os.path.basename(f_name).split(&#39;.&#39;)
                if system == &#39;Axona&#39;:
                    tag = ext
                elif system == &#39;Neuralynx&#39;:
                    tag = name
        return tag

    def resolve_analysis_path(self, spike=None, lfp=None):
        &#34;&#34;&#34;
        Resolves and returns path of the dataset where analysis results will be
        stroed. This path is also the unique unit ID.
        
        Parameters
        ----------
        spike : NSpike
            Spike data object
        lfp : NLfp
            Lfp data object
        
        Returns
        -------
        str
            Unique unit ID resolved from spike and lfp filenames which is also the
            name of the path to store the data of NeuroChaT analysis

        &#34;&#34;&#34;
        # Each input is an object
        try:
            data_type = spike.get_type()
        except:
            logging.error(&#39;The type of the data cannot be extracted!&#39;)
            
        path = &#39;&#39;
        if data_type == &#39;spike&#39;:
            tag = self.get_file_tag(spike)
            if spike.get_system() == &#39;Axona&#39; or not tag.startswith(&#39;TT&#39;):
                tag = &#39;TT&#39;+ tag
            path += tag + &#39;_SS_&#39;+ str(spike.get_unit_no())
        else:
            logging.error(&#39;Please specify a valid spike data!&#39;)

        try:
            data_type = lfp.get_type()
        except:
            logging.error(&#39;The type of the data cannot be extracted!&#39;)
            
        if data_type == &#39;lfp&#39;:
            path += &#39;_&#39;+ self.get_file_tag(lfp)

        return path

    def save_dataset(self, path=None, name=None, data=None, create_group=True):
        &#34;&#34;&#34;
        Stores a dataset to a specific path
        
        Parameters
        ----------
        path : str
            Path of a group in HDF5 file
        name : str
            Name of the new dataset
        data : ndarrray or list of numbers
            Data to be stored
        create_group : bool
            If True, creates a new group if the &#39;path&#39; is not in the file
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        # Path is an abosulte path of the group where the dataset will be stored
        # Abosulte path for the dataset will be created using the group path and the name of the dataset
        # If create_group = False (default), it will check if the path exists. If not error will be generated
        # If creat_group = True, it will create the group]\
        if not path:
            logging.error(&#39;Invalid group path specified!&#39;)
        if not name:
            logging.error(&#39;Please provide a name for the dataset!&#39;)
        if (path in self.f) or create_group:
            g = self.f.require_group(path)
            if name in g:
                del g[name]
            if isinstance(data, list): # This conditional restricts the None data to store, need to change
                data= [np.nan if item is None else item for item in data]
                try:
                    data = np.array(data)                    
                except:
                    pass
            try:
                g.create_dataset(name=name, data=data)
            except:
                logging.error(&#39;Error in creating &#39;+ name+ &#39; dataset to hdf5 file&#39;)            
        else:
            logging.error(&#39;hdf5 file path can be created or restored!&#39;)

    def get_dataset(self, group=None, path=&#39;&#39;, name=&#39;&#39;):
        &#34;&#34;&#34;
        Stores a dataset to a specific path
        
        Parameters
        ----------
        group : str
            Path of a group in HDF5 file
        path : str
            Name of the member group. This path is relative to the &#39;group&#39;
        name : str
            Name of the dataset
        
        Returns
        -------
        ndarray or numeric objects
            Value of the dataset

        &#34;&#34;&#34;
        
        if isinstance(group, h5py.Group):
            g = group
        else:
            g = self.f
        if path in g:
            if isinstance(g[path], h5py.Dataset):
                return np.array(g[path])
            elif isinstance(g[path], h5py.Group):
                g = g[path]
                if name in g:
                    return np.array(g[name])
                else:
                    logging.error(&#39;Specify a valid name for the required dataset&#39;)
        elif name in g:
            return np.array(g[name])
        else:
            logging.error(path + &#39; not found!&#39;+ &#39;Specify a valid path or name or check if a proper group is specified!&#39;)

#    def delete_group_data(self, path = None):
#        # Deletes everything within a group, not the group itself
#        if path in self.f:
#            g = self.f[path]
#        if g.keys():
#            for node in g.keys():
#                del self.f[path+ &#39;/&#39;+ node]

    def save_dict_recursive(self, path=None, name=None, data=None, create_group=True):
        &#34;&#34;&#34;
        Stores a dictionary dataset to a specific path. If the dictionary is nested,
        it creates a group for each of the outermost keys.
        
        Parameters
        ----------
        path : str
            Path of a group in HDF5 file
        name : str
            Name of the new dataset
        data : ndarrray or list of numbers
            Data to be stored
        create_group : bool
            If True, creates a new group if the &#39;path&#39; is not in the file
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if not isinstance(data, dict):
            logging.error(&#39;Nhdf class method save_dict_recursive() takes only dictionary data input!&#39;)
        else:
            for key, value in data.items():
                if isinstance(value, dict):
                    self.save_dict_recursive(path=path+ name+ &#39;/&#39;, name=key, data=data[key], create_group=create_group)
                else:
                    self.save_dataset(path=path + name, name=key, data=value, create_group=create_group)

    def save_attributes(self, path=None, attr=None):
        &#34;&#34;&#34;
        Stores an attribute to a group or dataset
        
        Parameters
        ----------
        path : str
            Path of a group or dataset in HDF5 file
        attr : dict
            Attribute names and values in a dictionary

        Returns
        -------
        None

        &#34;&#34;&#34;
        # path has to be the absolute path of a group
        if path in self.f:
            g = self.f[path]
            if isinstance(attr, dict):
                for key, val in attr.items():
                    g.attrs[key] = val
            else:
                logging.error(&#39;Please specify the attributes in a dictionary!&#39;)
        else:
            logging.error(&#39;Please provide a valid hdf5 path!&#39;)
    
    def save_object(self, obj=None):
        &#34;&#34;&#34;
        Stores a NeuroChaT dataset to the HDF5 file. It resolves the name first and
        then stores the data in the storage path
        
        Parameters
        ----------
        obj
            One of the NeuroChaT data types
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        try:
            obj_type= obj.get_type()
        except:
            logging.error(&#39;Cannot get object! It is not of one of the neurochat data types!&#39;)
        
        try:
            fun= getattr(self, &#39;save_&#39; + obj_type)
            fun(obj)
        except:
            logging.error(&#39;Failed to save the dataset!&#39;)
        
    def save_spatial(self, spatial=None):
        &#34;&#34;&#34;
        Stores NSpatial() dataset to the HDF5 file
        
        Parameters
        ----------
        spatial : NSpatial()
            Spatial data object in NeuroChaT
        
        Returns
        -------
        None

        &#34;&#34;&#34;

        # derive the path from the filename to ensure uniqueness
        self.set_filename(self.resolve_hdfname(data=spatial))
        # Get the lfp data path/group
        path = self.resolve_datapath(data=spatial)
        # delete old data
        if path in self.f:
            del self.f[path]
        
        # Create group afresh
        g = self.f.require_group(path)
        
        self.save_attributes(path=path, attr=spatial.get_record_info())
        
        g_loc = g.require_group(path+ &#39;/&#39;+ &#39;location&#39;)
        g_dir = g.require_group(path+ &#39;/&#39;+ &#39;direction&#39;)
        g_speed = g.require_group(path+ &#39;/&#39;+ &#39;speed&#39;)
        g_ang_vel = g.require_group(path+ &#39;/&#39;+ &#39;angular velocity&#39;)
        
        loc = np.empty((spatial.get_total_samples(), 2))
        loc[:, 0] = spatial.get_pos_x()
        loc[:, 1] = spatial.get_pos_y()
        
        g_loc.create_dataset(name=&#39;data&#39;, data=loc)
        g_loc.create_dataset(name=&#39;num_samples&#39;, data=spatial.get_total_samples())
        g_loc.create_dataset(name=&#39;timestamps&#39;, data=spatial.get_time())
        #            g_loc.create_dataset(name=&#39;unit&#39;, data=spatial.getUnit(var=&#39;speed&#39;)) # Unit information needs to be included
        # need to implement the spatial.getUnit() method
        
        g_dir.create_dataset(name=&#39;data&#39;, data=spatial.get_direction())
        g_dir.create_dataset(name=&#39;num_samples&#39;, data=spatial.get_total_samples())
        g_dir.create_dataset(name=&#39;timestamps&#39;, data=spatial.get_time())
        #            g_dir.create_dataset(name=&#39;timestamps&#39;, data=h5py.SoftLink(g_loc.name+ &#39;/timestamps&#39;))
        
        g_speed.create_dataset(name=&#39;data&#39;, data=spatial.get_speed())
        g_speed.create_dataset(name=&#39;num_samples&#39;, data=spatial.get_total_samples())
        g_speed.create_dataset(name=&#39;timestamps&#39;, data=spatial.get_time())
        
        g_ang_vel.create_dataset(name=&#39;data&#39;, data=spatial.get_ang_vel())
        g_ang_vel.create_dataset(name=&#39;num_samples&#39;, data=spatial.get_total_samples())
        g_ang_vel.create_dataset(name=&#39;timestamps&#39;, data=spatial.get_time())
        
        self.close()
        

    def save_lfp(self, lfp=None):
        &#34;&#34;&#34;
        Stores NLfp() dataset to the HDF5 file
        
        Parameters
        ----------
        lfp : NLfp()
            LFP data object in NeuroChaT
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        # derive the path from the filename to ensure uniqueness
        self.set_filename(self.resolve_hdfname(data=lfp))
        # Get the lfp data path/group
        path = self.resolve_datapath(data=lfp)
        # delete old data
        if path in self.f:
            del self.f[path]

        # Create group afresh
        g = self.f.require_group(path)

        self.save_attributes(path=path, attr=lfp.get_record_info())

        g.create_dataset(name=&#39;data&#39;, data=lfp.get_samples())
        g.create_dataset(name=&#39;num_samples&#39;, data=lfp.get_total_samples())
        g.create_dataset(name=&#39;timestamps&#39;, data=lfp.get_timestamp())
        
        self.close()

    def save_spike(self, spike=None):
        &#34;&#34;&#34;
        Stores NSpike() dataset to the HDF5 file
        
        Parameters
        ----------
        spike : NSpike()
            Spike data object in NeuroChaT
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        # derive the path from the filename to ensure uniqueness
        self.set_filename(self.resolve_hdfname(data=spike))
        # Get the spike data path/group
        path = self.resolve_datapath(data=spike)

        # delete old data
        if path in self.f:
            del self.f[path]

        # Create group afresh
        g = self.f.require_group(path)

        self.save_attributes(path=path, attr=spike.get_record_info())

        g_clust = g.require_group(path+ &#39;/&#39;+ &#39;Clustering&#39;)
        g_wave = g.require_group(path+ &#39;/&#39;+ &#39;EventWaveForm/WaveForm&#39;)

        # From chX dictionary, create a higher order np array

        waves = spike.get_waveform() # NC waves are stroed in waves[&#39;ch1&#39;], waves[&#39;ch2&#39;] etc. ways
        stacked_channels = np.empty((spike.get_total_spikes(), spike.get_samples_per_spike(), spike.get_total_channels()))
        i = 0
        for key, val in waves.items():
            stacked_channels[:, :, i] = val
            i += 1
        g_wave.create_dataset(name=&#39;data&#39;, data=stacked_channels)
        g_wave.create_dataset(name=&#39;electrode_idx&#39;, data=spike.get_channel_ids())
        g_wave.create_dataset(name=&#39;num_events&#39;, data=spike.get_total_spikes())
        g_wave.create_dataset(name=&#39;num_samples&#39;, data=spike.get_samples_per_spike())
        g_wave.create_dataset(name=&#39;timestamps&#39;, data=spike.get_timestamp())

        # save Clutser number
        g_clust.create_dataset(name=&#39;cluster_nums&#39;, data=spike.get_unit_list())
        g_clust.create_dataset(name=&#39;num&#39;, data=spike.get_unit_tags())
        g_clust.create_dataset(name=&#39;times&#39;, data=spike.get_timestamp())
        
        self.close()

    def save_cluster(self, clust=None):
        &#34;&#34;&#34;
        Stores NClust() dataset to the HDF5 file
        
        Parameters
        ----------
        clust : NClust()
            Cluster data object in NeuroChaT
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        # Nclust is a NSpike derivative (inherited from NSpike) to add clustering facilities to the NSpike data
        # But we will consider putting it within NSpike itself
        # This will store data to Shank&#39;s Clustering and Feature Extraction group
        
        logging.warning(&#39;save_cluster() method is not implemented yet!&#39;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="neurochat.nc_hdf.Nhdf"><code class="flex name class">
<span>class <span class="ident">Nhdf</span></span>
<span>(</span><span>**kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>The Nhdf class manages the import and export of various NeuroChaT dataset to
a HDF5 file dataset. It also creates and manages the nomenclature for storage
paths within the file.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class Nhdf(object):
    &#34;&#34;&#34;
    The Nhdf class manages the import and export of various NeuroChaT dataset to
    a HDF5 file dataset. It also creates and manages the nomenclature for storage
    paths within the file.
    
    &#34;&#34;&#34;
        
    def __init__(self, **kwargs):
        self._filename = kwargs.get(&#39;filename&#39;, &#39;&#39;)
        self.f = None
        
        self.__type = &#39;hdf&#39;
        
        if os.path.exists(self._filename):
            self.file()
            
    def get_type(self):
        &#34;&#34;&#34;
        Returns the type of object. For Nhdf, this is always `hdf` type
        
        Parameters
        ----------
        None
        
        Returns
        -------
        str

        &#34;&#34;&#34;
        return self.__type
    
    def get_filename(self):
        &#34;&#34;&#34;
        Returns the full file of the HDF5 dataset
        
        Parameters
        ----------
        None
        
        Returns
        -------
        str

        &#34;&#34;&#34;
        
        return self._filename

    def set_filename(self, filename=None):
        &#34;&#34;&#34;
        Sets the full file of the HDF5 dataset
        
        Parameters
        ----------
        filename : str
            Filename of the HDF5 dataset
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if filename:
            self._filename = filename
        try:
            self.file()
        except:
            logging.error(&#39;Invalid file!&#39;)

    def get_file_object(self):
        &#34;&#34;&#34;
        Returns the file object that is opened using h5py
        
        Parameters
        ----------
        None
        
        Returns
        -------
        object
            h5py file object

        &#34;&#34;&#34;
        if isinstance(self.f, io.IOBase):
            return self.f
        else:
            logging.warning(&#39;The file Nhdf instance is not open yet, use Nhdf.File() method to open it!&#39;)

    def file(self):
        &#34;&#34;&#34;
        Opens the file, and returns the file object
        
        Parameters
        ----------
        None
        
        Returns
        -------
        object
            h5py file object

        &#34;&#34;&#34;
        self.close()
        try:
            self.f = h5py.File(self._filename, &#39;a&#39;)
            self.initialize()
        except:
            logging.error(&#39;Cannot open &#39;+ self._filename)
            
        return self.f

    def close(self):
        &#34;&#34;&#34;
        Closes the h5py file object
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if isinstance(self.f, h5py.File):
            self.f.close()
            self.f = None

    def initialize(self):
        &#34;&#34;&#34;
        Initializes the basic groups for the HDF5 file
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        groups = [&#39;acquisition&#39;, &#39;processing&#39;, &#39;analysis&#39;, &#39;epochs&#39;, &#39;general&#39;, &#39;stimulus&#39;]
        for g in groups:
            self.f.require_group(g)
    
    def get_groups_in_path(self, path=&#39;&#39;):
        &#34;&#34;&#34;
        Returns the names of groups or datasets in a path 
        
        Parameters
        ----------
        path : str
            path to HDF5 file group
        
        Returns
        -------
        list
            Names of the groups or datasets in the path

        &#34;&#34;&#34;
        items= []
        if path in self.f:
            items = list(self.f[path].keys())            
        else:
            logging.warning(&#39;No groups in the path:&#39;+ path)
        
        return items
    
    @staticmethod
    def resolve_hdfname(data=None):
        &#34;&#34;&#34;
        Resolves and returns the name of the HDF5 file from the filenames of the NeuroChaT data
        
        Parameters
        ----------
        data
            One of the NeuroChaT data objects
        
        Returns
        -------
        hdf_name : str
            Hdf5 file name

        &#34;&#34;&#34;
        try:
            data_type = data.get_type()
        except:
            logging.error(&#39;The type of the data cannot be extracted!&#39;)
            
        hdf_name = None
        file_name = data.get_filename()
        system = data.get_system()
        if system == &#39;NWB&#39;:
            hdf_name = file_name.split(&#39;+&#39;)[0]
        if os.path.exists(file_name):
            words = file_name.split(os.sep)
            f_path = os.sep.join(words[0:-1])
            f_name = words[-1]
            if system == &#39;Axona&#39;:
                if data_type == &#39;spike&#39; or data_type == &#39;lfp&#39;:
                    hdf_name = os.sep.join([f_path, f_name.split(&#39;.&#39;)[0]+&#39;.hdf5&#39;])
                elif data_type == &#39;spatial&#39;:
                    hdf_name = os.sep.join([f_path, &#39;_&#39;.join(f_name.split(&#39;.&#39;)[0].split(&#39;_&#39;)[:-1])+&#39;.hdf5&#39;])
            elif system == &#39;Neuralynx&#39;:
                hdf_name = os.sep.join([f_path, f_path.split(os.sep)[-1]+&#39;.hdf5&#39;])

        return hdf_name

    def resolve_datapath(self, data=None):
        &#34;&#34;&#34;
        Resolves and returns path of the dataset from NeuroChaT data objects
        
        Parameters
        ----------
        data
            NeuroChaT data objects
        
        Returns
        -------
        str
            Path of the NeuroChaT data

        &#34;&#34;&#34;
        
        # No resolution for NWB file, this function will not be called if the system == &#39;NWB&#39;
        try:
            data_type = data.get_type()
        except:
            logging.error(&#39;The type of the data cannot be extracted!&#39;)
        path = None
        tag = self.get_file_tag(data)

        if data_type == &#39;spatial&#39;:
            path = &#39;/processing/Behavioural/Position&#39;
        elif tag and data_type == &#39;spike&#39;:
            path = &#39;/processing/Shank/&#39;+ tag
        elif tag and data_type == &#39;lfp&#39;:
            path = &#39;/processing/Neural Continuous/LFP/&#39;+ tag

        return path

    @staticmethod
    def get_file_tag(data=None):
        &#34;&#34;&#34;
        Resolves and returns the file tag or extension to name the group of the
        neural data in the HDF5 file
        
        Parameters
        ----------
        data : NSpike or NLfp
            Neural data objects of NeuroChaT
                    
        Returns
        -------
        str
            File extention (Axona) or name (Neuralynx) of the neural datasets

        &#34;&#34;&#34;
        try:
            data_type = data.get_type()
        except:
            logging.error(&#39;The type of the data cannot be extracted!&#39;)
        # data is one of NSpike or Nlfp instance
        tag = None
        if data_type == &#39;spike&#39; or data_type == &#39;lfp&#39;:
            f_name = data.get_filename()
            system = data.get_system()
            if system == &#39;NWB&#39;:
                tag = f_name.split(&#39;+&#39;)[-1].split(&#39;/&#39;)[-1]
            else:
                name, ext = os.path.basename(f_name).split(&#39;.&#39;)
                if system == &#39;Axona&#39;:
                    tag = ext
                elif system == &#39;Neuralynx&#39;:
                    tag = name
        return tag

    def resolve_analysis_path(self, spike=None, lfp=None):
        &#34;&#34;&#34;
        Resolves and returns path of the dataset where analysis results will be
        stroed. This path is also the unique unit ID.
        
        Parameters
        ----------
        spike : NSpike
            Spike data object
        lfp : NLfp
            Lfp data object
        
        Returns
        -------
        str
            Unique unit ID resolved from spike and lfp filenames which is also the
            name of the path to store the data of NeuroChaT analysis

        &#34;&#34;&#34;
        # Each input is an object
        try:
            data_type = spike.get_type()
        except:
            logging.error(&#39;The type of the data cannot be extracted!&#39;)
            
        path = &#39;&#39;
        if data_type == &#39;spike&#39;:
            tag = self.get_file_tag(spike)
            if spike.get_system() == &#39;Axona&#39; or not tag.startswith(&#39;TT&#39;):
                tag = &#39;TT&#39;+ tag
            path += tag + &#39;_SS_&#39;+ str(spike.get_unit_no())
        else:
            logging.error(&#39;Please specify a valid spike data!&#39;)

        try:
            data_type = lfp.get_type()
        except:
            logging.error(&#39;The type of the data cannot be extracted!&#39;)
            
        if data_type == &#39;lfp&#39;:
            path += &#39;_&#39;+ self.get_file_tag(lfp)

        return path

    def save_dataset(self, path=None, name=None, data=None, create_group=True):
        &#34;&#34;&#34;
        Stores a dataset to a specific path
        
        Parameters
        ----------
        path : str
            Path of a group in HDF5 file
        name : str
            Name of the new dataset
        data : ndarrray or list of numbers
            Data to be stored
        create_group : bool
            If True, creates a new group if the &#39;path&#39; is not in the file
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        # Path is an abosulte path of the group where the dataset will be stored
        # Abosulte path for the dataset will be created using the group path and the name of the dataset
        # If create_group = False (default), it will check if the path exists. If not error will be generated
        # If creat_group = True, it will create the group]\
        if not path:
            logging.error(&#39;Invalid group path specified!&#39;)
        if not name:
            logging.error(&#39;Please provide a name for the dataset!&#39;)
        if (path in self.f) or create_group:
            g = self.f.require_group(path)
            if name in g:
                del g[name]
            if isinstance(data, list): # This conditional restricts the None data to store, need to change
                data= [np.nan if item is None else item for item in data]
                try:
                    data = np.array(data)                    
                except:
                    pass
            try:
                g.create_dataset(name=name, data=data)
            except:
                logging.error(&#39;Error in creating &#39;+ name+ &#39; dataset to hdf5 file&#39;)            
        else:
            logging.error(&#39;hdf5 file path can be created or restored!&#39;)

    def get_dataset(self, group=None, path=&#39;&#39;, name=&#39;&#39;):
        &#34;&#34;&#34;
        Stores a dataset to a specific path
        
        Parameters
        ----------
        group : str
            Path of a group in HDF5 file
        path : str
            Name of the member group. This path is relative to the &#39;group&#39;
        name : str
            Name of the dataset
        
        Returns
        -------
        ndarray or numeric objects
            Value of the dataset

        &#34;&#34;&#34;
        
        if isinstance(group, h5py.Group):
            g = group
        else:
            g = self.f
        if path in g:
            if isinstance(g[path], h5py.Dataset):
                return np.array(g[path])
            elif isinstance(g[path], h5py.Group):
                g = g[path]
                if name in g:
                    return np.array(g[name])
                else:
                    logging.error(&#39;Specify a valid name for the required dataset&#39;)
        elif name in g:
            return np.array(g[name])
        else:
            logging.error(path + &#39; not found!&#39;+ &#39;Specify a valid path or name or check if a proper group is specified!&#39;)

#    def delete_group_data(self, path = None):
#        # Deletes everything within a group, not the group itself
#        if path in self.f:
#            g = self.f[path]
#        if g.keys():
#            for node in g.keys():
#                del self.f[path+ &#39;/&#39;+ node]

    def save_dict_recursive(self, path=None, name=None, data=None, create_group=True):
        &#34;&#34;&#34;
        Stores a dictionary dataset to a specific path. If the dictionary is nested,
        it creates a group for each of the outermost keys.
        
        Parameters
        ----------
        path : str
            Path of a group in HDF5 file
        name : str
            Name of the new dataset
        data : ndarrray or list of numbers
            Data to be stored
        create_group : bool
            If True, creates a new group if the &#39;path&#39; is not in the file
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if not isinstance(data, dict):
            logging.error(&#39;Nhdf class method save_dict_recursive() takes only dictionary data input!&#39;)
        else:
            for key, value in data.items():
                if isinstance(value, dict):
                    self.save_dict_recursive(path=path+ name+ &#39;/&#39;, name=key, data=data[key], create_group=create_group)
                else:
                    self.save_dataset(path=path + name, name=key, data=value, create_group=create_group)

    def save_attributes(self, path=None, attr=None):
        &#34;&#34;&#34;
        Stores an attribute to a group or dataset
        
        Parameters
        ----------
        path : str
            Path of a group or dataset in HDF5 file
        attr : dict
            Attribute names and values in a dictionary

        Returns
        -------
        None

        &#34;&#34;&#34;
        # path has to be the absolute path of a group
        if path in self.f:
            g = self.f[path]
            if isinstance(attr, dict):
                for key, val in attr.items():
                    g.attrs[key] = val
            else:
                logging.error(&#39;Please specify the attributes in a dictionary!&#39;)
        else:
            logging.error(&#39;Please provide a valid hdf5 path!&#39;)
    
    def save_object(self, obj=None):
        &#34;&#34;&#34;
        Stores a NeuroChaT dataset to the HDF5 file. It resolves the name first and
        then stores the data in the storage path
        
        Parameters
        ----------
        obj
            One of the NeuroChaT data types
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        try:
            obj_type= obj.get_type()
        except:
            logging.error(&#39;Cannot get object! It is not of one of the neurochat data types!&#39;)
        
        try:
            fun= getattr(self, &#39;save_&#39; + obj_type)
            fun(obj)
        except:
            logging.error(&#39;Failed to save the dataset!&#39;)
        
    def save_spatial(self, spatial=None):
        &#34;&#34;&#34;
        Stores NSpatial() dataset to the HDF5 file
        
        Parameters
        ----------
        spatial : NSpatial()
            Spatial data object in NeuroChaT
        
        Returns
        -------
        None

        &#34;&#34;&#34;

        # derive the path from the filename to ensure uniqueness
        self.set_filename(self.resolve_hdfname(data=spatial))
        # Get the lfp data path/group
        path = self.resolve_datapath(data=spatial)
        # delete old data
        if path in self.f:
            del self.f[path]
        
        # Create group afresh
        g = self.f.require_group(path)
        
        self.save_attributes(path=path, attr=spatial.get_record_info())
        
        g_loc = g.require_group(path+ &#39;/&#39;+ &#39;location&#39;)
        g_dir = g.require_group(path+ &#39;/&#39;+ &#39;direction&#39;)
        g_speed = g.require_group(path+ &#39;/&#39;+ &#39;speed&#39;)
        g_ang_vel = g.require_group(path+ &#39;/&#39;+ &#39;angular velocity&#39;)
        
        loc = np.empty((spatial.get_total_samples(), 2))
        loc[:, 0] = spatial.get_pos_x()
        loc[:, 1] = spatial.get_pos_y()
        
        g_loc.create_dataset(name=&#39;data&#39;, data=loc)
        g_loc.create_dataset(name=&#39;num_samples&#39;, data=spatial.get_total_samples())
        g_loc.create_dataset(name=&#39;timestamps&#39;, data=spatial.get_time())
        #            g_loc.create_dataset(name=&#39;unit&#39;, data=spatial.getUnit(var=&#39;speed&#39;)) # Unit information needs to be included
        # need to implement the spatial.getUnit() method
        
        g_dir.create_dataset(name=&#39;data&#39;, data=spatial.get_direction())
        g_dir.create_dataset(name=&#39;num_samples&#39;, data=spatial.get_total_samples())
        g_dir.create_dataset(name=&#39;timestamps&#39;, data=spatial.get_time())
        #            g_dir.create_dataset(name=&#39;timestamps&#39;, data=h5py.SoftLink(g_loc.name+ &#39;/timestamps&#39;))
        
        g_speed.create_dataset(name=&#39;data&#39;, data=spatial.get_speed())
        g_speed.create_dataset(name=&#39;num_samples&#39;, data=spatial.get_total_samples())
        g_speed.create_dataset(name=&#39;timestamps&#39;, data=spatial.get_time())
        
        g_ang_vel.create_dataset(name=&#39;data&#39;, data=spatial.get_ang_vel())
        g_ang_vel.create_dataset(name=&#39;num_samples&#39;, data=spatial.get_total_samples())
        g_ang_vel.create_dataset(name=&#39;timestamps&#39;, data=spatial.get_time())
        
        self.close()
        

    def save_lfp(self, lfp=None):
        &#34;&#34;&#34;
        Stores NLfp() dataset to the HDF5 file
        
        Parameters
        ----------
        lfp : NLfp()
            LFP data object in NeuroChaT
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        # derive the path from the filename to ensure uniqueness
        self.set_filename(self.resolve_hdfname(data=lfp))
        # Get the lfp data path/group
        path = self.resolve_datapath(data=lfp)
        # delete old data
        if path in self.f:
            del self.f[path]

        # Create group afresh
        g = self.f.require_group(path)

        self.save_attributes(path=path, attr=lfp.get_record_info())

        g.create_dataset(name=&#39;data&#39;, data=lfp.get_samples())
        g.create_dataset(name=&#39;num_samples&#39;, data=lfp.get_total_samples())
        g.create_dataset(name=&#39;timestamps&#39;, data=lfp.get_timestamp())
        
        self.close()

    def save_spike(self, spike=None):
        &#34;&#34;&#34;
        Stores NSpike() dataset to the HDF5 file
        
        Parameters
        ----------
        spike : NSpike()
            Spike data object in NeuroChaT
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        # derive the path from the filename to ensure uniqueness
        self.set_filename(self.resolve_hdfname(data=spike))
        # Get the spike data path/group
        path = self.resolve_datapath(data=spike)

        # delete old data
        if path in self.f:
            del self.f[path]

        # Create group afresh
        g = self.f.require_group(path)

        self.save_attributes(path=path, attr=spike.get_record_info())

        g_clust = g.require_group(path+ &#39;/&#39;+ &#39;Clustering&#39;)
        g_wave = g.require_group(path+ &#39;/&#39;+ &#39;EventWaveForm/WaveForm&#39;)

        # From chX dictionary, create a higher order np array

        waves = spike.get_waveform() # NC waves are stroed in waves[&#39;ch1&#39;], waves[&#39;ch2&#39;] etc. ways
        stacked_channels = np.empty((spike.get_total_spikes(), spike.get_samples_per_spike(), spike.get_total_channels()))
        i = 0
        for key, val in waves.items():
            stacked_channels[:, :, i] = val
            i += 1
        g_wave.create_dataset(name=&#39;data&#39;, data=stacked_channels)
        g_wave.create_dataset(name=&#39;electrode_idx&#39;, data=spike.get_channel_ids())
        g_wave.create_dataset(name=&#39;num_events&#39;, data=spike.get_total_spikes())
        g_wave.create_dataset(name=&#39;num_samples&#39;, data=spike.get_samples_per_spike())
        g_wave.create_dataset(name=&#39;timestamps&#39;, data=spike.get_timestamp())

        # save Clutser number
        g_clust.create_dataset(name=&#39;cluster_nums&#39;, data=spike.get_unit_list())
        g_clust.create_dataset(name=&#39;num&#39;, data=spike.get_unit_tags())
        g_clust.create_dataset(name=&#39;times&#39;, data=spike.get_timestamp())
        
        self.close()

    def save_cluster(self, clust=None):
        &#34;&#34;&#34;
        Stores NClust() dataset to the HDF5 file
        
        Parameters
        ----------
        clust : NClust()
            Cluster data object in NeuroChaT
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        # Nclust is a NSpike derivative (inherited from NSpike) to add clustering facilities to the NSpike data
        # But we will consider putting it within NSpike itself
        # This will store data to Shank&#39;s Clustering and Feature Extraction group
        
        logging.warning(&#39;save_cluster() method is not implemented yet!&#39;)</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="neurochat.nc_hdf.Nhdf.get_file_tag"><code class="name flex">
<span>def <span class="ident">get_file_tag</span></span>(<span>data=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Resolves and returns the file tag or extension to name the group of the
neural data in the HDF5 file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>NSpike</code> or <code>NLfp</code></dt>
<dd>Neural data objects of NeuroChaT</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>File extention (Axona) or name (Neuralynx) of the neural datasets</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@staticmethod
def get_file_tag(data=None):
    &#34;&#34;&#34;
    Resolves and returns the file tag or extension to name the group of the
    neural data in the HDF5 file
    
    Parameters
    ----------
    data : NSpike or NLfp
        Neural data objects of NeuroChaT
                
    Returns
    -------
    str
        File extention (Axona) or name (Neuralynx) of the neural datasets

    &#34;&#34;&#34;
    try:
        data_type = data.get_type()
    except:
        logging.error(&#39;The type of the data cannot be extracted!&#39;)
    # data is one of NSpike or Nlfp instance
    tag = None
    if data_type == &#39;spike&#39; or data_type == &#39;lfp&#39;:
        f_name = data.get_filename()
        system = data.get_system()
        if system == &#39;NWB&#39;:
            tag = f_name.split(&#39;+&#39;)[-1].split(&#39;/&#39;)[-1]
        else:
            name, ext = os.path.basename(f_name).split(&#39;.&#39;)
            if system == &#39;Axona&#39;:
                tag = ext
            elif system == &#39;Neuralynx&#39;:
                tag = name
    return tag</code></pre>
</details>
</dd>
<dt id="neurochat.nc_hdf.Nhdf.resolve_hdfname"><code class="name flex">
<span>def <span class="ident">resolve_hdfname</span></span>(<span>data=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Resolves and returns the name of the HDF5 file from the filenames of the NeuroChaT data</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>One of the NeuroChaT data objects</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>hdf_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Hdf5 file name</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@staticmethod
def resolve_hdfname(data=None):
    &#34;&#34;&#34;
    Resolves and returns the name of the HDF5 file from the filenames of the NeuroChaT data
    
    Parameters
    ----------
    data
        One of the NeuroChaT data objects
    
    Returns
    -------
    hdf_name : str
        Hdf5 file name

    &#34;&#34;&#34;
    try:
        data_type = data.get_type()
    except:
        logging.error(&#39;The type of the data cannot be extracted!&#39;)
        
    hdf_name = None
    file_name = data.get_filename()
    system = data.get_system()
    if system == &#39;NWB&#39;:
        hdf_name = file_name.split(&#39;+&#39;)[0]
    if os.path.exists(file_name):
        words = file_name.split(os.sep)
        f_path = os.sep.join(words[0:-1])
        f_name = words[-1]
        if system == &#39;Axona&#39;:
            if data_type == &#39;spike&#39; or data_type == &#39;lfp&#39;:
                hdf_name = os.sep.join([f_path, f_name.split(&#39;.&#39;)[0]+&#39;.hdf5&#39;])
            elif data_type == &#39;spatial&#39;:
                hdf_name = os.sep.join([f_path, &#39;_&#39;.join(f_name.split(&#39;.&#39;)[0].split(&#39;_&#39;)[:-1])+&#39;.hdf5&#39;])
        elif system == &#39;Neuralynx&#39;:
            hdf_name = os.sep.join([f_path, f_path.split(os.sep)[-1]+&#39;.hdf5&#39;])

    return hdf_name</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="neurochat.nc_hdf.Nhdf.close"><code class="name flex">
<span>def <span class="ident">close</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Closes the h5py file object</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def close(self):
    &#34;&#34;&#34;
    Closes the h5py file object
    
    Parameters
    ----------
    None
    
    Returns
    -------
    None

    &#34;&#34;&#34;
    
    if isinstance(self.f, h5py.File):
        self.f.close()
        self.f = None</code></pre>
</details>
</dd>
<dt id="neurochat.nc_hdf.Nhdf.file"><code class="name flex">
<span>def <span class="ident">file</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Opens the file, and returns the file object</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>object</code></dt>
<dd>h5py file object</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def file(self):
    &#34;&#34;&#34;
    Opens the file, and returns the file object
    
    Parameters
    ----------
    None
    
    Returns
    -------
    object
        h5py file object

    &#34;&#34;&#34;
    self.close()
    try:
        self.f = h5py.File(self._filename, &#39;a&#39;)
        self.initialize()
    except:
        logging.error(&#39;Cannot open &#39;+ self._filename)
        
    return self.f</code></pre>
</details>
</dd>
<dt id="neurochat.nc_hdf.Nhdf.get_dataset"><code class="name flex">
<span>def <span class="ident">get_dataset</span></span>(<span>self, group=None, path='', name='')</span>
</code></dt>
<dd>
<section class="desc"><p>Stores a dataset to a specific path</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>group</code></strong> :&ensp;<code>str</code></dt>
<dd>Path of a group in HDF5 file</dd>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the member group. This path is relative to the 'group'</dd>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the dataset</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code> or <code>numeric</code> <code>objects</code></dt>
<dd>Value of the dataset</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_dataset(self, group=None, path=&#39;&#39;, name=&#39;&#39;):
    &#34;&#34;&#34;
    Stores a dataset to a specific path
    
    Parameters
    ----------
    group : str
        Path of a group in HDF5 file
    path : str
        Name of the member group. This path is relative to the &#39;group&#39;
    name : str
        Name of the dataset
    
    Returns
    -------
    ndarray or numeric objects
        Value of the dataset

    &#34;&#34;&#34;
    
    if isinstance(group, h5py.Group):
        g = group
    else:
        g = self.f
    if path in g:
        if isinstance(g[path], h5py.Dataset):
            return np.array(g[path])
        elif isinstance(g[path], h5py.Group):
            g = g[path]
            if name in g:
                return np.array(g[name])
            else:
                logging.error(&#39;Specify a valid name for the required dataset&#39;)
    elif name in g:
        return np.array(g[name])
    else:
        logging.error(path + &#39; not found!&#39;+ &#39;Specify a valid path or name or check if a proper group is specified!&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_hdf.Nhdf.get_file_object"><code class="name flex">
<span>def <span class="ident">get_file_object</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the file object that is opened using h5py</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>object</code></dt>
<dd>h5py file object</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_file_object(self):
    &#34;&#34;&#34;
    Returns the file object that is opened using h5py
    
    Parameters
    ----------
    None
    
    Returns
    -------
    object
        h5py file object

    &#34;&#34;&#34;
    if isinstance(self.f, io.IOBase):
        return self.f
    else:
        logging.warning(&#39;The file Nhdf instance is not open yet, use Nhdf.File() method to open it!&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_hdf.Nhdf.get_filename"><code class="name flex">
<span>def <span class="ident">get_filename</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the full file of the HDF5 dataset</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_filename(self):
    &#34;&#34;&#34;
    Returns the full file of the HDF5 dataset
    
    Parameters
    ----------
    None
    
    Returns
    -------
    str

    &#34;&#34;&#34;
    
    return self._filename</code></pre>
</details>
</dd>
<dt id="neurochat.nc_hdf.Nhdf.get_groups_in_path"><code class="name flex">
<span>def <span class="ident">get_groups_in_path</span></span>(<span>self, path='')</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the names of groups or datasets in a path </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>path to HDF5 file group</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>Names of the groups or datasets in the path</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_groups_in_path(self, path=&#39;&#39;):
    &#34;&#34;&#34;
    Returns the names of groups or datasets in a path 
    
    Parameters
    ----------
    path : str
        path to HDF5 file group
    
    Returns
    -------
    list
        Names of the groups or datasets in the path

    &#34;&#34;&#34;
    items= []
    if path in self.f:
        items = list(self.f[path].keys())            
    else:
        logging.warning(&#39;No groups in the path:&#39;+ path)
    
    return items</code></pre>
</details>
</dd>
<dt id="neurochat.nc_hdf.Nhdf.get_type"><code class="name flex">
<span>def <span class="ident">get_type</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the type of object. For Nhdf, this is always <code>hdf</code> type</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_type(self):
    &#34;&#34;&#34;
    Returns the type of object. For Nhdf, this is always `hdf` type
    
    Parameters
    ----------
    None
    
    Returns
    -------
    str

    &#34;&#34;&#34;
    return self.__type</code></pre>
</details>
</dd>
<dt id="neurochat.nc_hdf.Nhdf.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Initializes the basic groups for the HDF5 file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def initialize(self):
    &#34;&#34;&#34;
    Initializes the basic groups for the HDF5 file
    
    Parameters
    ----------
    None
    
    Returns
    -------
    None

    &#34;&#34;&#34;
    groups = [&#39;acquisition&#39;, &#39;processing&#39;, &#39;analysis&#39;, &#39;epochs&#39;, &#39;general&#39;, &#39;stimulus&#39;]
    for g in groups:
        self.f.require_group(g)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_hdf.Nhdf.resolve_analysis_path"><code class="name flex">
<span>def <span class="ident">resolve_analysis_path</span></span>(<span>self, spike=None, lfp=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Resolves and returns path of the dataset where analysis results will be
stroed. This path is also the unique unit ID.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>spike</code></strong> :&ensp;<code>NSpike</code></dt>
<dd>Spike data object</dd>
<dt><strong><code>lfp</code></strong> :&ensp;<code>NLfp</code></dt>
<dd>Lfp data object</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>Unique unit ID resolved from spike and lfp filenames which is also the
name of the path to store the data of NeuroChaT analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def resolve_analysis_path(self, spike=None, lfp=None):
    &#34;&#34;&#34;
    Resolves and returns path of the dataset where analysis results will be
    stroed. This path is also the unique unit ID.
    
    Parameters
    ----------
    spike : NSpike
        Spike data object
    lfp : NLfp
        Lfp data object
    
    Returns
    -------
    str
        Unique unit ID resolved from spike and lfp filenames which is also the
        name of the path to store the data of NeuroChaT analysis

    &#34;&#34;&#34;
    # Each input is an object
    try:
        data_type = spike.get_type()
    except:
        logging.error(&#39;The type of the data cannot be extracted!&#39;)
        
    path = &#39;&#39;
    if data_type == &#39;spike&#39;:
        tag = self.get_file_tag(spike)
        if spike.get_system() == &#39;Axona&#39; or not tag.startswith(&#39;TT&#39;):
            tag = &#39;TT&#39;+ tag
        path += tag + &#39;_SS_&#39;+ str(spike.get_unit_no())
    else:
        logging.error(&#39;Please specify a valid spike data!&#39;)

    try:
        data_type = lfp.get_type()
    except:
        logging.error(&#39;The type of the data cannot be extracted!&#39;)
        
    if data_type == &#39;lfp&#39;:
        path += &#39;_&#39;+ self.get_file_tag(lfp)

    return path</code></pre>
</details>
</dd>
<dt id="neurochat.nc_hdf.Nhdf.resolve_datapath"><code class="name flex">
<span>def <span class="ident">resolve_datapath</span></span>(<span>self, data=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Resolves and returns path of the dataset from NeuroChaT data objects</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>NeuroChaT data objects</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>Path of the NeuroChaT data</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def resolve_datapath(self, data=None):
    &#34;&#34;&#34;
    Resolves and returns path of the dataset from NeuroChaT data objects
    
    Parameters
    ----------
    data
        NeuroChaT data objects
    
    Returns
    -------
    str
        Path of the NeuroChaT data

    &#34;&#34;&#34;
    
    # No resolution for NWB file, this function will not be called if the system == &#39;NWB&#39;
    try:
        data_type = data.get_type()
    except:
        logging.error(&#39;The type of the data cannot be extracted!&#39;)
    path = None
    tag = self.get_file_tag(data)

    if data_type == &#39;spatial&#39;:
        path = &#39;/processing/Behavioural/Position&#39;
    elif tag and data_type == &#39;spike&#39;:
        path = &#39;/processing/Shank/&#39;+ tag
    elif tag and data_type == &#39;lfp&#39;:
        path = &#39;/processing/Neural Continuous/LFP/&#39;+ tag

    return path</code></pre>
</details>
</dd>
<dt id="neurochat.nc_hdf.Nhdf.save_attributes"><code class="name flex">
<span>def <span class="ident">save_attributes</span></span>(<span>self, path=None, attr=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Stores an attribute to a group or dataset</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>Path of a group or dataset in HDF5 file</dd>
<dt><strong><code>attr</code></strong> :&ensp;<code>dict</code></dt>
<dd>Attribute names and values in a dictionary</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def save_attributes(self, path=None, attr=None):
    &#34;&#34;&#34;
    Stores an attribute to a group or dataset
    
    Parameters
    ----------
    path : str
        Path of a group or dataset in HDF5 file
    attr : dict
        Attribute names and values in a dictionary

    Returns
    -------
    None

    &#34;&#34;&#34;
    # path has to be the absolute path of a group
    if path in self.f:
        g = self.f[path]
        if isinstance(attr, dict):
            for key, val in attr.items():
                g.attrs[key] = val
        else:
            logging.error(&#39;Please specify the attributes in a dictionary!&#39;)
    else:
        logging.error(&#39;Please provide a valid hdf5 path!&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_hdf.Nhdf.save_cluster"><code class="name flex">
<span>def <span class="ident">save_cluster</span></span>(<span>self, clust=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Stores NClust() dataset to the HDF5 file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>clust</code></strong> :&ensp;<code>NClust</code>()</dt>
<dd>Cluster data object in NeuroChaT</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def save_cluster(self, clust=None):
    &#34;&#34;&#34;
    Stores NClust() dataset to the HDF5 file
    
    Parameters
    ----------
    clust : NClust()
        Cluster data object in NeuroChaT
    
    Returns
    -------
    None

    &#34;&#34;&#34;
    
    # Nclust is a NSpike derivative (inherited from NSpike) to add clustering facilities to the NSpike data
    # But we will consider putting it within NSpike itself
    # This will store data to Shank&#39;s Clustering and Feature Extraction group
    
    logging.warning(&#39;save_cluster() method is not implemented yet!&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_hdf.Nhdf.save_dataset"><code class="name flex">
<span>def <span class="ident">save_dataset</span></span>(<span>self, path=None, name=None, data=None, create_group=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Stores a dataset to a specific path</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>Path of a group in HDF5 file</dd>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the new dataset</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>ndarrray</code> or <code>list</code> of <code>numbers</code></dt>
<dd>Data to be stored</dd>
<dt><strong><code>create_group</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, creates a new group if the 'path' is not in the file</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def save_dataset(self, path=None, name=None, data=None, create_group=True):
    &#34;&#34;&#34;
    Stores a dataset to a specific path
    
    Parameters
    ----------
    path : str
        Path of a group in HDF5 file
    name : str
        Name of the new dataset
    data : ndarrray or list of numbers
        Data to be stored
    create_group : bool
        If True, creates a new group if the &#39;path&#39; is not in the file
    
    Returns
    -------
    None

    &#34;&#34;&#34;
    # Path is an abosulte path of the group where the dataset will be stored
    # Abosulte path for the dataset will be created using the group path and the name of the dataset
    # If create_group = False (default), it will check if the path exists. If not error will be generated
    # If creat_group = True, it will create the group]\
    if not path:
        logging.error(&#39;Invalid group path specified!&#39;)
    if not name:
        logging.error(&#39;Please provide a name for the dataset!&#39;)
    if (path in self.f) or create_group:
        g = self.f.require_group(path)
        if name in g:
            del g[name]
        if isinstance(data, list): # This conditional restricts the None data to store, need to change
            data= [np.nan if item is None else item for item in data]
            try:
                data = np.array(data)                    
            except:
                pass
        try:
            g.create_dataset(name=name, data=data)
        except:
            logging.error(&#39;Error in creating &#39;+ name+ &#39; dataset to hdf5 file&#39;)            
    else:
        logging.error(&#39;hdf5 file path can be created or restored!&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_hdf.Nhdf.save_dict_recursive"><code class="name flex">
<span>def <span class="ident">save_dict_recursive</span></span>(<span>self, path=None, name=None, data=None, create_group=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Stores a dictionary dataset to a specific path. If the dictionary is nested,
it creates a group for each of the outermost keys.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>Path of a group in HDF5 file</dd>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the new dataset</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>ndarrray</code> or <code>list</code> of <code>numbers</code></dt>
<dd>Data to be stored</dd>
<dt><strong><code>create_group</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, creates a new group if the 'path' is not in the file</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def save_dict_recursive(self, path=None, name=None, data=None, create_group=True):
    &#34;&#34;&#34;
    Stores a dictionary dataset to a specific path. If the dictionary is nested,
    it creates a group for each of the outermost keys.
    
    Parameters
    ----------
    path : str
        Path of a group in HDF5 file
    name : str
        Name of the new dataset
    data : ndarrray or list of numbers
        Data to be stored
    create_group : bool
        If True, creates a new group if the &#39;path&#39; is not in the file
    
    Returns
    -------
    None

    &#34;&#34;&#34;
    
    if not isinstance(data, dict):
        logging.error(&#39;Nhdf class method save_dict_recursive() takes only dictionary data input!&#39;)
    else:
        for key, value in data.items():
            if isinstance(value, dict):
                self.save_dict_recursive(path=path+ name+ &#39;/&#39;, name=key, data=data[key], create_group=create_group)
            else:
                self.save_dataset(path=path + name, name=key, data=value, create_group=create_group)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_hdf.Nhdf.save_lfp"><code class="name flex">
<span>def <span class="ident">save_lfp</span></span>(<span>self, lfp=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Stores NLfp() dataset to the HDF5 file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>lfp</code></strong> :&ensp;<code>NLfp</code>()</dt>
<dd>LFP data object in NeuroChaT</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def save_lfp(self, lfp=None):
    &#34;&#34;&#34;
    Stores NLfp() dataset to the HDF5 file
    
    Parameters
    ----------
    lfp : NLfp()
        LFP data object in NeuroChaT
    
    Returns
    -------
    None

    &#34;&#34;&#34;
    
    # derive the path from the filename to ensure uniqueness
    self.set_filename(self.resolve_hdfname(data=lfp))
    # Get the lfp data path/group
    path = self.resolve_datapath(data=lfp)
    # delete old data
    if path in self.f:
        del self.f[path]

    # Create group afresh
    g = self.f.require_group(path)

    self.save_attributes(path=path, attr=lfp.get_record_info())

    g.create_dataset(name=&#39;data&#39;, data=lfp.get_samples())
    g.create_dataset(name=&#39;num_samples&#39;, data=lfp.get_total_samples())
    g.create_dataset(name=&#39;timestamps&#39;, data=lfp.get_timestamp())
    
    self.close()</code></pre>
</details>
</dd>
<dt id="neurochat.nc_hdf.Nhdf.save_object"><code class="name flex">
<span>def <span class="ident">save_object</span></span>(<span>self, obj=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Stores a NeuroChaT dataset to the HDF5 file. It resolves the name first and
then stores the data in the storage path</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd>One of the NeuroChaT data types</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def save_object(self, obj=None):
    &#34;&#34;&#34;
    Stores a NeuroChaT dataset to the HDF5 file. It resolves the name first and
    then stores the data in the storage path
    
    Parameters
    ----------
    obj
        One of the NeuroChaT data types
    
    Returns
    -------
    None

    &#34;&#34;&#34;
    try:
        obj_type= obj.get_type()
    except:
        logging.error(&#39;Cannot get object! It is not of one of the neurochat data types!&#39;)
    
    try:
        fun= getattr(self, &#39;save_&#39; + obj_type)
        fun(obj)
    except:
        logging.error(&#39;Failed to save the dataset!&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_hdf.Nhdf.save_spatial"><code class="name flex">
<span>def <span class="ident">save_spatial</span></span>(<span>self, spatial=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Stores NSpatial() dataset to the HDF5 file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>spatial</code></strong> :&ensp;<code>NSpatial</code>()</dt>
<dd>Spatial data object in NeuroChaT</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def save_spatial(self, spatial=None):
    &#34;&#34;&#34;
    Stores NSpatial() dataset to the HDF5 file
    
    Parameters
    ----------
    spatial : NSpatial()
        Spatial data object in NeuroChaT
    
    Returns
    -------
    None

    &#34;&#34;&#34;

    # derive the path from the filename to ensure uniqueness
    self.set_filename(self.resolve_hdfname(data=spatial))
    # Get the lfp data path/group
    path = self.resolve_datapath(data=spatial)
    # delete old data
    if path in self.f:
        del self.f[path]
    
    # Create group afresh
    g = self.f.require_group(path)
    
    self.save_attributes(path=path, attr=spatial.get_record_info())
    
    g_loc = g.require_group(path+ &#39;/&#39;+ &#39;location&#39;)
    g_dir = g.require_group(path+ &#39;/&#39;+ &#39;direction&#39;)
    g_speed = g.require_group(path+ &#39;/&#39;+ &#39;speed&#39;)
    g_ang_vel = g.require_group(path+ &#39;/&#39;+ &#39;angular velocity&#39;)
    
    loc = np.empty((spatial.get_total_samples(), 2))
    loc[:, 0] = spatial.get_pos_x()
    loc[:, 1] = spatial.get_pos_y()
    
    g_loc.create_dataset(name=&#39;data&#39;, data=loc)
    g_loc.create_dataset(name=&#39;num_samples&#39;, data=spatial.get_total_samples())
    g_loc.create_dataset(name=&#39;timestamps&#39;, data=spatial.get_time())
    #            g_loc.create_dataset(name=&#39;unit&#39;, data=spatial.getUnit(var=&#39;speed&#39;)) # Unit information needs to be included
    # need to implement the spatial.getUnit() method
    
    g_dir.create_dataset(name=&#39;data&#39;, data=spatial.get_direction())
    g_dir.create_dataset(name=&#39;num_samples&#39;, data=spatial.get_total_samples())
    g_dir.create_dataset(name=&#39;timestamps&#39;, data=spatial.get_time())
    #            g_dir.create_dataset(name=&#39;timestamps&#39;, data=h5py.SoftLink(g_loc.name+ &#39;/timestamps&#39;))
    
    g_speed.create_dataset(name=&#39;data&#39;, data=spatial.get_speed())
    g_speed.create_dataset(name=&#39;num_samples&#39;, data=spatial.get_total_samples())
    g_speed.create_dataset(name=&#39;timestamps&#39;, data=spatial.get_time())
    
    g_ang_vel.create_dataset(name=&#39;data&#39;, data=spatial.get_ang_vel())
    g_ang_vel.create_dataset(name=&#39;num_samples&#39;, data=spatial.get_total_samples())
    g_ang_vel.create_dataset(name=&#39;timestamps&#39;, data=spatial.get_time())
    
    self.close()</code></pre>
</details>
</dd>
<dt id="neurochat.nc_hdf.Nhdf.save_spike"><code class="name flex">
<span>def <span class="ident">save_spike</span></span>(<span>self, spike=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Stores NSpike() dataset to the HDF5 file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>spike</code></strong> :&ensp;<code>NSpike</code>()</dt>
<dd>Spike data object in NeuroChaT</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def save_spike(self, spike=None):
    &#34;&#34;&#34;
    Stores NSpike() dataset to the HDF5 file
    
    Parameters
    ----------
    spike : NSpike()
        Spike data object in NeuroChaT
    
    Returns
    -------
    None

    &#34;&#34;&#34;
    
    # derive the path from the filename to ensure uniqueness
    self.set_filename(self.resolve_hdfname(data=spike))
    # Get the spike data path/group
    path = self.resolve_datapath(data=spike)

    # delete old data
    if path in self.f:
        del self.f[path]

    # Create group afresh
    g = self.f.require_group(path)

    self.save_attributes(path=path, attr=spike.get_record_info())

    g_clust = g.require_group(path+ &#39;/&#39;+ &#39;Clustering&#39;)
    g_wave = g.require_group(path+ &#39;/&#39;+ &#39;EventWaveForm/WaveForm&#39;)

    # From chX dictionary, create a higher order np array

    waves = spike.get_waveform() # NC waves are stroed in waves[&#39;ch1&#39;], waves[&#39;ch2&#39;] etc. ways
    stacked_channels = np.empty((spike.get_total_spikes(), spike.get_samples_per_spike(), spike.get_total_channels()))
    i = 0
    for key, val in waves.items():
        stacked_channels[:, :, i] = val
        i += 1
    g_wave.create_dataset(name=&#39;data&#39;, data=stacked_channels)
    g_wave.create_dataset(name=&#39;electrode_idx&#39;, data=spike.get_channel_ids())
    g_wave.create_dataset(name=&#39;num_events&#39;, data=spike.get_total_spikes())
    g_wave.create_dataset(name=&#39;num_samples&#39;, data=spike.get_samples_per_spike())
    g_wave.create_dataset(name=&#39;timestamps&#39;, data=spike.get_timestamp())

    # save Clutser number
    g_clust.create_dataset(name=&#39;cluster_nums&#39;, data=spike.get_unit_list())
    g_clust.create_dataset(name=&#39;num&#39;, data=spike.get_unit_tags())
    g_clust.create_dataset(name=&#39;times&#39;, data=spike.get_timestamp())
    
    self.close()</code></pre>
</details>
</dd>
<dt id="neurochat.nc_hdf.Nhdf.set_filename"><code class="name flex">
<span>def <span class="ident">set_filename</span></span>(<span>self, filename=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Sets the full file of the HDF5 dataset</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename of the HDF5 dataset</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_filename(self, filename=None):
    &#34;&#34;&#34;
    Sets the full file of the HDF5 dataset
    
    Parameters
    ----------
    filename : str
        Filename of the HDF5 dataset
    
    Returns
    -------
    None

    &#34;&#34;&#34;
    
    if filename:
        self._filename = filename
    try:
        self.file()
    except:
        logging.error(&#39;Invalid file!&#39;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="neurochat" href="index.html">neurochat</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="neurochat.nc_hdf.Nhdf" href="#neurochat.nc_hdf.Nhdf">Nhdf</a></code></h4>
<ul class="">
<li><code><a title="neurochat.nc_hdf.Nhdf.close" href="#neurochat.nc_hdf.Nhdf.close">close</a></code></li>
<li><code><a title="neurochat.nc_hdf.Nhdf.file" href="#neurochat.nc_hdf.Nhdf.file">file</a></code></li>
<li><code><a title="neurochat.nc_hdf.Nhdf.get_dataset" href="#neurochat.nc_hdf.Nhdf.get_dataset">get_dataset</a></code></li>
<li><code><a title="neurochat.nc_hdf.Nhdf.get_file_object" href="#neurochat.nc_hdf.Nhdf.get_file_object">get_file_object</a></code></li>
<li><code><a title="neurochat.nc_hdf.Nhdf.get_file_tag" href="#neurochat.nc_hdf.Nhdf.get_file_tag">get_file_tag</a></code></li>
<li><code><a title="neurochat.nc_hdf.Nhdf.get_filename" href="#neurochat.nc_hdf.Nhdf.get_filename">get_filename</a></code></li>
<li><code><a title="neurochat.nc_hdf.Nhdf.get_groups_in_path" href="#neurochat.nc_hdf.Nhdf.get_groups_in_path">get_groups_in_path</a></code></li>
<li><code><a title="neurochat.nc_hdf.Nhdf.get_type" href="#neurochat.nc_hdf.Nhdf.get_type">get_type</a></code></li>
<li><code><a title="neurochat.nc_hdf.Nhdf.initialize" href="#neurochat.nc_hdf.Nhdf.initialize">initialize</a></code></li>
<li><code><a title="neurochat.nc_hdf.Nhdf.resolve_analysis_path" href="#neurochat.nc_hdf.Nhdf.resolve_analysis_path">resolve_analysis_path</a></code></li>
<li><code><a title="neurochat.nc_hdf.Nhdf.resolve_datapath" href="#neurochat.nc_hdf.Nhdf.resolve_datapath">resolve_datapath</a></code></li>
<li><code><a title="neurochat.nc_hdf.Nhdf.resolve_hdfname" href="#neurochat.nc_hdf.Nhdf.resolve_hdfname">resolve_hdfname</a></code></li>
<li><code><a title="neurochat.nc_hdf.Nhdf.save_attributes" href="#neurochat.nc_hdf.Nhdf.save_attributes">save_attributes</a></code></li>
<li><code><a title="neurochat.nc_hdf.Nhdf.save_cluster" href="#neurochat.nc_hdf.Nhdf.save_cluster">save_cluster</a></code></li>
<li><code><a title="neurochat.nc_hdf.Nhdf.save_dataset" href="#neurochat.nc_hdf.Nhdf.save_dataset">save_dataset</a></code></li>
<li><code><a title="neurochat.nc_hdf.Nhdf.save_dict_recursive" href="#neurochat.nc_hdf.Nhdf.save_dict_recursive">save_dict_recursive</a></code></li>
<li><code><a title="neurochat.nc_hdf.Nhdf.save_lfp" href="#neurochat.nc_hdf.Nhdf.save_lfp">save_lfp</a></code></li>
<li><code><a title="neurochat.nc_hdf.Nhdf.save_object" href="#neurochat.nc_hdf.Nhdf.save_object">save_object</a></code></li>
<li><code><a title="neurochat.nc_hdf.Nhdf.save_spatial" href="#neurochat.nc_hdf.Nhdf.save_spatial">save_spatial</a></code></li>
<li><code><a title="neurochat.nc_hdf.Nhdf.save_spike" href="#neurochat.nc_hdf.Nhdf.save_spike">save_spike</a></code></li>
<li><code><a title="neurochat.nc_hdf.Nhdf.set_filename" href="#neurochat.nc_hdf.Nhdf.set_filename">set_filename</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.6.3</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>