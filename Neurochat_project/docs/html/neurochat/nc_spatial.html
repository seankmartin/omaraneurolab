<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.6.3" />
<title>neurochat.nc_spatial API documentation</title>
<meta name="description" content="This module implements NSpatial Class for NeuroChaT software â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>neurochat.nc_spatial</code></h1>
</header>
<section id="section-intro">
<p>This module implements NSpatial Class for NeuroChaT software</p>
<p>@author: Md Nurul Islam; islammn at tcd dot ie</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
&#34;&#34;&#34;
This module implements NSpatial Class for NeuroChaT software

@author: Md Nurul Islam; islammn at tcd dot ie
&#34;&#34;&#34;

import os
import re

import logging

from collections import OrderedDict as oDict
from copy import deepcopy

from neurochat.nc_utils import chop_edges, corr_coeff, extrema,\
            find, find2d, find_chunk, histogram, histogram2d, \
            linfit, residual_stat, rot_2d, smooth_1d, smooth_2d, \
            centre_of_mass, find_true_ranges

from neurochat.nc_base import NAbstract
from neurochat.nc_circular import CircStat
from neurochat.nc_hdf import Nhdf

from neurochat.nc_spike import NSpike
from neurochat.nc_lfp import NLfp
from neurochat.nc_event import NEvent

import numpy as np
import numpy.random as nprand

import scipy as sc
from scipy.optimize import curve_fit

import scipy.signal as sg

from sklearn.linear_model import LinearRegression

class NSpatial(NAbstract):
    &#34;&#34;&#34;
    This data class is the placeholder for the dataset that contains information
    about the spatial behaviour of the animal. It decodes data from different 
    formats and analyses the correlation of spatial information with the spiking
    activity of a unit.
    &#34;&#34;&#34;    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._time = []
        self._timestamp = []
        self._total_samples = []
        self._fs = []
        self._pixel_size = 3
        self._pos_x = []
        self._pos_y = []
        self._direction = []
        self._speed = []
        self._ang_vel = []
        self._border_dist = []
        self._xbound = []
        self._ybound = []
        self._dist_map = []
        self.spike = []
        self.lfp = []
        
        self.__type = &#39;spatial&#39;
        
    def get_type(self):
        &#34;&#34;&#34;
        Returns the type of object. For NSpatial, this is always `spatial` type
        
        Parameters
        ----------
        None
        
        Returns
        -------
        str

        &#34;&#34;&#34; 
        
        return self.__type        

    def subsample(self, sample_range=None):
        &#34;&#34;&#34;
        Extract a time range from the positions.

        NOTE for now, the duration will be longer than sample time.
        Duration is actually from 0 to max recording length.
        This is to easier match ndata which assumes recordings start at 0.
        
        Parameters
        ----------
        sample_range : tuple
            the time in seconds to extract from the positions.
        
        Returns
        -------
        NSpike
            subsampled version of initial spatial object
        &#34;&#34;&#34;
        if sample_range is None:
            return self
        new_spatial = deepcopy(self)
        lower, upper = sample_range
        times = self._time
        sample_spatial_idx = (
            (times &lt;= upper) &amp; (times &gt;= lower)).nonzero()
        new_spatial._set_time(self._time[sample_spatial_idx])
        new_spatial._set_pos_x(self._pos_x[sample_spatial_idx])
        new_spatial._set_pos_y(self._pos_y[sample_spatial_idx])
        new_spatial._set_direction(self._direction[sample_spatial_idx])
        new_spatial._set_speed(self._speed[sample_spatial_idx])
        new_spatial.set_ang_vel(self._ang_vel[sample_spatial_idx])
        # NOTE can use to set proper duration
        #new_spatial._set_duration(upper-lower)
        return new_spatial

    def set_pixel_size(self, pixel_size):
        &#34;&#34;&#34;
        Sets the size of pixel size by which the entire foraged arena is tessellated
        
        Parameters
        ----------
        pixel_size : int
            Pixel size of the foraged arena
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        self._pixel_size = pixel_size
        
    def _set_time(self, time):
        &#34;&#34;&#34;
        Sets the time for all the spatial samples. It also resets the `timestamp`
        or the the temporal resolution of the spatial samples and recalculates
        the sampling rate.
        
        Parameters
        ----------
        time : list or ndarray
            Timestamps of all spatial samples
            
        Returns
        -------
        None

        &#34;&#34;&#34;
  
        
        self._time = time
        self._set_timestamp()
        self._set_sampling_rate()
        
    def _set_timestamp(self, timestamp=None):
        &#34;&#34;&#34;
        Sets the timestamps for the spatial information. Here, it is defined as
        the temporal resolution of the spatial samples, not the happening time of
        each sample. This way it is different from NLfp and NSpike timestamp 
        definition
        
        Parameters
        ----------
        timestamp : list or ndarray
            Timestamps of all spiking waveforms
            
        Returns
        -------
        None

        &#34;&#34;&#34;

        
        if timestamp:
            self._timestamp = timestamp
        elif np.array(self._time).any():
            self._timestamp = np.diff(np.array(self._time)).mean()
            
    def _set_sampling_rate(self, sampling_rate=None):
        &#34;&#34;&#34;
        Sets the sampling rate of the spatial information
                
        Parameters
        ----------
        sampling_rate : int
            Sampling rate of the spatial information
 
        Returns
        -------
        None    

        &#34;&#34;&#34;
        
        if sampling_rate:
            self._fs = sampling_rate
        elif np.array(self._time).any():
            self._fs = 1/np.diff(np.array(self._time)).mean()

    def _set_pos_x(self, pos_x):
        &#34;&#34;&#34;
        Sets the X-coordinate of the location of the animal
                
        Parameters
        ----------
        pos_x : ndarray
            X-ccordinate of the location of the animal
 
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        self._pos_x = pos_x
        
    def _set_pos_y(self, pos_y):
        &#34;&#34;&#34;
        Sets the Y-coordinate of the location of the animal
                
        Parameters
        ----------
        pos_y : ndarray
            Y-ccordinate of the location of the animal
 
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        self._pos_y = pos_y
        
    def _set_direction(self, direction):
        &#34;&#34;&#34;
        Sets the head-direction of the animal
                
        Parameters
        ----------
        direction : ndarray
            Head-direction of the animal
 
        Returns
        -------
        None

        &#34;&#34;&#34;
        self._direction = direction
        
    def _set_speed(self, speed):
        &#34;&#34;&#34;
        Sets the speed of the animal
                
        Parameters
        ----------
        speed : ndarray
            Speed of the animal
 
        Returns
        -------
        None

        &#34;&#34;&#34;
        self._speed = speed
        
    def set_ang_vel(self, ang_vel):
        &#34;&#34;&#34;
        Sets the angular head velocity (AHV) of the animal
                
        Parameters
        ----------
        ang_vel : ndarray
            Angular head velocity (AHV) of the animal
 
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        self._ang_vel = ang_vel
        
    def set_border(self, border):
        &#34;&#34;&#34;
        Sets the distance of the animal from the arena border
                
        Parameters
        ----------
        border : ndarray
            Distance of the animal from the arena border
 
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        self._border_dist = border[0]
        self._xbound = border[1]
        self._ybound = border[2]
        self._dist_map = border[3]

    def get_total_samples(self):
        &#34;&#34;&#34;
        Returns the number of spatial samples
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Total spatial samples

        &#34;&#34;&#34;        
        return self._time.size
    
    def get_sampling_rate(self):
        &#34;&#34;&#34;
        Returns the sampling rate of the spatial samples
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Spatial data sampling rate

        &#34;&#34;&#34;
    
        return self._fs
    
    def get_duration(self):
        &#34;&#34;&#34;
        Returns the duration of the experiment
                
        Parameters
        ----------
        None
 
        Returns
        -------
        float
            Duration of the experiment

        &#34;&#34;&#34;
        if len(self._time) == 0:
            return 0
        return self._time[-1]
        
    
    def get_pixel_size(self):
        &#34;&#34;&#34;
        Returns the pixel size of the recorded arena
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Pixel size

        &#34;&#34;&#34;    
        return self._pixel_size
    
    def get_time(self):
        &#34;&#34;&#34;
        Returns the time of individual spatial samples
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Total spatial samples

        &#34;&#34;&#34;
    
        return self._time
    
    def get_timestamp(self):
        &#34;&#34;&#34;
        Returns the temporal resolution of spatial samples
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Temporal resolution of spatial samples

        &#34;&#34;&#34;
        
        return self._timestamp
    
    def get_pos_x(self):
        &#34;&#34;&#34;
        Returns the X-ccordinates of animal&#39;s location
                
        Parameters
        ----------
        None
 
        Returns
        -------
        ndarray
            X-coordinates of animal&#39;s location

        &#34;&#34;&#34;
    
        return self._pos_x
    
    def get_pos_y(self):
        &#34;&#34;&#34;
        Returns the Y-ccordinates of animal&#39;s location
                
        Parameters
        ----------
        None
 
        Returns
        -------
        ndarray
            Y-coordinates of animal&#39;s location

        &#34;&#34;&#34;
    
        return self._pos_y
    
    def get_direction(self):
        &#34;&#34;&#34;
        Returns head direction of the animal
                
        Parameters
        ----------
        None
 
        Returns
        -------
        ndarray
            Head direction of the animal

        &#34;&#34;&#34;
        
        return self._direction
    
    def get_speed(self):
        &#34;&#34;&#34;
        Returns speed of the animal
                
        Parameters
        ----------
        None
 
        Returns
        -------
        ndarray
            Speed of the animal

        &#34;&#34;&#34;       
        return self._speed
    
    def get_ang_vel(self):
        &#34;&#34;&#34;
        Returns angular head velocity of the animal
                
        Parameters
        ----------
        None
 
        Returns
        -------
        ndarray
            Angular head velocity of the animal

        &#34;&#34;&#34;
        
        return self._ang_vel
    
    def get_border(self):
        &#34;&#34;&#34;
        Returns animal&#39;s distance from the border
                
        Parameters
        ----------
        None
 
        Returns
        -------
        ndarray
            Animal&#39;s distance from the border

        &#34;&#34;&#34;
        
        return self._border_dist, self._xbound, self._ybound, self._dist_map

    def set_spike(self, spike, **kwargs):
        &#34;&#34;&#34;
        Adds the NSpike object to NSpatial object 
                
        Parameters
        ----------
        spike : NSpike
            NSpike object to be added to the NSpatial object. If no spike object
            is provided, a new NSpike() object is created.
        **kwargs
            Keyword argumemts for creating the new NSpike instance
 
        Returns
        -------
        None

        &#34;&#34;&#34;

        
        if spike is isinstance(spike, NSpike):
            self.spike = spike
        else:
            cls = NSpike if not spike else spike
            spike = cls(**kwargs)
        self.spike = spike

    def set_lfp(self, lfp, **kwargs):
        &#34;&#34;&#34;
        Adds the NLfp object to NSpatial object 
                
        Parameters
        ----------
        lfp : NLfp
            NLfp object to be added to the NSpatial object. If no spike object
            is provided, a new NLfp() object is created.
        **kwargs
            Keyword argumemts for creating the new NLfp instance
 
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if lfp is isinstance(lfp, NLfp):
            self.lfp = lfp
        else:
            cls = NLfp if not lfp else lfp
            lfp = cls(**kwargs)
        self.lfp = lfp

    def set_spike_name(self, name=None):
        &#34;&#34;&#34;
        Sets the name of the spike dataset
        
        Parameters
        ----------
        name : str
            Name of the spike dataset
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if name is not None:
            self.spike.set_name(name)
            
    def set_spike_filename(self, filename=None):
        &#34;&#34;&#34;
        Sets file name of the spike dataset
        
        Parameters
        ----------
        name : str
            Full file directory of the spike dataset
        
        Returns
        -------
        None
        &#34;&#34;&#34;
        
        if filename is not None:
            self.spike.set_filename()

    def set_lfp_name(self, name=None):
        &#34;&#34;&#34;
        Sets the name of the lfp dataset
        
        Parameters
        ----------
        name : str
            Name of the lfp dataset
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        self.lfp.set_name(name)
        
    def set_lfp_filename(self, filename=None):
        &#34;&#34;&#34;
        Sets file name of the lfp dataset
        
        Parameters
        ----------
        name : str
            Full file directory of the lfp dataset
        
        Returns
        -------
        None
        &#34;&#34;&#34;
        self.lfp.set_filename(filename)

    def set_event(self, event, **kwargs):
        &#34;&#34;&#34;
        Sets the NEvent() object to NSpatial().         
        
        Parameters
        ----------
        event
            NEvent or its childclass or NEvent() object
        
        Returns
        -------
        NEvent()
        
        &#34;&#34;&#34;
        
        if event is isinstance(event, NEvent):
            self.event = event
        else:
            cls = NEvent if not event else event
            event = cls(**kwargs)
        self.event = event

    def set_event_name(self, name=None):
        &#34;&#34;&#34;
        Sets the name of the event object.         
        
        Parameters
        ----------
        name : str
            Name of the vent dataset
        
        Returns
        -------
        None
        
        &#34;&#34;&#34;
        
        self.event.set_name(name)
        
    def set_event_filename(self, filename=None):
        &#34;&#34;&#34;
        Sets the filename for the event
        
        Parameters
        ----------
        filename : str
            Full file of the event dataset
        
        Returns
        -------
        None
        
        &#34;&#34;&#34;
        
        self.event.set_filename(filename)

    def set_system(self, system=None):
        &#34;&#34;&#34;
        Sets the data format or recording system.
        
        Parameters
        ----------
        system : str
            Data format or recording system
        
        Returns
        -------
        None
        
        &#34;&#34;&#34;
        
        if system is not None:
            self._system = system

            if self.spike:
                self.spike.set_system(system)
            if self.lfp:
                self.lfp.set_system(system)

    def load_spike(self):
        &#34;&#34;&#34;
        Loads the composing spike object         
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None        
        
        &#34;&#34;&#34;
        
        self.spike.load()
        
    def load_lfp(self):
        &#34;&#34;&#34;
        Loads the composite lfp object         
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None        
        
        &#34;&#34;&#34;

        
        self.lfp.load()

    def save_to_hdf5(self, file_name=None, system=None):
        &#34;&#34;&#34;
        Save spatial dataset to HDF5 file         
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None        
        
        &#34;&#34;&#34;

        hdf = Nhdf()
        if file_name and system:
            if os.path.exists(file_name):
                self.set_filename(file_name)
                self.set_system(system)
                self.load()
            else:
                logging.error(&#39;Specified file cannot be found!&#39;)

        hdf.save_spatial(spatial=self)
        hdf.close()

    def load(self, filename=None, system=None):
        &#34;&#34;&#34;
        Loads the spatial object         
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None        
        
        &#34;&#34;&#34;

        if system is None:
            system = self._system
        else:
            self._system = system
        if filename is None:
            filename = self._filename
        else:
            filename = self._filename
        loader = getattr(self, &#39;load_spatial_&#39;+ system)
        loader(filename)
        try:
            self.smooth_speed()
        except:
            logging.warning(self.get_system() + &#39; files may not have speed data!&#39;)
        if not np.array(self._ang_vel).any():
            self.set_ang_vel(self.calc_ang_vel())
        self.set_border(self.calc_border())

    def load_spatial_Axona(self, file_name):
        &#34;&#34;&#34;
        Loads Axona format spatial data to the NSpatial() object
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None        
        
        &#34;&#34;&#34;

        try:
            f = open(file_name, &#39;rt&#39;)
            self._set_data_source(file_name)
            self._set_source_format(&#39;Axona&#39;)
            while True:
                line = f.readline()
                if line == &#39;&#39;:
                    break
                elif line.startswith(&#39;time&#39;):
                    spatial_data = np.loadtxt(f, dtype=&#39;float&#39;, usecols=range(5))
            self._set_time(spatial_data[:, 0])
            self._set_pos_x(spatial_data[:, 1]- np.min(spatial_data[:, 1]))
            self._set_pos_y(spatial_data[:, 2]- np.min(spatial_data[:, 2]))
            self._set_direction(spatial_data[:, 3])
            self._set_speed(spatial_data[:, 4])
            f.seek(0, 0)
            pixel_size = list(map(float, re.findall(r&#34;\d+.\d+|\d+&#34;, f.readline())))
            self.set_pixel_size(pixel_size)
            self.smooth_direction()
        except:
            logging.error(&#39;File does not exist or is open in another process!&#39;)

    def load_spatial_NWB(self, file_name):
        &#34;&#34;&#34;
        Loads HDF5 format spatial data to the NSpatial() object
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None        
        
        &#34;&#34;&#34;
        file_name, path = file_name.split(&#39;+&#39;)
        if os.path.exists(file_name):
            hdf = Nhdf()
            hdf.set_filename(file_name)

            _record_info = {}
    
            if path in hdf.f:
                g = hdf.f[path]
            elif &#39;/processing/Behavioural/Position&#39; in hdf.f:
                path = &#39;/processing/Behavioural/Position&#39;
                g = hdf.f[path]
                logging.info(&#39;Path for spatial data set to: &#39; + path)
            else:
                logging.error(&#39;Path for spatial data does not exist!&#39;)
    
            for key, value in g.attrs.items():
                _record_info[key] = value
            
            self.set_record_info(_record_info)
    
            if path+ &#39;/&#39;+ &#39;location&#39; in g:
                g_loc = g[path+ &#39;/&#39;+ &#39;location&#39;]
                data = hdf.get_dataset(group=g_loc, name=&#39;data&#39;)
                self._set_pos_x(data[:, 0])
                self._set_pos_y(data[:, 1])
                self._set_time(hdf.get_dataset(group=g_loc, name=&#39;timestamps&#39;))
            else:
                logging.error(&#39;Spatial location information not found!&#39;)
    
            if path+ &#39;/&#39;+ &#39;direction&#39; in g:
                g_dir = g[path+ &#39;/&#39;+ &#39;direction&#39;]
                data = hdf.get_dataset(group=g_dir, name=&#39;data&#39;)
                self._set_direction(data)
            else:
                logging.error(&#39;Spatial direction information not found!&#39;)
    
            if path+ &#39;/&#39;+ &#39;speed&#39; in g:
                g_speed = g[path+ &#39;/&#39;+ &#39;speed&#39;]
                data = hdf.get_dataset(group=g_speed, name=&#39;data&#39;)
                self._set_speed(data)
            else:
                logging.error(&#39;Spatial speed information not found!&#39;)
    
            if path+ &#39;/&#39;+ &#39;angular velocity&#39; in g:
                g_ang_vel = g[path+ &#39;/&#39;+ &#39;angular velocity&#39;]
                data = hdf.get_dataset(group=g_ang_vel, name=&#39;data&#39;)
                self.set_ang_vel(data)
            else:
                self.set_ang_vel(np.array([]))
                logging.warning(&#39;Spatial angular velocity information not found, will be calculated from direction!&#39;)
    
            hdf.close()
            
    def load_spatial_Neuralynx(self, file_name):
        &#34;&#34;&#34;
        Loads Neuralynx format spatial data to the NSpatial() object
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None        
        
        &#34;&#34;&#34;

        self._set_data_source(file_name)
        self._set_source_format(&#39;Neuralynx&#39;)

        # Format description for the NLX file:
        header_offset = 16*1024 # fixed for NLX files

        bytes_start_record = 2
        bytes_origin_id = 2
        bytes_videoRec_size = 2
        bytes_per_timestamp = 8
        bytes_per_bitfield = 4*400
        bytes_sncrc = 2
        bytes_per_xloc = 4
        bytes_per_yloc = 4
        bytes_per_angle = 4
        bytes_per_target = 4*50

        record_size = None
        with open(file_name, &#39;rb&#39;) as f:
            while True:
                line = f.readline()
                try:
                    line = line.decode(&#39;UTF-8&#39;)
                except:
                    break

                if line == &#39;&#39;:
                    break
                if &#39;SamplingFrequency&#39; in line:
                    self._set_sampling_rate(float(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line))))
                if &#39;RecordSize&#39; in line:
                    record_size = int(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line)))
                if &#39;Time Opened&#39; in line:
                    self._set_date(re.search(r&#39;\d+/\d+/\d+&#39;, line).group())
                    self._set_time(re.search(r&#39;\d+:\d+:\d+&#39;, line).group())
                if &#39;FileVersion&#39; in line:
                    self._set_file_version(line.split()[1])

            if not record_size:
                record_size = bytes_start_record+ \
                             bytes_origin_id+ \
                             bytes_videoRec_size+  \
                             bytes_per_timestamp+ \
                             bytes_per_bitfield+ \
                             bytes_sncrc+ \
                             bytes_per_xloc+ \
                             bytes_per_yloc+ \
                             bytes_per_angle+ \
                             bytes_per_target

            time_offset = bytes_start_record+ \
                             bytes_origin_id+ \
                             bytes_videoRec_size
            xloc_offset = time_offset+ \
                         bytes_per_timestamp+ \
                         bytes_per_bitfield+ \
                         bytes_sncrc
            yloc_offset = xloc_offset+bytes_per_xloc
            angle_offset = yloc_offset+bytes_per_xloc

            f.seek(0, 2)
            self._total_samples = int((f.tell()- header_offset)/ record_size)
            spatial_data = np.zeros([self._total_samples, 4])

            f.seek(header_offset)
            for i in np.arange(self._total_samples):
                sample_bytes = np.fromfile(f, dtype=&#39;uint8&#39;, count=record_size)
                spatial_data[i, 0] = int.from_bytes(sample_bytes[time_offset+ np.arange(bytes_per_timestamp)], byteorder=&#39;little&#39;, signed=False)
                spatial_data[i, 1] = int.from_bytes(sample_bytes[xloc_offset+ np.arange(bytes_per_xloc)], byteorder=&#39;little&#39;, signed=False)
                spatial_data[i, 2] = int.from_bytes(sample_bytes[yloc_offset+ np.arange(bytes_per_yloc)], byteorder=&#39;little&#39;, signed=False)
                spatial_data[i, 3] = int.from_bytes(sample_bytes[angle_offset+ np.arange(bytes_per_angle)], byteorder=&#39;little&#39;, signed=False)

            spatial_data[:, 0] /= 10**6
            spatial_data[:, 0] -= np.min(spatial_data[:, 0])
            self._timestamp = np.mean(np.diff(spatial_data[:, 0]))
            self._set_sampling_rate(1/self._timestamp)
            self._set_time(spatial_data[:, 0])
            self._set_pos_x(spatial_data[:, 1]- np.min(spatial_data[:, 1]))
            self._set_pos_y(spatial_data[:, 2]- np.min(spatial_data[:, 2]))
            self._set_direction(spatial_data[:, 3])
            # Neuralynx data does not have any speed information
            self.smooth_direction()

    def smooth_speed(self):
        &#34;&#34;&#34;
        Smoothes the speed data using a moving-average box filter
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None        
        
        &#34;&#34;&#34;
        
        self._set_speed(smooth_1d(self.get_speed(), &#39;b&#39;, 5))
        
    def smooth_direction(self):
        &#34;&#34;&#34;
        Smoothes the angular head direction data using a moving circular average
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None
        
        See also
        --------
        nc_circular.CircStat().circ_smooth()
        
        &#34;&#34;&#34;
        
        cs = CircStat()
        cs.set_theta(self.get_direction())
        self._set_direction(cs.circ_smooth(filttype=&#39;b&#39;, filtsize=5))

    def calc_ang_vel(self, npoint=5):
        &#34;&#34;&#34;
        Calculates the angular head velocity of the animal from the direction data
        Each sample is the slope of a fitted line of five directional data centred
        around current sample.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None        
        
        &#34;&#34;&#34;
        
        theta = self.get_direction()
        ang_vel = np.zeros(theta.shape)
        N = theta.size
        L = npoint
        l = int(np.floor(L/2))
        cs = CircStat()
        for i in np.arange(l):
            y = cs.circ_regroup(theta[:L-l+ i])
            ang_vel[i] = np.polyfit(np.arange(len(y)), y, 1)[0]

        for i in np.arange(l, N- l, 1):
            y = cs.circ_regroup(theta[i-l:i+l+ 1])
            ang_vel[i] = np.polyfit(np.arange(len(y)), y, 1)[0]

        for i in np.arange(N- l, N):
            y = cs.circ_regroup(theta[i- l:])
            ang_vel[i] = np.polyfit(np.arange(len(y)), y, 1)[0]

        return ang_vel*self.get_sampling_rate()

    def calc_border(self, **kwargs):
        &#34;&#34;&#34;
        Identifies the border of the recording arena from the trace of the foraging of the
        animal in the arena
        
        Parameters
        ----------
        **kwargs
            Keyword arguments
        
        Returns
        -------
        border_dist : ndarray
            Distance of the animal from the border at each behavioural samples
        xedges : ndarray
            Pixelated edge of the x-axis
        yedges : ndarray
            Pixelated edge of the y-axis
        dist_mat : ndarray
            A matrix of distance of each pixel of the arena from the identified
            border
        
        &#34;&#34;&#34;
        
        # define edges
        pixel = kwargs.get(&#39;pixel&#39;, 3)
        chop_bound = kwargs.get(&#39;chop_bound&#39;, 5)

        xedges = np.arange(0, np.ceil(np.max(self._pos_x)), pixel)
        yedges = np.arange(0, np.ceil(np.max(self._pos_y)), pixel)

        tmap, yedges, xedges = histogram2d(self._pos_y, self._pos_x, yedges, xedges)
        if abs(xedges.size- yedges.size) &lt;= chop_bound:
            tmap = chop_edges(tmap, min(tmap.shape), min(tmap.shape))[2]
        else:
            tmap = chop_edges(tmap, tmap.shape[1], tmap.shape[0])[2]

        ybin, xbin = tmap.shape

        border = np.zeros(tmap.shape)
        border[tmap &gt; 0] = False
        border[tmap == 0] = True

        for J in np.arange(ybin):
            for I in np.arange(xbin):
                if not border[J, I] and (J == ybin-1 or J == 0 or I == xbin-1 or I == 0):
                    border[J, I] = True


        # Optimize the border
        optBorder = np.zeros(border.shape)
        for i in np.arange(border.shape[0]):
            for j in np.arange(border.shape[1]):
                if border[i, j]:
                    if i == 0: # along the 1st row
                        if border[i, j] != border[i + 1, j]:
                            optBorder[i, j] = True
                    elif j == 0: # along the 1st column
                        if border[i, j] != border[i, j + 1]:
                            optBorder[i, j] = True
                    elif i == border.shape[0] - 1: # along the last row
                        if border[i, j] != border[i - 1, j]:
                            optBorder[i, j] = True
                    elif j == border.shape[1] - 1:# along the last column
                        if border[i, j] != border[i, j - 1]:
                            optBorder[i, j] = True
                    else: # other cases
                        if (border[i, j] != border[i, j + 1]) or (border[i, j] != border[i + 1, j])\
                                or (border[i, j] != border[i, j - 1]) or (border[i, j] != border[i - 1, j]):
                            optBorder[i, j] = True

        border = optBorder

        xborder = np.zeros(tmap.shape, dtype=bool)
        yborder = np.zeros(tmap.shape, dtype=bool)
        for J in np.arange(ybin):
            xborder[J, find(border[J, :], 1, &#39;first&#39;)] = True # 1 added/subed to the next pixel of the traversed arena as the border
            xborder[J, find(border[J, :], 1, &#39;last&#39;)] = True
        for I in np.arange(xbin):
            yborder[find(border[:, I], 1, &#39;first&#39;), I] = True
            yborder[find(border[:, I], 1, &#39;last&#39;), I] = True

        #        self.border = border
        border = xborder | yborder
        self.tmap = tmap*self._timestamp

        distMat = np.zeros(border.shape)
        xx, yy = np.meshgrid(np.arange(xbin), np.arange(ybin))
        borderDist = np.zeros(self._time.size)

        xedges = np.arange(xbin)*pixel
        yedges = np.arange(ybin)*pixel
        xind = histogram(self._pos_x, xedges)[1]
        yind = histogram(self._pos_y, yedges)[1]

        for J in np.arange(ybin):
            for I in np.arange(xbin):
                dist_arr = (
                    np.abs(xx[border] - xx[J, I]) +
                    np.abs(yy[border] - yy[J, I]))
                if dist_arr.size == 0:
                    logging.error(&#34;could not calculate border&#34;)
                    return None, None, None, None
                tmp_dist = np.min(dist_arr)
                if find(np.logical_and(xind == I, yind == J)).size:
                    borderDist[np.logical_and(xind == I, yind == J)] = tmp_dist
                distMat[J, I] = tmp_dist
        
        dist_mat= distMat*pixel
        border_dist= borderDist*pixel
        
        return border_dist, xedges, yedges, dist_mat

    @staticmethod
    def skaggs_info(firing_rate, visit_time):
        &#34;&#34;&#34;
        Calculates the Skaggs information content of the spatial firing
        
        Parameters
        ----------
        firing_rate : ndarray
            Firing rate of the unit at each pixelated location or binned information,
            i.e., binned speed or head-direction            
        visit_time : ndarray
            Amount of time animal spent in each pixel or bin
        
        Returns
        -------
        float
            Skaggs information content
        
        &#34;&#34;&#34;
        
        firing_rate[np.isnan(firing_rate)] = 0
        Li = firing_rate # Lambda
        L = np.sum(firing_rate*visit_time)/ visit_time.sum()
        P = visit_time/visit_time.sum()
        
        return np.sum(P[Li &gt; 0]*(Li[Li &gt; 0]/L)*np.log2(Li[Li &gt; 0]/L))

    @staticmethod
    def spatial_sparsity(firing_rate, visit_time):
        &#34;&#34;&#34;
        Calculates the spatial sparsity of the spatial firing
        
        Parameters
        ----------
        firing_rate : ndarray
            Firing rate of the unit at each pixelated location  
        visit_time : ndarray
            Amount of time animal spent in each pixel
        
        Returns
        -------
        float
            Spatial sparsity
        
        &#34;&#34;&#34;
        
        firing_rate[np.isnan(firing_rate)] = 0
        Li = firing_rate # Lambda
        # L = np.sum(firing_rate*visit_time)/ visit_time.sum()
        P = visit_time/visit_time.sum()
        return np.sum(P*Li)**2/ np.sum(P*Li**2)

    def speed(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Calculates the firing rate of the unit at different binned speeds.
        
        The spike rate vs speed is fitted with a linear equation and goodness of fit
        is measured
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
        &#34;&#34;&#34;

        _results = oDict()
        graph_data = {}
        update = kwargs.get(&#39;update&#39;, True) # When update = True, it will use the
                                            #results for statistics, if False,
                                            #i.e. in Multiple Regression, it will ignore updating
        binsize = kwargs.get(&#39;binsize&#39;, 1)
        min_speed, max_speed = kwargs.get(&#39;range&#39;, [0, 40])

        speed = self.get_speed()
        max_speed = min(max_speed, np.ceil(speed.max()/binsize)*binsize)
        min_speed = max(min_speed, np.floor(speed.min()/binsize)*binsize)
        bins = np.arange(min_speed, max_speed, binsize)

        vid_count = histogram(ftimes, self.get_time())[0]
        visit_time, speedInd = histogram(speed, bins)[0:2]
        visit_time = visit_time/self.get_sampling_rate()

        rate = np.array([sum(vid_count[speedInd == i]) for i in range(len(bins))])/ visit_time
        rate[np.isnan(rate)] = 0

        _results[&#39;Speed Skaggs&#39;] = self.skaggs_info(rate, visit_time)

        rate = rate[visit_time &gt; 1]
        bins = bins[visit_time &gt; 1]

        fit_result = linfit(bins, rate)

        _results[&#39;Speed Pears R&#39;] = fit_result[&#39;Pearson R&#39;]
        _results[&#39;Speed Pears P&#39;] = fit_result[&#39;Pearson P&#39;]
        graph_data[&#39;bins&#39;] = bins
        graph_data[&#39;rate&#39;] = rate
        graph_data[&#39;fitRate&#39;] = fit_result[&#39;yfit&#39;]

        if update:
            self.update_result(_results)
        return graph_data

    def angular_velocity(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Calculates the firing rate of the unit at different binned angular head velocity.
        
        The spike rate vs speed is fitted with a linear equation individually
        for the negative and positive angular velocities, and goodness of fit
        is measured
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
        &#34;&#34;&#34;
        
        _results = oDict()
        graph_data = {}
        update = kwargs.get(&#39;update&#39;, True) # When update = True, it will use the
                                            #results for statistics, if False,
                                            #i.e. in Multiple Regression, it will ignore updating
        binsize = kwargs.get(&#39;binsize&#39;, 10)
        min_vel, max_vel = kwargs.get(&#39;range&#39;, [-100, 100])
        cutoff = kwargs.get(&#39;cutoff&#39;, 10)

        ang_vel = self.get_ang_vel()

        max_vel = min(max_vel, np.ceil(ang_vel.max()/binsize)*binsize)
        min_vel = max(min_vel, np.floor(ang_vel.min()/binsize)*binsize)
        bins = np.arange(min_vel, max_vel, binsize)

        vid_count = histogram(ftimes, self.get_time())[0]
        visit_time, velInd = histogram(ang_vel, bins)[0:2]
        visit_time = visit_time/self.get_sampling_rate()

        rate = np.array([sum(vid_count[velInd == i]) for i in range(len(bins))])/ visit_time
        rate[np.isnan(rate)] = 0

        _results[&#39;speedSkaggs&#39;] = self.skaggs_info(rate, visit_time)

        rate = rate[visit_time &gt; 1]
        bins = bins[visit_time &gt; 1]


        fit_result = linfit(bins[bins &lt;= -cutoff], rate[bins &lt;= -cutoff])

        _results[&#39;Ang Vel Left Pears R&#39;] = fit_result[&#39;Pearson R&#39;]
        _results[&#39;Ang Vel Left Pears P&#39;] = fit_result[&#39;Pearson P&#39;]
        graph_data[&#39;leftBins&#39;] = bins[bins &lt;= -cutoff]
        graph_data[&#39;leftRate&#39;] = rate[bins &lt;= -cutoff]
        graph_data[&#39;leftFitRate&#39;] = fit_result[&#39;yfit&#39;]

        fit_result = linfit(bins[bins &gt;= cutoff], rate[bins &gt;= cutoff])

        _results[&#39;Ang Vel Right Pears R&#39;] = fit_result[&#39;Pearson R&#39;]
        _results[&#39;Ang Vel Right Pears P&#39;] = fit_result[&#39;Pearson P&#39;]
        graph_data[&#39;rightBins&#39;] = bins[bins &gt;= cutoff]
        graph_data[&#39;rightRate&#39;] = rate[bins &gt;= cutoff]
        graph_data[&#39;rightFitRate&#39;] = fit_result[&#39;yfit&#39;]

        if update:
            self.update_result(_results)
        return graph_data

    def place(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Calculates the two-dimensional firing rate of the unit with respect to
        the location of the animal in the environment. This is called Firing map.
        
        Specificity indices are measured to assess the quality of location-specific firing of the unit.
        
        This method also plot the events of spike occurring superimposed on the
        trace of the animal in the arena, commonly known as Spike Plot.
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
        &#34;&#34;&#34;
        
        _results = oDict()
        graph_data = {}
        update = kwargs.get(&#39;update&#39;, True)
        pixel = kwargs.get(&#39;pixel&#39;, 3)
        chop_bound = kwargs.get(&#39;chop_bound&#39;, 5)
        filttype, filtsize = kwargs.get(&#39;filter&#39;, [&#39;b&#39;, 5])
        lim = kwargs.get(&#39;range&#39;, [0, self.get_duration()])
        brAdjust = kwargs.get(&#39;brAdjust&#39;, True)
        thresh = kwargs.get(&#39;fieldThresh&#39;, 0.2)
        required_neighbours = kwargs.get(&#39;minPlaceFieldNeighbours&#39;, 9)
        smooth_place = kwargs.get(&#39;smoothPlace&#39;, False)
        separate_border_data = kwargs.get(
            &#34;separateBorderData&#34;, False)

        # xedges = np.arange(0, np.ceil(np.max(self._pos_x)), pixel)
        # yedges = np.arange(0, np.ceil(np.max(self._pos_y)), pixel)

        # Update the border to match the requested pixel size
        if separate_border_data:
            self.set_border(
                separate_border_data.calc_border(**kwargs))
            times = self._time
            lower, upper = (times.min(), times.max())
            new_times = separate_border_data._time
            sample_spatial_idx = (
                (new_times &lt;= upper) &amp; (new_times &gt;= lower)).nonzero()
            self._border_dist = self._border_dist[sample_spatial_idx]
        else:  
            self.set_border(self.calc_border(**kwargs))

        xedges = self._xbound
        yedges = self._ybound

        spikeLoc = self.get_event_loc(ftimes, **kwargs)[1]
        posX = self._pos_x[np.logical_and(self.get_time() &gt;= lim[0], self.get_time() &lt;= lim[1])]
        posY = self._pos_y[np.logical_and(self.get_time() &gt;= lim[0], self.get_time() &lt;= lim[1])]

        tmap, yedges, xedges = histogram2d(posY, posX, yedges, xedges)

        if tmap.shape[0] != tmap.shape[1] &amp; np.abs(tmap.shape[0]- tmap.shape[1]) &lt;= chop_bound:
            tmap = chop_edges(tmap, min(tmap.shape), min(tmap.shape))[2]
        tmap /= self.get_sampling_rate()

        ybin, xbin = tmap.shape
        xedges = np.arange(xbin)*pixel
        yedges = np.arange(ybin)*pixel

        spike_count = histogram2d(spikeLoc[1], spikeLoc[0], yedges, xedges)[0]
        fmap = np.divide(spike_count, tmap, out=np.zeros_like(spike_count), where=tmap != 0)

        if brAdjust:
            nfmap = fmap/ fmap.max()
            if np.sum(np.logical_and(nfmap &gt;= 0.2, tmap != 0)) &gt;= 0.8*nfmap[tmap != 0].flatten().shape[0]:
                back_rate = np.mean(fmap[np.logical_and(nfmap &gt;= 0.2, nfmap &lt; 0.4)])
                fmap -= back_rate
                fmap[fmap &lt; 0] = 0

        if filttype is not None:
            smoothMap = smooth_2d(fmap, filttype, filtsize)
        else :
            smoothMap = fmap
        
        if smooth_place:
            pmap = smoothMap
        else:
            pmap = fmap
        
        pmap[tmap == 0] = None
        pfield, largest_group = NSpatial.place_field(
            pmap, thresh, required_neighbours)
        # if largest_group == 0:
        #     if smooth_place:
        #         info = &#34;where the place field was calculated from smoothed data&#34;
        #     else:
        #         info = &#34;where the place field was calculated from raw data&#34;
        #     logging.info(
        #         &#34;Lack of high firing neighbours to identify place field &#34; +
        #         info)
        centroid = NSpatial.place_field_centroid(pfield, pmap, largest_group)
        #centroid is currently in co-ordinates, convert to pixels
        centroid = centroid * pixel + (pixel * 0.5)
        #flip x and y
        centroid = centroid[::-1]
        
        p_shape = pfield.shape
        maxes = [xedges.max(), yedges.max()]
        scales = (
             maxes[0] / p_shape[1],
             maxes[1] / p_shape[0])
        co_ords = np.array(np.where(pfield == largest_group))
        boundary = [None, None]
        for i in range(2):
            j = (i + 1) % 2
            boundary[i] = (
                co_ords[j].min() * scales[i],
                np.clip((co_ords[j].max()+1) * scales[i], 0, maxes[i]))
        inside_x = (
            (boundary[0][0] &lt;= spikeLoc[0]) &amp;
            (spikeLoc[0] &lt;= boundary[0][1]))
        inside_y = (
            (boundary[1][0] &lt;= spikeLoc[1]) &amp;
            (spikeLoc[1] &lt;= boundary[1][1]))
        co_ords = np.nonzero(np.logical_and(inside_x, inside_y))

        if update:
            _results[&#39;Spatial Skaggs&#39;] = self.skaggs_info(fmap, tmap)
            _results[&#39;Spatial Sparsity&#39;] = self.spatial_sparsity(fmap, tmap)
            _results[&#39;Spatial Coherence&#39;] = np.corrcoef(fmap[tmap != 0].flatten(), smoothMap[tmap != 0].flatten())[0, 1]
            _results[&#39;Found strong place field&#39;] = (largest_group != 0)
            _results[&#39;Place field Centroid x&#39;] = centroid[0]
            _results[&#39;Place field Centroid y&#39;] = centroid[1]
            _results[&#39;Place field Boundary x&#39;] = boundary[0]
            _results[&#39;Place field Boundary y&#39;] = boundary[1]
            _results[&#39;Number of Spikes in Place Field&#39;] = co_ords[0].size
            _results[&#39;Percentage of Spikes in Place Field&#39;] = co_ords[0].size*100 / ftimes.size
            self.update_result(_results)

        smoothMap[tmap == 0] = None

        graph_data[&#39;posX&#39;] = posX
        graph_data[&#39;posY&#39;] = posY
        graph_data[&#39;fmap&#39;] = fmap
        graph_data[&#39;smoothMap&#39;] = smoothMap
        graph_data[&#39;firingMap&#39;] = fmap
        graph_data[&#39;tmap&#39;] = tmap
        graph_data[&#39;xedges&#39;] = xedges
        graph_data[&#39;yedges&#39;] = yedges
        graph_data[&#39;spikeLoc&#39;] = spikeLoc
        graph_data[&#39;placeField&#39;] = pfield
        graph_data[&#39;largestPlaceGroup&#39;] = largest_group
        graph_data[&#39;placeBoundary&#39;] = boundary
        graph_data[&#39;indicesInPlaceField&#39;] = co_ords
        graph_data[&#39;centroid&#39;] = centroid

        return graph_data
    
    # Created by Sean Martin: 13/02/2019
    def place_field_centroid_zscore(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        A naive method to find the centroid of the place field using the 
        z-score of the normal distribution to remove outliers, 
        and then averaging remaining locations&#39; co-ordinates.
                
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        **kwargs
            Keyword arguments
 
        Returns
        -------
        ndarray
            The centroid of the place field
        &#34;&#34;&#34;

        _results = oDict()
        update = kwargs.get(&#39;update&#39;, True)
        lim = kwargs.get(&#39;range&#39;, [0, self.get_duration()])
        remove_outliers = kwargs.get(&#39;remove_outliers&#39;, True)
        threshold = kwargs.get(&#39;z_threshold&#39;, 2)

        spikeLoc = self.get_event_loc(ftimes, **kwargs)[1]
        
        if remove_outliers:
            z_scores = sc.stats.zscore(spikeLoc, axis=1)
            # Filter out locations with x or y outside of 3 std devs.
            filter_array = np.logical_and((abs(z_scores[0]) &lt; threshold).astype(bool), (abs(z_scores[1]) &lt; threshold).astype(bool))
            spikeLoc[0] = spikeLoc[0][filter_array]
            spikeLoc[1] = spikeLoc[1][filter_array]

        centroid = np.average(spikeLoc, axis=1)
        if update:
            _results[&#39;Place field Centroid x&#39;] = centroid[0]
            _results[&#39;Place field Centroid y&#39;] = centroid[1]
            self.update_result(_results)
        return centroid

    def non_moving_periods(self, **kwargs):
        &#34;&#34;&#34;
        Returns a number of tuples indicating ranges where the subject is not moving

        kwargs
        ------
        should_smooth : bool
            flags if the speed data should be smoothed, default False
        min_range : float
            the minimum amount of time that the subject should not be moving for
        moving_thresh : float
            any speed above this thresh is considered to be movement
        &#34;&#34;&#34;
        should_smooth = kwargs.get(&#34;should_smooth&#34;, False)
        min_range = kwargs.get(&#34;min_range&#34;, 150)
        moving_thresh = kwargs.get(&#34;moving_thresh&#34;, 2.5)

        if should_smooth:
            self.smooth_speed()
        not_moving = self.get_speed() &lt; moving_thresh

        return find_true_ranges(
            self.get_time(), not_moving, min_range)

    def get_non_moving_times(self, **kwargs):
        &#34;&#34;&#34; Returns the times where the subject is not moving&#34;&#34;&#34;
        ranges = self.non_moving_periods(**kwargs)
        time_data = [
            val for val in self.get_time()
            if any(lower &lt;= val &lt;= upper for (lower, upper) in ranges)
        ]
        return time_data

    def loc_time_lapse(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Calculates the firing rate map and idnetifies the location of the spiking events
        at certain intervals. This method is useful in observing the evolution of
        unit-activity as the animal traverses the environment.
        
        Following intervals ar used:
            0-1min, 0-2min, 0-4min, 0-8min, 0-16min or 0-end depending on the recording duration
            0-1min, 1-2min, 2-4min, 4-8min, 8-16min or 16-end depending on the recording duration
                
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
        &#34;&#34;&#34;
        
        graph_data = oDict()
        pixel = kwargs.get(&#39;pixel&#39;, 3)
        chop_bound = kwargs.get(&#39;chop_bound&#39;, 5)
        filter = kwargs.get(&#39;filter&#39;, [&#39;b&#39;, 5])
        brAdjust = kwargs.get(&#39;brAdjust&#39;, True)

        lim = [0, 1*60]
        graph_data[&#39;0To1min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

        lim = [0, 2*60]
        graph_data[&#39;0To2min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

        lim = [0, 4*60]
        graph_data[&#39;0To4min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

        lim = [0, 8*60]
        graph_data[&#39;0To8min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

        # 0-1min, 1-2min,  2-4min, 4-8min
        lim = [1*60, 2*60]
        graph_data[&#39;1To2min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

        lim = [2*60, 4*60]
        graph_data[&#39;2To4min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

        lim = [4*60, 8*60]
        graph_data[&#39;4To8min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

        ## 0-16min, 8-16min 0-20min, 16-20min

        if self.get_duration() &gt; 8*60:
            if self.get_duration() &gt; 16*60:
                lim = [0, 16*60]
                graph_data[&#39;0To16min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

                lim = [8*60, 16*60]
                graph_data[&#39;8To16min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

                lim = [0, self.get_duration()]
                graph_data[&#39;0ToEnd&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

                lim = [16*60, self.get_duration()]
                graph_data[&#39;16ToEnd&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

            else:
                lim = [0, self.get_duration()]
                graph_data[&#39;0ToEnd&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

                lim = [8*60, self.get_duration()]
                graph_data[&#39;8ToEnd&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

            return graph_data

    def hd_rate(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Calculates the firing rate of the unit with respect to the head-direciton
        of the animal in the environment. This is calle Tuning curve.
        
        Precited firing map from the locational firing is also calculated and
        distributive ratio is measured along with the Skaggs information.
        
        Spike-plot similar to locational firing is developed but in the circular bins
        which shows the direction of the animal&#39;s head at each spike&#39;s occurring time.
                                
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
        &#34;&#34;&#34;
        
        _results = oDict()
        graph_data = {}
        update = kwargs.get(&#39;update&#39;, True)
        binsize = kwargs.get(&#39;binsize&#39;, 5) # in degrees
        filttype, filtsize = kwargs.get(&#39;filter&#39;, [&#39;b&#39;, 5])
        lim = kwargs.get(&#39;range&#39;, [0, self.get_duration()])

        bins = np.arange(0, 360, binsize)

        spike_hd = self.get_event_loc(ftimes, **kwargs)[2]
        direction = self.get_direction()[np.logical_and(self.get_time() &gt;= lim[0], self.get_time() &lt;= lim[1])]

        tcount, ind, bins = histogram(direction, bins)

        tcount = tcount/ self.get_sampling_rate()

        spike_count = histogram(spike_hd, bins)[0].astype(tcount.dtype)

        hd_rate = np.divide(spike_count, tcount, out=np.zeros_like(spike_count), where=tcount != 0, casting=&#39;unsafe&#39;)

        smoothRate = smooth_1d(hd_rate, filttype, filtsize)

        if update:
            _results[&#39;HD Skaggs&#39;] = self.skaggs_info(hd_rate, tcount)
            cs = CircStat(rho=smoothRate, theta=bins)
            results = cs.calc_stat()
            _results[&#39;HD Rayl Z&#39;] = results[&#39;RaylZ&#39;]
            _results[&#39;HD Rayl P&#39;] = results[&#39;RaylP&#39;]
            _results[&#39;HD von Mises K&#39;] = results[&#39;vonMisesK&#39;]
            
            _results[&#39;HD Mean&#39;] = results[&#39;meanTheta&#39;]
            _results[&#39;HD Mean Rate&#39;] = results[&#39;meanRho&#39;]
            _results[&#39;HD Res Vect&#39;] = results[&#39;resultant&#39;]

            binInterp = np.arange(360)
            rateInterp = np.interp(binInterp, bins, hd_rate)

            _results[&#39;HD Peak Rate&#39;] = np.amax(rateInterp)
            _results[&#39;HD Peak&#39;] = binInterp[np.argmax(rateInterp)]

            half_max = np.amin(rateInterp)+ (np.amax(rateInterp)- np.amin(rateInterp))/2
            d = np.sign(half_max - rateInterp[0:-1]) - np.sign(half_max - rateInterp[1:])
            left_possible = find(d &gt; 0)
            if len(left_possible) &gt; 0:
                left_idx = left_possible[0]
            else:
                left_idx = None
            
            right_possible = find(d &lt; 0)
            if len(right_possible) &gt; 0:
                right_idx = right_possible[-1]
            else:
                right_idx = None
            _results[&#39;HD Half Width&#39;] = None if (not left_idx or not right_idx or left_idx &gt; right_idx) \
                                        else binInterp[right_idx]- binInterp[left_idx]

            pixel = kwargs.get(&#39;pixel&#39;, 3)
            placeData = self.place(ftimes, pixel=pixel)
            fmap = placeData[&#39;smoothMap&#39;]
            fmap[np.isnan(fmap)] = 0
            hdPred = np.zeros(bins.size)
            for i, b in enumerate(bins):
                hdInd = np.logical_and(direction &gt;= b, direction &lt; b+ binsize)
                tmap = histogram2d(self.get_pos_y()[hdInd], self.get_pos_x()[hdInd], placeData[&#39;yedges&#39;], placeData[&#39;xedges&#39;])[0]
                tmap /= self.get_sampling_rate()
                hdPred[i] = np.sum(fmap*tmap)/ tmap.sum()

            graph_data[&#39;hdPred&#39;] = smooth_1d(hdPred, &#39;b&#39;, 5)
            self.update_result(_results)

        graph_data[&#39;hd&#39;] = direction
        graph_data[&#39;hdRate&#39;] = hd_rate
        graph_data[&#39;smoothRate&#39;] = smoothRate
        graph_data[&#39;tcount&#39;] = tcount
        graph_data[&#39;bins&#39;] = bins
        graph_data[&#39;spike_hd&#39;] = spike_hd

        cs = CircStat()
        cs.set_theta(spike_hd)
        graph_data[&#39;scatter_radius&#39;], graph_data[&#39;scatter_bins&#39;] = cs.circ_scatter(bins=2, step=0.05)


        return graph_data

    def hd_rate_ccw(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Calculates the tuning curve but split into clock-wise vs counterclockwise
        head-directional movement.
                                
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
        &#34;&#34;&#34;
        
        _results = oDict()
        graph_data = {}
        update = kwargs.get(&#39;update&#39;, True)
        binsize = kwargs.get(&#39;binsize&#39;, 5) # in degrees
        filttype, filtsize = kwargs.get(&#39;filter&#39;, [&#39;b&#39;, 5])
        lim = kwargs.get(&#39;range&#39;, [0, self.get_duration()])
        thresh = kwargs.get(&#39;thresh&#39;, 30)

        edges = np.arange(0, 360, binsize)

        spikeInd, spikeLoc, spike_hd = self.get_event_loc(ftimes, **kwargs)
        vidInd = np.logical_and(self.get_time() &gt;= lim[0], self.get_time() &lt;= lim[1])
        direction = self.get_direction()[vidInd]

        ccwSpike_hd = spike_hd[self.get_ang_vel()[spikeInd] &lt; -thresh]
        cwSpike_hd = spike_hd[self.get_ang_vel()[spikeInd] &gt; thresh]

        ccw_dir = direction[self.get_ang_vel()[vidInd] &lt; -thresh]
        cw_dir = direction[self.get_ang_vel()[vidInd] &gt; thresh]


        binInterp = np.arange(360)

        tcount, ind, bins = histogram(cw_dir, edges)
        tcount = tcount/ self.get_sampling_rate()
        spike_count = histogram(cwSpike_hd, edges)[0].astype(tcount.dtype)
        cwRate = np.divide(spike_count, tcount, out=np.zeros_like(spike_count), where=tcount != 0, casting=&#39;unsafe&#39;)
        cwRate = np.interp(binInterp, bins, smooth_1d(cwRate, filttype, filtsize))

        tcount, ind, bins = histogram(ccw_dir, edges)
        tcount = tcount/ self.get_sampling_rate()
        spike_count = histogram(ccwSpike_hd, edges)[0].astype(tcount.dtype)
        ccwRate = np.divide(spike_count, tcount, out=np.zeros_like(spike_count), where=tcount != 0, casting=&#39;unsafe&#39;)
        ccwRate = np.interp(binInterp, bins, smooth_1d(ccwRate, filttype, filtsize))

        if update:
            _results[&#39;HD Delta&#39;] = binInterp[np.argmax(ccwRate)]- binInterp[np.argmax(cwRate)]
            _results[&#39;HD Peak CW&#39;] = np.argmax(cwRate)
            _results[&#39;HD Peak CCW&#39;] = np.argmax(ccwRate)
            _results[&#39;HD Peak Rate CW&#39;] = np.amax(cwRate)
            _results[&#39;HD Peak Rate CCW&#39;] = np.amax(ccwRate)
            self.update_result(_results)

        graph_data[&#39;bins&#39;] = binInterp
        graph_data[&#39;hdRateCW&#39;] = cwRate
        graph_data[&#39;hdRateCCW&#39;] = ccwRate

        return graph_data

    def hd_time_lapse(self, ftimes):
        &#34;&#34;&#34;
        Calculates the tuning curve and idnetifies the location of the spiking events
        at certain intervals. This method is useful in observing the evolution of
        unit-activity as the animal traverses the environment.
        
        Following intervals ar used:
            0-1min, 0-2min, 0-4min, 0-8min, 0-16min or 0-end depending on the recording duration
            0-1min, 1-2min, 2-4min, 4-8min, 8-16min or 16-end depending on the recording duration
                
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
        &#34;&#34;&#34;
        
        #### Breaking down the spike plot for firing evolution
        # 0-1min,  0-2min, 0-4min, 0-8min
        graph_data = oDict()
        lim = [0, 1*60]
        graph_data[&#39;0To1min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

        lim = [0, 2*60]
        graph_data[&#39;0To2min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

        lim = [0, 4*60]
        graph_data[&#39;0To4min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

        lim = [0, 8*60]
        graph_data[&#39;0To8min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

        # 0-1min, 1-2min,  2-4min, 4-8min
        lim = [1*60, 2*60]
        graph_data[&#39;1To2min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

        lim = [2*60, 4*60]
        graph_data[&#39;2To4min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

        lim = [4*60, 8*60]
        graph_data[&#39;4To8min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

        ## 0-16min, 8-16min 0-20min, 16-20min

        if self.get_duration() &gt; 8*60:
            if self.get_duration() &gt; 16*60:
                lim = [0, 16*60]
                graph_data[&#39;0To16min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

                lim = [8*60, 16*60]
                graph_data[&#39;8To16min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

                lim = [0, self.get_duration()]
                graph_data[&#39;0ToEnd&#39;] = self.hd_rate(ftimes, range=lim, update=False)

                lim = [16*60, self.get_duration()]
                graph_data[&#39;16ToEnd&#39;] = self.hd_rate(ftimes, range=lim, update=False)

            else:
                lim = [0, self.get_duration()]
                graph_data[&#39;0ToEnd&#39;] = self.hd_rate(ftimes, range=lim, update=False)

                lim = [8*60, self.get_duration()]
                graph_data[&#39;8ToEnd&#39;] = self.hd_rate(ftimes, range=lim, update=False)

            return graph_data

    def hd_shuffle(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Shuffling analysis of the unit to see if the head-directional firing specifity
        is by chance or actually correlated to the head-direction of the animal
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;
        
        _results = oDict()
        graph_data = {}
        nshuff = kwargs.get(&#39;nshuff&#39;, 500)
        limit = kwargs.get(&#39;limit&#39;, 0)
        bins = kwargs.get(&#39;bins&#39;, 100)
        # limit = 0 implies enirely random shuffle, limit = &#39;x&#39; implies nshuff number of shuffles in the range [-x x]
        dur = self.get_time()[-1]
        shift = nprand.uniform(low=limit- dur, high=dur- limit, size=nshuff)
        raylZ = np.zeros((nshuff,))
        vonMisesK = np.zeros((nshuff,))
        for i in np.arange(nshuff):
            shift_ftimes = ftimes+ shift[i]
            # Wrapping up the time
            shift_ftimes[shift_ftimes &gt; dur] -= dur
            shift_ftimes[shift_ftimes &lt; 0] += dur

            hdData = self.hd_rate(shift_ftimes, update=False)
            cs = CircStat(rho=hdData[&#39;smoothRate&#39;], theta=hdData[&#39;bins&#39;])
            results = cs.calc_stat()
            raylZ[i] = results[&#39;RaylZ&#39;]
            vonMisesK[i] = results[&#39;vonMisesK&#39;]

        graph_data[&#39;raylZ&#39;] = raylZ
        graph_data[&#39;vonMisesK&#39;] = vonMisesK
        hdData = self.hd_rate(ftimes, update=False)
        cs.set_rho(hdData[&#39;smoothRate&#39;])
        results = cs.calc_stat()
        graph_data[&#39;refRaylZ&#39;] = results[&#39;RaylZ&#39;]
        graph_data[&#39;refVonMisesK&#39;] = results[&#39;vonMisesK&#39;]

        graph_data[&#39;raylZCount&#39;], ind, graph_data[&#39;raylZEdges&#39;] = histogram(raylZ, bins=bins)
        graph_data[&#39;raylZPer95&#39;] = np.percentile(raylZ, 95)

        graph_data[&#39;vonMisesKCount&#39;], ind, graph_data[&#39;vonMisesKEdges&#39;] = histogram(vonMisesK, bins=bins)
        graph_data[&#39;vonMisesKPer95&#39;] = np.percentile(vonMisesK, 95)

        _results[&#39;HD Shuff Rayl Z Per 95&#39;] = np.percentile(raylZ, 95)
        _results[&#39;HD Shuff von Mises K Per 95&#39;] = np.percentile(vonMisesK, 95)
        self.update_result(_results)

        return graph_data

    def hd_shift(self, ftimes, shift_ind=np.arange(-10, 11)):
        &#34;&#34;&#34;
        Analysis of firing specificity of the unit with respect to animal&#39;s head
        direction to oberve whether it represents past direction or anicipates a
        future direction.
                
        Parameters
        ----------
        shift_ind : ndarray
            Index of spatial resolution shift for the spike event time. Shift -1
            implies shift to the past by 1 spatial time resolution, and +2 implies
            shift to the future by 2 spatial time resoultion.
 
        Returns
        -------
        dict
            Graphical data of the analysis
    
        &#34;&#34;&#34;
        
        _results = oDict()
        graph_data = {}
        shift = shift_ind/self.get_sampling_rate()
        shiftlen = shift.size
        dur = self.get_time()[-1]
        delta = np.zeros((shiftlen,))
        skaggs = np.zeros((shiftlen,))
        peakRate = np.zeros((shiftlen,))

        for i in np.arange(shiftlen):
            shift_ftimes = ftimes+ shift[i]
            # Wrapping up the time
            shift_ftimes[shift_ftimes &gt; dur] -= dur
            shift_ftimes[shift_ftimes &lt; 0] += dur

            hdData = self.hd_rate_ccw(shift_ftimes, update=False)
            delta[i] = hdData[&#39;bins&#39;][np.argmax(hdData[&#39;hdRateCCW&#39;])]- hdData[&#39;bins&#39;][np.argmax(hdData[&#39;hdRateCW&#39;])]
            hdData = self.hd_rate(shift_ftimes, update=False)
            peakRate[i] = np.amax(hdData[&#39;smoothRate&#39;])
            skaggs[i] = self.skaggs_info(hdData[&#39;hdRate&#39;], hdData[&#39;tcount&#39;])

        graph_data[&#39;delta&#39;] = delta
        graph_data[&#39;skaggs&#39;] = skaggs
        graph_data[&#39;peakRate&#39;] = peakRate
        graph_data[&#39;shiftTime&#39;] = shift*1000 # changing to milisecond

        # Find out the optimum skaggs location
        shiftUpsamp = np.arange(shift[0], shift[-1], np.mean(np.diff(shift))/10)
        skaggsUpsamp = np.interp(shiftUpsamp, shift, skaggs)
        peakRateUpsamp = np.interp(shiftUpsamp, shift, peakRate)

        dfit_result = linfit(shift, delta)
        deltaFit = dfit_result[&#39;yfit&#39;]
        sortInd = np.argsort(deltaFit)
        _results[&#39;HD ATI&#39;] = np.interp(0, deltaFit[sortInd], shift[sortInd])*1000 if dfit_result[&#39;Pearson R&#39;] &gt;= 0.85 else np.nan

        graph_data[&#39;deltaFit&#39;] = deltaFit
        imax = sg.argrelmax(skaggsUpsamp)[0]
        maxloc = find(skaggsUpsamp[imax] == skaggsUpsamp.max())
        _results[&#39;HD Opt Shift Skaggs&#39;] = np.nan if maxloc.size != 1 else \
                    (np.nan if imax[maxloc] == 0 or imax[maxloc] == skaggsUpsamp.size else shiftUpsamp[imax[maxloc]][0]*1000) # in milisecond

        imax = sg.argrelmax(peakRateUpsamp)[0]
        maxloc = find(peakRateUpsamp[imax] == peakRateUpsamp.max())
        _results[&#39;HD Opt Shift Peak Rate&#39;] = np.nan if maxloc.size != 1 else \
                    (np.nan if imax[maxloc] == 0 or imax[maxloc] == peakRateUpsamp.size else shiftUpsamp[imax[maxloc]][0]*1000) # in milisecond
        self.update_result(_results)

        return graph_data

    @staticmethod
    def place_field(pmap, thresh=0.2, required_neighbours=9):
        &#34;&#34;&#34;
        Calculates a mapping over the captured arena.
        For each bin in the place map, it is assigned to an integer group.
        These groups denote which neighbouring area the bin belongs to.

        Parameters
        ----------
        pmap : ndarray
            The firing map to calculate place fields from
        thresh : float
            The fraction of the peak firing that a bin must exceed
        required_neighbours : int
            The number of adjacent bins that must be together to form a field
        &#34;&#34;&#34;

        def alongColumn(pfield, ptag, J, I):
            &#34;&#34;&#34;
            Iterates along the columns of the ptags to find vertical neighbours

            Parameters
            ----------
            pfield : ndarray
                The place field map, consisting of 
                1 for groups satisying rules and 0 otherwise
            ptag : ndarray
                The place field map, grouped into tags of neighbouring areas
            J : int
                The vertical index to start searching at
            I : int
                The horizontal index to start searching at
            &#34;&#34;&#34;

            Ji = J
            Ii = I
            rows=[]
            while J+1 &lt; ptag.shape[0]:
                if not pfield[J+1, I] or ptag[J+1, I]:
                    break
                else:
                    ptag[J+1, I] = ptag[J, I]
                    rows.append(J+1)
                    J += 1
            J = Ji
            while J-1 &gt;0:
                if not pfield[J-1, I] or ptag[J-1, I]:
                    break
                else:
                    ptag[J-1, I] = ptag[J, I]
                    rows.append(J-1)
                    J-=1
            for J in rows:
                if J != Ji:
                    ptag = alongRows(pfield, ptag, J, Ii)
            return ptag

        def alongRows(pfield, ptag, J, I):
            &#34;&#34;&#34;
            Iterates along the columns of the ptags, finds horizontal neighbours

            Parameters
            ----------
            pfield : ndarray
                The place field map, consisting of 
                1 for groups satisying rules and 0 otherwise
            ptag : ndarray
                The place field map, grouped into tags of neighbouring areas
            J : int
                The vertical index to start searching at
            I : int
                The horizontal index to start searching at
            &#34;&#34;&#34;

            Ii = I
            columns=[]
            while I+1&lt;= ptag.shape[1]:
                if not pfield[J, I+1] or ptag[J, I+1]:
                    break
                else:
                    ptag[J, I+1] = ptag[J, I]
                    columns.append(I+1)
                    I += 1
            I = Ii
            while I-1 &gt;=0:
                if not pfield[J, I-1] or ptag[J, I-1]:
                    break
                else:
                    ptag[J, I-1] = ptag[J, I]
                    columns.append(I-1)
                I-=1
            for I in columns:
                if I!= Ii:
                    ptag = alongColumn(pfield, ptag, J, I)
            return ptag
        
        # Finding the place map firing field:
        # Rules to form a field: 
            # 1. There are sufficient spikes in bin 
            # 2. The bin shares a side with other bins which contain spikes
        
        # Apply Rule 1
        where_are_NaNs = np.isnan(pmap)
        pmap[where_are_NaNs] = 0
        pmap = pmap/pmap.max()
        weights = pmap
        pmap = pmap &gt; thresh

        # Pad the place field with a single layer of zeros to compare neighbours
        pfield = np.zeros(np.add(pmap.shape, 2))
        pfield[1:-1, 1:-1] = pmap

        # Apply rule 2
        has_neighbour_horizontal = np.logical_or(
            pfield[0:-2, 1:-1], pfield[2:, 1:-1])
        has_neighbour_vertical = np.logical_or(
            pfield[1:-1, 0:-2], pfield[1:-1, 2:])

        # Combine rules 1 and 2
        pfield[1:-1, 1:-1] = np.logical_and(
            pmap, 
            np.logical_or(has_neighbour_horizontal, has_neighbour_vertical))
        
        # Initialise all tags to 0
        ptag = np.zeros(pfield.shape, dtype = int)

        # Find the first non zero entry of the pfield
        J, I = find2d(pfield)
        
        #Group all the neighbouring pixels
        group = 1
        for (j, i) in zip(J, I): 
            if ptag[j, i] == 0:
                ptag[j, i] = group
                group = group + 1
                # Tag all neighbours as being of the same group
                ptag = alongColumn(pfield, ptag, j, i)
        
        # Remove the padding
        ptag = ptag[1:-1, 1:-1]

        # Find the largest field, and also remove fields that are too small
        # If there are no large enough fields, label all bins as 0
        uniques, counts = np.unique(ptag[ptag &gt; 0], return_counts=True)
        max_count, largest_group_num = 0, 0
        reduction = 0
        for unique, count in zip(uniques, counts):
            # Don&#39;t consider groups that are small
            unique = unique - reduction
            if count &lt; required_neighbours:
                ptag[ptag == unique] = 0
                ptag[ptag &gt; unique] = ptag[ptag &gt; unique] - 1
                reduction = reduction + 1
            # Define the largest group to be the one with largest weight
            # Could also be the one with the largest area
            else:
                interest_weights = weights[ptag == unique]
                weight = np.sum(interest_weights)
                if weight &gt; max_count:
                    max_count = weight
                    largest_group_num = unique

        return ptag, largest_group_num

    @staticmethod
    def place_field_centroid(pfield, fmap, group_num, **kwargs):
        &#34;&#34;&#34;
        Calculate the centroid of a place field
    
        Parameters
        ----------
        pfield : ndarray
            Input place field consisting of a map of groups
        fmap : ndarray
            Input firing map
        group_num : int
            The group to get the centroid for
        **kwargs :
            Keyword arguments
    
        Returns
        -------
        ndarray
            A list of co-ordinates for each place field group
        &#34;&#34;&#34;
        # For each group, get the list of co-ordinates from the pfield
        co_ords = np.array(np.where(pfield == group_num))
        weights = fmap[co_ords[0], co_ords[1]]
        return centre_of_mass(co_ords, weights, axis=1)
           
      
    def get_event_loc(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Calculates location of the event from its timestamps.
                                
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking or any other events
        **kwargs
            Keyword arguments
 
        Returns
        -------
        ndarray        
            Index of the events in spatial-timestamps
        [ Two item list containing
            ndarray
                x-coordinates of the event location
            ndarray
                y-ccordinates of the event location
        ]
        ndarray
            direction of the animal at the time of the event            
        &#34;&#34;&#34;
        
        
        time = self.get_time()
        lim = kwargs.get(&#39;range&#39;, [0, time.max()])

        # Sean - Why is zero idx is always thrown away?
        keep_zero_idx = kwargs.get(&#39;keep_zero_idx&#39;, False)
        
        hist = histogram(
            ftimes[np.logical_and(
                    ftimes &gt;= lim[0], ftimes &lt; lim[1])], 
            time)
        vidInd = hist[1]

        if keep_zero_idx:
            retInd = vidInd
        else:
            retInd = vidInd[vidInd != 0]
        
        return retInd, [self._pos_x[retInd], self._pos_y[retInd]], self._direction[retInd]

    def loc_shuffle(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Shuffling analysis of the unit to see if the locational firing specifity
        is by chance or actually correlated to the location of the animal
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;
        
        _results = oDict()
        graph_data = {}

        nshuff = kwargs.get(&#39;nshuff&#39;, 500)
        limit = kwargs.get(&#39;limit&#39;, 0)
        bins = kwargs.get(&#39;bins&#39;, 100)
        brAdjust = kwargs.get(&#39;brAdjust&#39;, False)
        pixel = kwargs.get(&#39;pixel&#39;, 3)
        chop_bound = kwargs.get(&#39;chop_bound&#39;, 5)
        filter = kwargs.get(&#39;filter&#39;, [&#39;b&#39;, 5])
        # limit = 0 implies enirely random shuffle, limit = &#39;x&#39; implies nshuff number of shuffles in the range [-x x]
        dur = self.get_time()[-1]
        shift = nprand.uniform(low=limit- dur, high=dur- limit, size=nshuff)
        skaggs = np.zeros((nshuff,))
        sparsity = np.zeros((nshuff,))
        coherence = np.zeros((nshuff,))
        for i in np.arange(nshuff):
            shift_ftimes = ftimes+ shift[i]
            # Wrapping up the time
            shift_ftimes[shift_ftimes &gt; dur] -= dur
            shift_ftimes[shift_ftimes &lt; 0] += dur

            placeData = self.place(shift_ftimes, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)
            skaggs[i] = self.skaggs_info(placeData[&#39;fmap&#39;], placeData[&#39;tmap&#39;])
            sparsity[i] = self.spatial_sparsity(placeData[&#39;fmap&#39;], placeData[&#39;tmap&#39;])
            coherence[i] = np.corrcoef(placeData[&#39;fmap&#39;][placeData[&#39;tmap&#39;] != 0].flatten(), \
                            placeData[&#39;smoothMap&#39;][placeData[&#39;tmap&#39;] != 0].flatten())[0, 1]

        graph_data[&#39;skaggs&#39;] = skaggs
        graph_data[&#39;coherence&#39;] = coherence
        graph_data[&#39;sparsity&#39;] = sparsity

        placeData = self.place(ftimes, pixel=pixel, filter=filter, brAdjust=brAdjust,\
                              chop_bound=chop_bound, update=False)
        graph_data[&#39;refSkaggs&#39;] = self.skaggs_info(placeData[&#39;fmap&#39;], placeData[&#39;tmap&#39;])
        graph_data[&#39;refSparsity&#39;] = self.spatial_sparsity(placeData[&#39;fmap&#39;], placeData[&#39;tmap&#39;])
        graph_data[&#39;refCoherence&#39;] = np.corrcoef(placeData[&#39;fmap&#39;][placeData[&#39;tmap&#39;] != 0].flatten(), \
                            placeData[&#39;smoothMap&#39;][placeData[&#39;tmap&#39;] != 0].flatten())[0, 1]

        graph_data[&#39;skaggsCount&#39;], graph_data[&#39;skaggsEdges&#39;] = np.histogram(skaggs, bins=bins)
        graph_data[&#39;skaggs95&#39;] = np.percentile(skaggs, 95)

        graph_data[&#39;sparsityCount&#39;], graph_data[&#39;sparsityEdges&#39;] = np.histogram(sparsity, bins=bins)
        graph_data[&#39;sparsity05&#39;] = np.percentile(sparsity, 5)

        graph_data[&#39;coherenceCount&#39;], graph_data[&#39;coherenceEdges&#39;] = np.histogram(coherence, bins=bins)
        graph_data[&#39;coherence95&#39;] = np.percentile(coherence, 95)

        _results[&#39;Loc Skaggs 95&#39;] = np.percentile(skaggs, 95)
        _results[&#39;Loc Sparsity 05&#39;] = np.percentile(sparsity, 95)
        _results[&#39;Loc Coherence 95&#39;] = np.percentile(coherence, 95)

        self.update_result(_results)

        return graph_data

    def loc_shift(self, ftimes, shift_ind=np.arange(-10, 11), **kwargs):
        &#34;&#34;&#34;
        Analysis of firing specificity of the unit with respect to animal&#39;s location
        to oberve whether it represents past location of the animal or anicipates a
        future location.
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        shift_ind : ndarray
            Index of spatial resolution shift for the spike event time. Shift -1
            implies shift to the past by 1 spatial time resolution, and +2 implies
            shift to the future by 2 spatial time resoultion.
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;
        
        _results = oDict()
        graph_data = {}

        brAdjust = kwargs.get(&#39;brAdjust&#39;, False)
        pixel = kwargs.get(&#39;pixel&#39;, 3)
        chop_bound = kwargs.get(&#39;chop_bound&#39;, 5)
        _filter = kwargs.get(&#39;filter&#39;, [&#39;b&#39;, 5])

        # limit = 0 implies enirely random shuffle, limit = &#39;x&#39; implies nshuff number of shuffles in the range [-x x]
        shift = shift_ind/self.get_sampling_rate()
        shiftlen = shift.size
        dur = self.get_time()[-1]
        skaggs = np.zeros((shiftlen,))
        sparsity = np.zeros((shiftlen,))
        coherence = np.zeros((shiftlen,))

        for i in np.arange(shiftlen):
            shift_ftimes = ftimes+ shift[i]
            # Wrapping up the time
            shift_ftimes[shift_ftimes &gt; dur] -= dur
            shift_ftimes[shift_ftimes &lt; 0] += dur

            placeData = self.place(shift_ftimes, pixel=pixel, filter=_filter, \
                                  brAdjust=brAdjust, chop_bound=chop_bound, update=False)
            skaggs[i] = self.skaggs_info(placeData[&#39;fmap&#39;], placeData[&#39;tmap&#39;])
            sparsity[i] = self.spatial_sparsity(placeData[&#39;fmap&#39;], placeData[&#39;tmap&#39;])
            coherence[i] = np.corrcoef(placeData[&#39;fmap&#39;][placeData[&#39;tmap&#39;] != 0].flatten(), \
                            placeData[&#39;smoothMap&#39;][placeData[&#39;tmap&#39;] != 0].flatten())[0, 1]

        graph_data[&#39;skaggs&#39;] = skaggs
        graph_data[&#39;sparsity&#39;] = sparsity
        graph_data[&#39;coherence&#39;] = coherence

        graph_data[&#39;shiftTime&#39;] = shift

        # Find out the optimum skaggs location
        shiftUpsamp = np.arange(shift[0], shift[-1], np.mean(np.diff(shift))/4)
        skaggsUpsamp = np.interp(shiftUpsamp, shift, skaggs)
        sparsityUpsamp = np.interp(shiftUpsamp, shift, sparsity)
        coherenceUpsamp = np.interp(shiftUpsamp, shift, coherence)

        imax = sg.argrelmax(skaggsUpsamp)[0]
        maxloc = find(skaggsUpsamp[imax] == skaggsUpsamp.max())
        _results[&#39;Loc Opt Shift Skaggs&#39;] = np.nan if maxloc.size != 1 else (np.nan if imax[maxloc] == 0 or imax[maxloc] == skaggsUpsamp.size else shiftUpsamp[imax[maxloc]])

        imin = sg.argrelmin(sparsityUpsamp)[0]
        minloc = find(sparsityUpsamp[imin] == sparsityUpsamp.min())
        _results[&#39;Loc Opt Shift Sparsity&#39;] = np.nan if minloc.size != 1 else (np.nan if imin[minloc] == 0 or imin[minloc] == sparsityUpsamp.size else shiftUpsamp[imin[minloc]])

        imax = sg.argrelmax(coherenceUpsamp)[0]
        maxloc = find(coherenceUpsamp[imax] == coherenceUpsamp.max())
        _results[&#39;Loc Opt Shift Coherence&#39;] = np.nan if maxloc.size != 1 else (np.nan if imax[maxloc] == 0 or imax[maxloc] == coherenceUpsamp.size else shiftUpsamp[imax[maxloc]])

        self.update_result(_results)

        return graph_data

    def loc_auto_corr(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Calculates the two-dimensional correlation of firing map which is the
        map of the firing rate of the animal with respect to its location
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit        
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;
        graph_data = {}

        minPixel = kwargs.get(&#39;minPixel&#39;, 20)
        pixel = kwargs.get(&#39;pixel&#39;, 3)

        if &#39;update&#39; in kwargs.keys():
            del kwargs[&#39;update&#39;]
        placeData = self.place(ftimes, update=False, **kwargs)

        fmap = placeData[&#39;smoothMap&#39;]
        fmap[np.isnan(fmap)] = 0
        leny, lenx = fmap.shape

        xshift = np.arange(-(lenx-1), lenx)
        yshift = np.arange(-(leny-1), leny)

        corrMap = np.zeros((yshift.size, xshift.size))

        for J, ysh  in enumerate(yshift):
            for I, xsh in enumerate(xshift):
                if ysh &gt;= 0:
                    map1YInd = np.arange(ysh, leny)
                    map2YInd = np.arange(leny - ysh)
                elif ysh &lt; 0:
                    map1YInd = np.arange(leny + ysh)
                    map2YInd = np.arange(-ysh, leny)

                if xsh &gt;= 0:
                    map1XInd = np.arange(xsh, lenx)
                    map2XInd = np.arange(lenx - xsh)
                elif xsh &lt; 0:
                    map1XInd = np.arange(lenx + xsh)
                    map2XInd = np.arange(-xsh, lenx)
                map1 = fmap[tuple(np.meshgrid(map1YInd, map1XInd))]
                map2 = fmap[tuple(np.meshgrid(map2YInd, map2XInd))]
                if map1.size &lt; minPixel:
                    corrMap[J, I] = -1
                else:
                    corrMap[J, I] = corr_coeff(map1, map2)

        graph_data[&#39;corrMap&#39;] = corrMap
        graph_data[&#39;xshift&#39;] = xshift*pixel
        graph_data[&#39;yshift&#39;] = yshift*pixel

        return graph_data

    def loc_rot_corr(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Calculates the rotational correlation of the locational firing rate of the animal with
        respect to location, also called firing map    
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit        
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
    
        &#34;&#34;&#34;    
        
        graph_data = {}

        binsize = kwargs.get(&#39;binsize&#39;, 3) #degrees
#        filttype, filtsize = kwargs.get(&#39;filter&#39;, [&#39;b&#39;, 3])

        bins = np.arange(0, 360, binsize)
        placeData = self.place(ftimes, update=False, **kwargs)

        fmap = placeData[&#39;smoothMap&#39;]
        fmap[np.isnan(fmap)] = 0

        rotCorr = [corr_coeff(rot_2d(fmap, theta), fmap) for k, theta in enumerate(bins)]

        graph_data[&#39;rotAngle&#39;] = bins
        graph_data[&#39;rotCorr&#39;] = rotCorr

        return graph_data

    def border(self, ftimes, **kwargs):        
        &#34;&#34;&#34;
        Analysis of the firing characteristic of a unit with respect to the
        environmental border
                
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit        
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
    
        &#34;&#34;&#34;
                
        _results = oDict()
        graph_data = {}

        dist, xedges, yedges, distMat = self.get_border()
        pixel = np.diff(xedges).mean()

        update = kwargs.get(&#39;update&#39;, True)
        thresh = kwargs.get(&#39;thresh&#39;, 0.2)
        cbinsize = kwargs.get(&#39;cbinsize&#39;, 5) # Circular binsize in degrees
        lim = kwargs.get(&#39;range&#39;, [0, self.get_duration()])

        steps = kwargs.get(&#39;nstep&#39;, 5)

        distBins = np.arange(dist.min(), dist.max() + pixel, pixel)

        if &#39;update&#39; in kwargs.keys():
            del kwargs[&#39;update&#39;]

        placeData = self.place(ftimes, range=lim, update=False, **kwargs)
        fmap = placeData[&#39;smoothMap&#39;]

        xind = np.array([])
        yind = np.array([])
        if placeData[&#39;xedges&#39;].max() &lt; xedges.max():
            xind = xedges &lt;= placeData[&#39;xedges&#39;].max()
            xedges = xedges[xind]
        if placeData[&#39;yedges&#39;].max() &lt; yedges.max():
            yind = yedges &lt;= placeData[&#39;yedges&#39;].max()
            yedges = yedges[xind]

        if xind.any():
            distMat = distMat[:, xind]
        if yind.any():
            distMat = distMat[yind, :]

        nanInd = np.isnan(fmap)
        fmap[nanInd] = 0

        smoothRate = np.zeros(distBins.shape) # Calculated from smooth FR map not by smoothing from raw rate
        for i, edge in enumerate(distBins):
            edge_ind = distMat == edge
            if edge_ind.any() and np.logical_and(np.logical_not(nanInd), distMat == edge).any():
                smoothRate[i] = fmap[np.logical_and(np.logical_not(nanInd), distMat == edge)].mean()
#        smoothRate = smooth_1d(smoothRate, filttype, filtsize)

        fmap /= fmap.max()

        tcount = histogram(dist, distBins)[0]

        tcount = tcount/ self.get_sampling_rate()

        spikeDist = dist[self.get_event_loc(ftimes)[0]]
        spike_count = histogram(spikeDist, distBins)[0].astype(tcount.dtype)

        distRate = np.divide(spike_count, tcount, out=np.zeros_like(spike_count),\
                            where=tcount != 0, casting=&#39;unsafe&#39;) # for skaggs only
        pixelCount = histogram(distMat[np.logical_not(nanInd)], distBins)[0]
        distCount = np.divide(histogram(distMat[fmap &gt;= thresh], distBins)[0], pixelCount, \
                             out=np.zeros_like(distBins), where=pixelCount != 0, casting=&#39;unsafe&#39;)

        circBins = np.arange(0, 360, cbinsize)

        X, Y = np.meshgrid(xedges, np.flipud(yedges))
        X = X- xedges[-1]/2
        Y = Y- yedges[-1]/2
        angDist = np.arctan2(Y, X)* 180/np.pi
        angDist[angDist &lt; 0] += 360

        meanDist = distMat[fmap &gt;= thresh].mean()

        cs = CircStat()
        cs.set_theta(angDist[np.logical_and(distMat &lt;= meanDist, fmap &gt;= thresh)])
        angDistCount = cs.circ_histogram(circBins)[0]

        # Circular linear map
        circLinMap = np.zeros((distBins.size, circBins.size))

        for i, edge in enumerate(distBins):
            cs.set_theta(angDist[np.logical_and(distMat == edge, fmap &gt;= thresh)])
            circLinMap[i, :] = cs.circ_histogram(circBins)[0]

        perSteps = np.arange(0, 1, 1/steps)
        perDist = np.zeros(steps)

        for i in np.arange(steps):
            perDist[i] = distMat[np.logical_and(np.logical_not(nanInd), \
                        np.logical_and(fmap &gt;= perSteps[i], fmap &lt; perSteps[i]+ 1/steps))].mean()
        if update:
            _results[&#39;Border Skaggs&#39;] = self.skaggs_info(distRate, tcount)

            angDistExt = np.append(angDistCount, angDistCount)

            segsize = find_chunk(angDistExt &gt; 0)[0]
            _results[&#39;Border Ang Ext&#39;] = max(segsize)*cbinsize

            cBinsInterp = np.arange(0, 360, 0.1)
            dBinsInterp = np.arange(0, distBins[-1] + pixel, 0.1)
            graph_data[&#39;cBinsInterp&#39;] = cBinsInterp
            graph_data[&#39;dBinsInterp&#39;] = dBinsInterp
            graph_data[&#39;circLinMap&#39;] = sc.interpolate.interp2d(circBins, distBins, circLinMap, kind=&#39;cubic&#39;)(cBinsInterp, dBinsInterp)

            self.update_result(_results)

        graph_data[&#39;distBins&#39;] = distBins
        graph_data[&#39;distCount&#39;] = distCount
        graph_data[&#39;circBins&#39;] = circBins
        graph_data[&#39;angDistCount&#39;] = angDistCount
        graph_data[&#39;distRate&#39;] = distRate
        graph_data[&#39;smoothRate&#39;] = smoothRate
        graph_data[&#39;perSteps&#39;] = perSteps*100
        graph_data[&#39;perDist&#39;] = perDist

        return graph_data

    def gradient(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Analysis of gradient cell, a unit whose firing rate gradually increases 
        as the animal traverses from the border to the cneter of the environment
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit        
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
        &#34;&#34;&#34;
        
        _results = oDict()
        graph_data = {}

        alim = kwargs.get(&#39;alim&#39;, 0.25)
        blim = kwargs.get(&#39;blim&#39;, 0.25)
        clim = kwargs.get(&#39;clim&#39;, 0.5)

        graph_data = self.border(ftimes, **kwargs)

        x = graph_data[&#39;distBins&#39;]
        y = graph_data[&#39;smoothRate&#39;]
        x = x[np.isfinite(y)]
        y = y[np.isfinite(y)]
        y = np.log(y, out=np.zeros_like(y), where=y != 0, casting=&#39;unsafe&#39;)
        ai = y.max()
        y0 = y[x == 0]
        bi = ai- y0

        d_half = x.mean()
        for i, dist in enumerate(x):
            if i &lt; x.size-1 and (y0+ bi/2) &gt; y[i] and (y0+ bi/2) &lt;= y[i+1]:
                d_half = x[i:i+2].mean()

        ci = np.log(2)/d_half

        def fit_func(x, a, b, c):
            return a- b*np.exp(-c*x)

        popt, pcov = curve_fit(fit_func, x, y, \
                                p0=[ai, bi, ci], \
                                bounds=([(1- alim)*ai, (1- blim)*bi, (1- clim)*ci], \
                                [(1+ alim)*ai, (1+ blim)*bi, (1+ clim)*ci]), \
                                max_nfev=100000)
        a, b, c = popt

        y_fit = fit_func(x, *popt)


        gof = residual_stat(y, y_fit, 3)
        rateFit = np.exp(y_fit)

        graph_data[&#39;distBins&#39;] = x
#        graph_data[&#39;smoothRate&#39;] = y
        graph_data[&#39;rateFit&#39;] = rateFit
        graph_data[&#39;diffRate&#39;] = b*c*np.multiply(rateFit, np.exp(-c*x))

        _results[&#39;Grad Pearse R&#39;] = gof[&#39;Pearson R&#39;]
        _results[&#39;Grad Pearse P&#39;] = gof[&#39;Pearson P&#39;]
        _results[&#39;Grad adj Rsq&#39;] = gof[&#39;adj Rsq&#39;]
        _results[&#39;Grad Max Growth Rate&#39;] = c*np.exp(a-1)
        _results[&#39;Grad Inflect Dist&#39;] = np.log(b)/c

        self.update_result(_results)
        return graph_data

    def grid(self, ftimes, **kwargs):        
        &#34;&#34;&#34;
        Analysis of Grid cells characterised by formation of grid-like pattern
        of high activity in the firing-rate map        
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit   
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
    
        &#34;&#34;&#34;
        
        _results = oDict()
        tol = kwargs.get(&#39;angtol&#39;, 2)
        binsize = kwargs.get(&#39;binsize&#39;, 3)
        bins = np.arange(0, 360, binsize)

        graph_data = self.loc_auto_corr(ftimes, update=False, **kwargs)
        corrMap = graph_data[&#39;corrMap&#39;]
        corrMap[np.isnan(corrMap)] = 0
        xshift = graph_data[&#39;xshift&#39;]
        yshift = graph_data[&#39;yshift&#39;]

        pixel = np.int(np.diff(xshift).mean())

        ny, nx = corrMap.shape
        rpeaks = np.zeros(corrMap.shape, dtype=bool)
        cpeaks = np.zeros(corrMap.shape, dtype=bool)
        for j in np.arange(ny):
            rpeaks[j, extrema(corrMap[j, :])[1]] = True
        for i in np.arange(nx):
            cpeaks[extrema(corrMap[:, i])[1], i] = True
        ymax, xmax = find2d(np.logical_and(rpeaks, cpeaks))

        peakDist = np.sqrt((ymax- find(yshift == 0))**2+ (xmax- find(xshift == 0))**2)
        sortInd = np.argsort(peakDist)
        ymax, xmax, peakDist = ymax[sortInd], xmax[sortInd], peakDist[sortInd]

        ymax, xmax, peakDist = (ymax[1:7], xmax[1:7], peakDist[1:7]) if ymax.size &gt;= 7 else ([], [], [])
        theta = np.arctan2(yshift[ymax], xshift[xmax])*180/np.pi
        theta[theta &lt; 0] += 360
        sortInd = np.argsort(theta)
        ymax, xmax, peakDist, theta = (ymax[sortInd], xmax[sortInd], peakDist[sortInd], theta[sortInd])

        graph_data[&#39;ymax&#39;] = yshift[ymax]
        graph_data[&#39;xmax&#39;] = xshift[xmax]

        meanDist = peakDist.mean()
        X, Y = np.meshgrid(xshift, yshift)
        distMat = np.sqrt(X**2 + Y**2)/pixel

        if len(ymax) == np.logical_and(peakDist &gt; 0.75*meanDist, peakDist &lt; 1.25*meanDist).sum(): # if all of them are within tolerance(25%)
            maskInd = np.logical_and(distMat &gt; 0.5*meanDist, distMat &lt; 1.5*meanDist)
            rotCorr = np.array([corr_coeff(rot_2d(corrMap, theta)[maskInd], corrMap[maskInd]) for k, theta in enumerate(bins)])
            ramax, rimax, ramin, rimin = extrema(rotCorr)
            mThetaPk, mThetaTr = (np.diff(bins[rimax]).mean(), np.diff(bins[rimin]).mean()) if rimax.size and rimin.size else (None, None)
            graph_data[&#39;rimax&#39;] = rimax
            graph_data[&#39;rimin&#39;] = rimin
            graph_data[&#39;anglemax&#39;] = bins[rimax]
            graph_data[&#39;anglemin&#39;] = bins[rimin]
            graph_data[&#39;rotAngle&#39;] = bins
            graph_data[&#39;rotCorr&#39;] = rotCorr

            if mThetaPk is not None and mThetaTr is not None:
                isGrid = True if 60 - tol &lt; mThetaPk &lt; 60 + tol and 60 - tol &lt; mThetaTr &lt; 60 + tol else False
            else:
                isGrid = False

            meanAlpha = np.diff(theta).mean()
            psi = theta[np.array([2, 3, 4, 5, 0, 1])]- theta
            psi[psi &lt; 0] += 360
            meanPsi = psi.mean()

            _results[&#39;Is Grid&#39;] = isGrid and 120 - tol &lt; meanPsi &lt; 120 + tol and 60 - tol &lt; meanAlpha &lt; 60 + tol
            _results[&#39;Grid Mean Alpha&#39;] = meanAlpha
            _results[&#39;Grid Mean Psi&#39;] = meanPsi
            _results[&#39;Grid Spacing&#39;] = meanDist*pixel
            _results[&#39;Grid Score&#39;] = rotCorr[rimax].max()- rotCorr[rimin].min() # Difference between highest Pearson R at peaks and lowest at troughs
            _results[&#39;Grid Orientation&#39;] = theta[0]

        else:
            _results[&#39;Is Grid&#39;] = False

        self.update_result(_results)
        return graph_data

    def multiple_regression(self, ftimes, **kwargs):        
        &#34;&#34;&#34;
        Multiple-rgression analysis where firing rate for each variable, namely
        location, head-direction, speed, AHV, and distance from border, are used
        to regress the instantaneous firing rate of the unit.
                
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit   
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;
        
        _results = oDict()
        graph_data = oDict()
        subsampInterv = kwargs.get(&#39;subsampInterv&#39;, 0.1)
        episode = kwargs.get(&#39;episode&#39;, 120)
        nrep = kwargs.get(&#39;nrep&#39;, 1000)
        sampRate = 1/subsampInterv
        stamp = subsampInterv
 
        a_size = np.round(self.get_duration(), 4)
        time = np.linspace(
            0, a_size, endpoint=True,
            num=np.round(a_size/stamp)+1)
        time = np.round(time, 4)
        Y = histogram(ftimes, time)[0]* sampRate # Instant firing rate

        nt = time.size
        xloc, yloc, loc, hd, speed, ang_vel, distBorder = list(np.zeros((7, nt)))
        tmp = self.place(ftimes)
        placeRate, xedges, yedges = (tmp[&#39;smoothMap&#39;], tmp[&#39;xedges&#39;], tmp[&#39;yedges&#39;])
        placeRate[np.isnan(placeRate)] = 0
        for i in np.arange(nt):
            ind = find(np.logical_and(self.get_time() &gt;= time[i], self.get_time() &lt; time[i]+ stamp))
            if len(ind) == 0:
                continue
            xloc[i] = np.median(self.get_pos_x()[ind])
            yloc[i] = np.median(self.get_pos_y()[ind])
            if histogram(yloc[i], yedges)[1] &lt; yedges.size and histogram(xloc[i], xedges)[1] &lt; xedges.size:
                loc[i] = placeRate[histogram(yloc[i], yedges)[1], histogram(xloc[i], xedges)[1]]
            hd[i] = np.median(self.get_direction()[ind])
            speed[i] = np.median(self.get_speed()[ind])
            ang_vel[i] = np.median(self.get_ang_vel()[ind])
            distBorder[i] = np.median(self.get_border()[0][ind])

        tmp = self.hd_rate(ftimes, update=False)
        hd_rate, hdBins = (tmp[&#39;hdRate&#39;], tmp[&#39;bins&#39;])
        cs = CircStat()
        cs.set_theta(hd)
        hd = hd_rate[cs.circ_histogram(hdBins)[1]] # replaced by corresponding rate
        # Speed+ ang_vel will be linearly modelled, so no transformation required; ang_vel will be replaced by the non-linear rate
        tmp = self.border(ftimes, update=False)
        borderRate, borderBins = (tmp[&#39;distRate&#39;], tmp[&#39;distBins&#39;])
        distBorder = borderRate[histogram(distBorder, borderBins)[1]] # replaced by corresponding rate

        ns = int(episode/stamp) # row to select in random

        X = np.vstack((loc, hd, speed, ang_vel, distBorder)).transpose()
        lm = LinearRegression(fit_intercept=True, normalize=True)

        Rsq = np.zeros((nrep, 6))
        for i in np.arange(nrep):
            ind = np.random.permutation(time.size)[:ns]
            lm.fit(X[ind, :], Y[ind])
            Rsq[i, 0] = lm.score(X[ind, :], Y[ind])
            for j in np.arange(5):
                varind = np.array([k for k in range(5) if k != j])
                lm.fit(X[np.ix_(ind, varind)], Y[ind]) #np.ix_ is used for braodcasting the index arrays
                Rsq[i, j+1] = Rsq[i, 0]- lm.score(X[np.ix_(ind, varind)], Y[ind])

        meanRsq = Rsq.mean(axis=0)
        # Regresssion parameters are alays stored in following order
        varOrder = [&#39;Total&#39;, &#39;Loc&#39;, &#39;HD&#39;, &#39;Speed&#39;, &#39;Ang Vel&#39;, &#39;Dist Border&#39;]

#        graph_data[&#39;order&#39;] = varOrder
        graph_data[&#39;Rsq&#39;] = Rsq
        graph_data[&#39;meanRsq&#39;] = meanRsq
        graph_data[&#39;maxRsq&#39;] = Rsq.max(axis=0)
        graph_data[&#39;minRsq&#39;] = Rsq.min(axis=0)
        graph_data[&#39;stdRsq&#39;] = Rsq.std(axis=0)
        
        _results[&#39;Mult Rsq&#39;] = meanRsq[0]
        for i, key in enumerate(varOrder):
            if i &gt; 0:
                _results[&#39;Semi Rsq &#39;+key] = meanRsq[i]
        self.update_result(_results)

        return graph_data

    def interdependence(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Interdependence analysis where firing rate of each variable is predicted
        from another variable and the distributive ratio is measured between the
        predicted firing rate and the caclulated firing rate.
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        **kwargs
            Keyword arguments
 
        Returns
        -------
        None
        
        &#34;&#34;&#34;        

        _results = oDict()
        pixel = kwargs.get(&#39;pixel&#39;, 3)
        hdbinsize = kwargs.get(&#39;hdbinsize&#39;, 5)
        spbinsize = kwargs.get(&#39;spbinsize&#39;, 1)
        sprange = kwargs.get(&#39;sprange&#39;, [0, 40])
        abinsize = kwargs.get(&#39;abinsize&#39;, 10)
        ang_velrange = kwargs.get(&#39;ang_velrange&#39;, [-500, 500])

        placeData = self.place(ftimes, pixel=pixel, update=False)
        fmap = placeData[&#39;smoothMap&#39;]
        fmap[np.isnan(fmap)] = 0
        xloc = self.get_pos_x()
        yloc = self.get_pos_y()
        xedges = placeData[&#39;xedges&#39;]
        yedges = placeData[&#39;yedges&#39;]

        hdData = self.hd_rate(ftimes, binsize=hdbinsize, update=False)
        bins = hdData[&#39;bins&#39;]
        predRate = np.zeros(bins.size)
        for i, b in enumerate(bins):
            ind = np.logical_and(hdData[&#39;hd&#39;] &gt;= b, hdData[&#39;hd&#39;] &lt; b + hdbinsize)
            tmap = histogram2d(yloc[ind], xloc[ind], yedges, xedges)[0]
            tmap /= self.get_sampling_rate()
            predRate[i] = np.sum(fmap*tmap)/ tmap.sum()
        _results[&#39;DR HP&#39;] = np.abs(np.log((1 + hdData[&#39;smoothRate&#39;])/ (1 + predRate))).sum()/bins.size

        spData = self.speed(ftimes, binsize=spbinsize, range=sprange, update=False)
        bins = spData[&#39;bins&#39;]
        predRate = np.zeros(bins.size)
        speed = self.get_speed()
        for i, b in enumerate(bins):
            ind = np.logical_and(speed &gt;= b, speed &lt; b + spbinsize)
            tmap = histogram2d(yloc[ind], xloc[ind], yedges, xedges)[0]
            tmap /= self.get_sampling_rate()
            predRate[i] = np.sum(fmap*tmap)/ tmap.sum()
        _results[&#39;DR SP&#39;] = np.abs(np.log((1 + spData[&#39;rate&#39;])/ (1 + predRate))).sum()/bins.size

        ang_velData = self.angular_velocity(ftimes, binsize=abinsize, range=ang_velrange, update=False)
        bins = np.hstack((ang_velData[&#39;leftBins&#39;], ang_velData[&#39;rightBins&#39;]))
        predRate = np.zeros(bins.size)
        ang_vel = self.get_ang_vel()
        for i, b in enumerate(bins):
            ind = np.logical_and(ang_vel &gt;= b, ang_vel &lt; b + abinsize)
            tmap = histogram2d(yloc[ind], xloc[ind], yedges, xedges)[0]
            tmap /= self.get_sampling_rate()
            predRate[i] = np.sum(fmap*tmap)/ tmap.sum()
        ang_velObs = np.hstack((ang_velData[&#39;leftRate&#39;], ang_velData[&#39;rightRate&#39;]))
        _results[&#39;DR AP&#39;] = np.abs(np.log((1 + ang_velObs)/ (1 + predRate))).sum()/bins.size

        borderData = self.border(ftimes, update=False)
        bins = borderData[&#39;distBins&#39;]
        dbinsize = np.diff(bins).mean()
        predRate = np.zeros(bins.size)
        border = self.get_border()[0]
        for i, b in enumerate(bins):
            ind = np.logical_and(border &gt;= b, border &lt; b + dbinsize)
            tmap = histogram2d(yloc[ind], xloc[ind], yedges, xedges)[0]
            tmap /= self.get_sampling_rate()
            predRate[i] = np.sum(fmap*tmap)/ tmap.sum()
        _results[&#39;DR BP&#39;] = np.abs(np.log((1 + borderData[&#39;distRate&#39;]) / (1 + predRate))).sum()/bins.size

        self.update_result(_results)

#    def __getattr__(self, arg):
#        if hasattr(self.spike, arg):
#            return getattr(self.spike, arg)
#        elif hasattr(self.lfp, arg):
#            return getattr(self.lfp, arg)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="neurochat.nc_spatial.NSpatial"><code class="flex name class">
<span>class <span class="ident">NSpatial</span></span>
<span>(</span><span>**kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>This data class is the placeholder for the dataset that contains information
about the spatial behaviour of the animal. It decodes data from different
formats and analyses the correlation of spatial information with the spiking
activity of a unit.</p>
<p>Instantiate the <code>NAbstract</code> class</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class NSpatial(NAbstract):
    &#34;&#34;&#34;
    This data class is the placeholder for the dataset that contains information
    about the spatial behaviour of the animal. It decodes data from different 
    formats and analyses the correlation of spatial information with the spiking
    activity of a unit.
    &#34;&#34;&#34;    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._time = []
        self._timestamp = []
        self._total_samples = []
        self._fs = []
        self._pixel_size = 3
        self._pos_x = []
        self._pos_y = []
        self._direction = []
        self._speed = []
        self._ang_vel = []
        self._border_dist = []
        self._xbound = []
        self._ybound = []
        self._dist_map = []
        self.spike = []
        self.lfp = []
        
        self.__type = &#39;spatial&#39;
        
    def get_type(self):
        &#34;&#34;&#34;
        Returns the type of object. For NSpatial, this is always `spatial` type
        
        Parameters
        ----------
        None
        
        Returns
        -------
        str

        &#34;&#34;&#34; 
        
        return self.__type        

    def subsample(self, sample_range=None):
        &#34;&#34;&#34;
        Extract a time range from the positions.

        NOTE for now, the duration will be longer than sample time.
        Duration is actually from 0 to max recording length.
        This is to easier match ndata which assumes recordings start at 0.
        
        Parameters
        ----------
        sample_range : tuple
            the time in seconds to extract from the positions.
        
        Returns
        -------
        NSpike
            subsampled version of initial spatial object
        &#34;&#34;&#34;
        if sample_range is None:
            return self
        new_spatial = deepcopy(self)
        lower, upper = sample_range
        times = self._time
        sample_spatial_idx = (
            (times &lt;= upper) &amp; (times &gt;= lower)).nonzero()
        new_spatial._set_time(self._time[sample_spatial_idx])
        new_spatial._set_pos_x(self._pos_x[sample_spatial_idx])
        new_spatial._set_pos_y(self._pos_y[sample_spatial_idx])
        new_spatial._set_direction(self._direction[sample_spatial_idx])
        new_spatial._set_speed(self._speed[sample_spatial_idx])
        new_spatial.set_ang_vel(self._ang_vel[sample_spatial_idx])
        # NOTE can use to set proper duration
        #new_spatial._set_duration(upper-lower)
        return new_spatial

    def set_pixel_size(self, pixel_size):
        &#34;&#34;&#34;
        Sets the size of pixel size by which the entire foraged arena is tessellated
        
        Parameters
        ----------
        pixel_size : int
            Pixel size of the foraged arena
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        self._pixel_size = pixel_size
        
    def _set_time(self, time):
        &#34;&#34;&#34;
        Sets the time for all the spatial samples. It also resets the `timestamp`
        or the the temporal resolution of the spatial samples and recalculates
        the sampling rate.
        
        Parameters
        ----------
        time : list or ndarray
            Timestamps of all spatial samples
            
        Returns
        -------
        None

        &#34;&#34;&#34;
  
        
        self._time = time
        self._set_timestamp()
        self._set_sampling_rate()
        
    def _set_timestamp(self, timestamp=None):
        &#34;&#34;&#34;
        Sets the timestamps for the spatial information. Here, it is defined as
        the temporal resolution of the spatial samples, not the happening time of
        each sample. This way it is different from NLfp and NSpike timestamp 
        definition
        
        Parameters
        ----------
        timestamp : list or ndarray
            Timestamps of all spiking waveforms
            
        Returns
        -------
        None

        &#34;&#34;&#34;

        
        if timestamp:
            self._timestamp = timestamp
        elif np.array(self._time).any():
            self._timestamp = np.diff(np.array(self._time)).mean()
            
    def _set_sampling_rate(self, sampling_rate=None):
        &#34;&#34;&#34;
        Sets the sampling rate of the spatial information
                
        Parameters
        ----------
        sampling_rate : int
            Sampling rate of the spatial information
 
        Returns
        -------
        None    

        &#34;&#34;&#34;
        
        if sampling_rate:
            self._fs = sampling_rate
        elif np.array(self._time).any():
            self._fs = 1/np.diff(np.array(self._time)).mean()

    def _set_pos_x(self, pos_x):
        &#34;&#34;&#34;
        Sets the X-coordinate of the location of the animal
                
        Parameters
        ----------
        pos_x : ndarray
            X-ccordinate of the location of the animal
 
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        self._pos_x = pos_x
        
    def _set_pos_y(self, pos_y):
        &#34;&#34;&#34;
        Sets the Y-coordinate of the location of the animal
                
        Parameters
        ----------
        pos_y : ndarray
            Y-ccordinate of the location of the animal
 
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        self._pos_y = pos_y
        
    def _set_direction(self, direction):
        &#34;&#34;&#34;
        Sets the head-direction of the animal
                
        Parameters
        ----------
        direction : ndarray
            Head-direction of the animal
 
        Returns
        -------
        None

        &#34;&#34;&#34;
        self._direction = direction
        
    def _set_speed(self, speed):
        &#34;&#34;&#34;
        Sets the speed of the animal
                
        Parameters
        ----------
        speed : ndarray
            Speed of the animal
 
        Returns
        -------
        None

        &#34;&#34;&#34;
        self._speed = speed
        
    def set_ang_vel(self, ang_vel):
        &#34;&#34;&#34;
        Sets the angular head velocity (AHV) of the animal
                
        Parameters
        ----------
        ang_vel : ndarray
            Angular head velocity (AHV) of the animal
 
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        self._ang_vel = ang_vel
        
    def set_border(self, border):
        &#34;&#34;&#34;
        Sets the distance of the animal from the arena border
                
        Parameters
        ----------
        border : ndarray
            Distance of the animal from the arena border
 
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        self._border_dist = border[0]
        self._xbound = border[1]
        self._ybound = border[2]
        self._dist_map = border[3]

    def get_total_samples(self):
        &#34;&#34;&#34;
        Returns the number of spatial samples
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Total spatial samples

        &#34;&#34;&#34;        
        return self._time.size
    
    def get_sampling_rate(self):
        &#34;&#34;&#34;
        Returns the sampling rate of the spatial samples
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Spatial data sampling rate

        &#34;&#34;&#34;
    
        return self._fs
    
    def get_duration(self):
        &#34;&#34;&#34;
        Returns the duration of the experiment
                
        Parameters
        ----------
        None
 
        Returns
        -------
        float
            Duration of the experiment

        &#34;&#34;&#34;
        if len(self._time) == 0:
            return 0
        return self._time[-1]
        
    
    def get_pixel_size(self):
        &#34;&#34;&#34;
        Returns the pixel size of the recorded arena
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Pixel size

        &#34;&#34;&#34;    
        return self._pixel_size
    
    def get_time(self):
        &#34;&#34;&#34;
        Returns the time of individual spatial samples
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Total spatial samples

        &#34;&#34;&#34;
    
        return self._time
    
    def get_timestamp(self):
        &#34;&#34;&#34;
        Returns the temporal resolution of spatial samples
                
        Parameters
        ----------
        None
 
        Returns
        -------
        int
            Temporal resolution of spatial samples

        &#34;&#34;&#34;
        
        return self._timestamp
    
    def get_pos_x(self):
        &#34;&#34;&#34;
        Returns the X-ccordinates of animal&#39;s location
                
        Parameters
        ----------
        None
 
        Returns
        -------
        ndarray
            X-coordinates of animal&#39;s location

        &#34;&#34;&#34;
    
        return self._pos_x
    
    def get_pos_y(self):
        &#34;&#34;&#34;
        Returns the Y-ccordinates of animal&#39;s location
                
        Parameters
        ----------
        None
 
        Returns
        -------
        ndarray
            Y-coordinates of animal&#39;s location

        &#34;&#34;&#34;
    
        return self._pos_y
    
    def get_direction(self):
        &#34;&#34;&#34;
        Returns head direction of the animal
                
        Parameters
        ----------
        None
 
        Returns
        -------
        ndarray
            Head direction of the animal

        &#34;&#34;&#34;
        
        return self._direction
    
    def get_speed(self):
        &#34;&#34;&#34;
        Returns speed of the animal
                
        Parameters
        ----------
        None
 
        Returns
        -------
        ndarray
            Speed of the animal

        &#34;&#34;&#34;       
        return self._speed
    
    def get_ang_vel(self):
        &#34;&#34;&#34;
        Returns angular head velocity of the animal
                
        Parameters
        ----------
        None
 
        Returns
        -------
        ndarray
            Angular head velocity of the animal

        &#34;&#34;&#34;
        
        return self._ang_vel
    
    def get_border(self):
        &#34;&#34;&#34;
        Returns animal&#39;s distance from the border
                
        Parameters
        ----------
        None
 
        Returns
        -------
        ndarray
            Animal&#39;s distance from the border

        &#34;&#34;&#34;
        
        return self._border_dist, self._xbound, self._ybound, self._dist_map

    def set_spike(self, spike, **kwargs):
        &#34;&#34;&#34;
        Adds the NSpike object to NSpatial object 
                
        Parameters
        ----------
        spike : NSpike
            NSpike object to be added to the NSpatial object. If no spike object
            is provided, a new NSpike() object is created.
        **kwargs
            Keyword argumemts for creating the new NSpike instance
 
        Returns
        -------
        None

        &#34;&#34;&#34;

        
        if spike is isinstance(spike, NSpike):
            self.spike = spike
        else:
            cls = NSpike if not spike else spike
            spike = cls(**kwargs)
        self.spike = spike

    def set_lfp(self, lfp, **kwargs):
        &#34;&#34;&#34;
        Adds the NLfp object to NSpatial object 
                
        Parameters
        ----------
        lfp : NLfp
            NLfp object to be added to the NSpatial object. If no spike object
            is provided, a new NLfp() object is created.
        **kwargs
            Keyword argumemts for creating the new NLfp instance
 
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if lfp is isinstance(lfp, NLfp):
            self.lfp = lfp
        else:
            cls = NLfp if not lfp else lfp
            lfp = cls(**kwargs)
        self.lfp = lfp

    def set_spike_name(self, name=None):
        &#34;&#34;&#34;
        Sets the name of the spike dataset
        
        Parameters
        ----------
        name : str
            Name of the spike dataset
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        if name is not None:
            self.spike.set_name(name)
            
    def set_spike_filename(self, filename=None):
        &#34;&#34;&#34;
        Sets file name of the spike dataset
        
        Parameters
        ----------
        name : str
            Full file directory of the spike dataset
        
        Returns
        -------
        None
        &#34;&#34;&#34;
        
        if filename is not None:
            self.spike.set_filename()

    def set_lfp_name(self, name=None):
        &#34;&#34;&#34;
        Sets the name of the lfp dataset
        
        Parameters
        ----------
        name : str
            Name of the lfp dataset
        
        Returns
        -------
        None

        &#34;&#34;&#34;
        
        self.lfp.set_name(name)
        
    def set_lfp_filename(self, filename=None):
        &#34;&#34;&#34;
        Sets file name of the lfp dataset
        
        Parameters
        ----------
        name : str
            Full file directory of the lfp dataset
        
        Returns
        -------
        None
        &#34;&#34;&#34;
        self.lfp.set_filename(filename)

    def set_event(self, event, **kwargs):
        &#34;&#34;&#34;
        Sets the NEvent() object to NSpatial().         
        
        Parameters
        ----------
        event
            NEvent or its childclass or NEvent() object
        
        Returns
        -------
        NEvent()
        
        &#34;&#34;&#34;
        
        if event is isinstance(event, NEvent):
            self.event = event
        else:
            cls = NEvent if not event else event
            event = cls(**kwargs)
        self.event = event

    def set_event_name(self, name=None):
        &#34;&#34;&#34;
        Sets the name of the event object.         
        
        Parameters
        ----------
        name : str
            Name of the vent dataset
        
        Returns
        -------
        None
        
        &#34;&#34;&#34;
        
        self.event.set_name(name)
        
    def set_event_filename(self, filename=None):
        &#34;&#34;&#34;
        Sets the filename for the event
        
        Parameters
        ----------
        filename : str
            Full file of the event dataset
        
        Returns
        -------
        None
        
        &#34;&#34;&#34;
        
        self.event.set_filename(filename)

    def set_system(self, system=None):
        &#34;&#34;&#34;
        Sets the data format or recording system.
        
        Parameters
        ----------
        system : str
            Data format or recording system
        
        Returns
        -------
        None
        
        &#34;&#34;&#34;
        
        if system is not None:
            self._system = system

            if self.spike:
                self.spike.set_system(system)
            if self.lfp:
                self.lfp.set_system(system)

    def load_spike(self):
        &#34;&#34;&#34;
        Loads the composing spike object         
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None        
        
        &#34;&#34;&#34;
        
        self.spike.load()
        
    def load_lfp(self):
        &#34;&#34;&#34;
        Loads the composite lfp object         
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None        
        
        &#34;&#34;&#34;

        
        self.lfp.load()

    def save_to_hdf5(self, file_name=None, system=None):
        &#34;&#34;&#34;
        Save spatial dataset to HDF5 file         
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None        
        
        &#34;&#34;&#34;

        hdf = Nhdf()
        if file_name and system:
            if os.path.exists(file_name):
                self.set_filename(file_name)
                self.set_system(system)
                self.load()
            else:
                logging.error(&#39;Specified file cannot be found!&#39;)

        hdf.save_spatial(spatial=self)
        hdf.close()

    def load(self, filename=None, system=None):
        &#34;&#34;&#34;
        Loads the spatial object         
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None        
        
        &#34;&#34;&#34;

        if system is None:
            system = self._system
        else:
            self._system = system
        if filename is None:
            filename = self._filename
        else:
            filename = self._filename
        loader = getattr(self, &#39;load_spatial_&#39;+ system)
        loader(filename)
        try:
            self.smooth_speed()
        except:
            logging.warning(self.get_system() + &#39; files may not have speed data!&#39;)
        if not np.array(self._ang_vel).any():
            self.set_ang_vel(self.calc_ang_vel())
        self.set_border(self.calc_border())

    def load_spatial_Axona(self, file_name):
        &#34;&#34;&#34;
        Loads Axona format spatial data to the NSpatial() object
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None        
        
        &#34;&#34;&#34;

        try:
            f = open(file_name, &#39;rt&#39;)
            self._set_data_source(file_name)
            self._set_source_format(&#39;Axona&#39;)
            while True:
                line = f.readline()
                if line == &#39;&#39;:
                    break
                elif line.startswith(&#39;time&#39;):
                    spatial_data = np.loadtxt(f, dtype=&#39;float&#39;, usecols=range(5))
            self._set_time(spatial_data[:, 0])
            self._set_pos_x(spatial_data[:, 1]- np.min(spatial_data[:, 1]))
            self._set_pos_y(spatial_data[:, 2]- np.min(spatial_data[:, 2]))
            self._set_direction(spatial_data[:, 3])
            self._set_speed(spatial_data[:, 4])
            f.seek(0, 0)
            pixel_size = list(map(float, re.findall(r&#34;\d+.\d+|\d+&#34;, f.readline())))
            self.set_pixel_size(pixel_size)
            self.smooth_direction()
        except:
            logging.error(&#39;File does not exist or is open in another process!&#39;)

    def load_spatial_NWB(self, file_name):
        &#34;&#34;&#34;
        Loads HDF5 format spatial data to the NSpatial() object
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None        
        
        &#34;&#34;&#34;
        file_name, path = file_name.split(&#39;+&#39;)
        if os.path.exists(file_name):
            hdf = Nhdf()
            hdf.set_filename(file_name)

            _record_info = {}
    
            if path in hdf.f:
                g = hdf.f[path]
            elif &#39;/processing/Behavioural/Position&#39; in hdf.f:
                path = &#39;/processing/Behavioural/Position&#39;
                g = hdf.f[path]
                logging.info(&#39;Path for spatial data set to: &#39; + path)
            else:
                logging.error(&#39;Path for spatial data does not exist!&#39;)
    
            for key, value in g.attrs.items():
                _record_info[key] = value
            
            self.set_record_info(_record_info)
    
            if path+ &#39;/&#39;+ &#39;location&#39; in g:
                g_loc = g[path+ &#39;/&#39;+ &#39;location&#39;]
                data = hdf.get_dataset(group=g_loc, name=&#39;data&#39;)
                self._set_pos_x(data[:, 0])
                self._set_pos_y(data[:, 1])
                self._set_time(hdf.get_dataset(group=g_loc, name=&#39;timestamps&#39;))
            else:
                logging.error(&#39;Spatial location information not found!&#39;)
    
            if path+ &#39;/&#39;+ &#39;direction&#39; in g:
                g_dir = g[path+ &#39;/&#39;+ &#39;direction&#39;]
                data = hdf.get_dataset(group=g_dir, name=&#39;data&#39;)
                self._set_direction(data)
            else:
                logging.error(&#39;Spatial direction information not found!&#39;)
    
            if path+ &#39;/&#39;+ &#39;speed&#39; in g:
                g_speed = g[path+ &#39;/&#39;+ &#39;speed&#39;]
                data = hdf.get_dataset(group=g_speed, name=&#39;data&#39;)
                self._set_speed(data)
            else:
                logging.error(&#39;Spatial speed information not found!&#39;)
    
            if path+ &#39;/&#39;+ &#39;angular velocity&#39; in g:
                g_ang_vel = g[path+ &#39;/&#39;+ &#39;angular velocity&#39;]
                data = hdf.get_dataset(group=g_ang_vel, name=&#39;data&#39;)
                self.set_ang_vel(data)
            else:
                self.set_ang_vel(np.array([]))
                logging.warning(&#39;Spatial angular velocity information not found, will be calculated from direction!&#39;)
    
            hdf.close()
            
    def load_spatial_Neuralynx(self, file_name):
        &#34;&#34;&#34;
        Loads Neuralynx format spatial data to the NSpatial() object
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None        
        
        &#34;&#34;&#34;

        self._set_data_source(file_name)
        self._set_source_format(&#39;Neuralynx&#39;)

        # Format description for the NLX file:
        header_offset = 16*1024 # fixed for NLX files

        bytes_start_record = 2
        bytes_origin_id = 2
        bytes_videoRec_size = 2
        bytes_per_timestamp = 8
        bytes_per_bitfield = 4*400
        bytes_sncrc = 2
        bytes_per_xloc = 4
        bytes_per_yloc = 4
        bytes_per_angle = 4
        bytes_per_target = 4*50

        record_size = None
        with open(file_name, &#39;rb&#39;) as f:
            while True:
                line = f.readline()
                try:
                    line = line.decode(&#39;UTF-8&#39;)
                except:
                    break

                if line == &#39;&#39;:
                    break
                if &#39;SamplingFrequency&#39; in line:
                    self._set_sampling_rate(float(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line))))
                if &#39;RecordSize&#39; in line:
                    record_size = int(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line)))
                if &#39;Time Opened&#39; in line:
                    self._set_date(re.search(r&#39;\d+/\d+/\d+&#39;, line).group())
                    self._set_time(re.search(r&#39;\d+:\d+:\d+&#39;, line).group())
                if &#39;FileVersion&#39; in line:
                    self._set_file_version(line.split()[1])

            if not record_size:
                record_size = bytes_start_record+ \
                             bytes_origin_id+ \
                             bytes_videoRec_size+  \
                             bytes_per_timestamp+ \
                             bytes_per_bitfield+ \
                             bytes_sncrc+ \
                             bytes_per_xloc+ \
                             bytes_per_yloc+ \
                             bytes_per_angle+ \
                             bytes_per_target

            time_offset = bytes_start_record+ \
                             bytes_origin_id+ \
                             bytes_videoRec_size
            xloc_offset = time_offset+ \
                         bytes_per_timestamp+ \
                         bytes_per_bitfield+ \
                         bytes_sncrc
            yloc_offset = xloc_offset+bytes_per_xloc
            angle_offset = yloc_offset+bytes_per_xloc

            f.seek(0, 2)
            self._total_samples = int((f.tell()- header_offset)/ record_size)
            spatial_data = np.zeros([self._total_samples, 4])

            f.seek(header_offset)
            for i in np.arange(self._total_samples):
                sample_bytes = np.fromfile(f, dtype=&#39;uint8&#39;, count=record_size)
                spatial_data[i, 0] = int.from_bytes(sample_bytes[time_offset+ np.arange(bytes_per_timestamp)], byteorder=&#39;little&#39;, signed=False)
                spatial_data[i, 1] = int.from_bytes(sample_bytes[xloc_offset+ np.arange(bytes_per_xloc)], byteorder=&#39;little&#39;, signed=False)
                spatial_data[i, 2] = int.from_bytes(sample_bytes[yloc_offset+ np.arange(bytes_per_yloc)], byteorder=&#39;little&#39;, signed=False)
                spatial_data[i, 3] = int.from_bytes(sample_bytes[angle_offset+ np.arange(bytes_per_angle)], byteorder=&#39;little&#39;, signed=False)

            spatial_data[:, 0] /= 10**6
            spatial_data[:, 0] -= np.min(spatial_data[:, 0])
            self._timestamp = np.mean(np.diff(spatial_data[:, 0]))
            self._set_sampling_rate(1/self._timestamp)
            self._set_time(spatial_data[:, 0])
            self._set_pos_x(spatial_data[:, 1]- np.min(spatial_data[:, 1]))
            self._set_pos_y(spatial_data[:, 2]- np.min(spatial_data[:, 2]))
            self._set_direction(spatial_data[:, 3])
            # Neuralynx data does not have any speed information
            self.smooth_direction()

    def smooth_speed(self):
        &#34;&#34;&#34;
        Smoothes the speed data using a moving-average box filter
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None        
        
        &#34;&#34;&#34;
        
        self._set_speed(smooth_1d(self.get_speed(), &#39;b&#39;, 5))
        
    def smooth_direction(self):
        &#34;&#34;&#34;
        Smoothes the angular head direction data using a moving circular average
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None
        
        See also
        --------
        nc_circular.CircStat().circ_smooth()
        
        &#34;&#34;&#34;
        
        cs = CircStat()
        cs.set_theta(self.get_direction())
        self._set_direction(cs.circ_smooth(filttype=&#39;b&#39;, filtsize=5))

    def calc_ang_vel(self, npoint=5):
        &#34;&#34;&#34;
        Calculates the angular head velocity of the animal from the direction data
        Each sample is the slope of a fitted line of five directional data centred
        around current sample.
        
        Parameters
        ----------
        None
        
        Returns
        -------
        None        
        
        &#34;&#34;&#34;
        
        theta = self.get_direction()
        ang_vel = np.zeros(theta.shape)
        N = theta.size
        L = npoint
        l = int(np.floor(L/2))
        cs = CircStat()
        for i in np.arange(l):
            y = cs.circ_regroup(theta[:L-l+ i])
            ang_vel[i] = np.polyfit(np.arange(len(y)), y, 1)[0]

        for i in np.arange(l, N- l, 1):
            y = cs.circ_regroup(theta[i-l:i+l+ 1])
            ang_vel[i] = np.polyfit(np.arange(len(y)), y, 1)[0]

        for i in np.arange(N- l, N):
            y = cs.circ_regroup(theta[i- l:])
            ang_vel[i] = np.polyfit(np.arange(len(y)), y, 1)[0]

        return ang_vel*self.get_sampling_rate()

    def calc_border(self, **kwargs):
        &#34;&#34;&#34;
        Identifies the border of the recording arena from the trace of the foraging of the
        animal in the arena
        
        Parameters
        ----------
        **kwargs
            Keyword arguments
        
        Returns
        -------
        border_dist : ndarray
            Distance of the animal from the border at each behavioural samples
        xedges : ndarray
            Pixelated edge of the x-axis
        yedges : ndarray
            Pixelated edge of the y-axis
        dist_mat : ndarray
            A matrix of distance of each pixel of the arena from the identified
            border
        
        &#34;&#34;&#34;
        
        # define edges
        pixel = kwargs.get(&#39;pixel&#39;, 3)
        chop_bound = kwargs.get(&#39;chop_bound&#39;, 5)

        xedges = np.arange(0, np.ceil(np.max(self._pos_x)), pixel)
        yedges = np.arange(0, np.ceil(np.max(self._pos_y)), pixel)

        tmap, yedges, xedges = histogram2d(self._pos_y, self._pos_x, yedges, xedges)
        if abs(xedges.size- yedges.size) &lt;= chop_bound:
            tmap = chop_edges(tmap, min(tmap.shape), min(tmap.shape))[2]
        else:
            tmap = chop_edges(tmap, tmap.shape[1], tmap.shape[0])[2]

        ybin, xbin = tmap.shape

        border = np.zeros(tmap.shape)
        border[tmap &gt; 0] = False
        border[tmap == 0] = True

        for J in np.arange(ybin):
            for I in np.arange(xbin):
                if not border[J, I] and (J == ybin-1 or J == 0 or I == xbin-1 or I == 0):
                    border[J, I] = True


        # Optimize the border
        optBorder = np.zeros(border.shape)
        for i in np.arange(border.shape[0]):
            for j in np.arange(border.shape[1]):
                if border[i, j]:
                    if i == 0: # along the 1st row
                        if border[i, j] != border[i + 1, j]:
                            optBorder[i, j] = True
                    elif j == 0: # along the 1st column
                        if border[i, j] != border[i, j + 1]:
                            optBorder[i, j] = True
                    elif i == border.shape[0] - 1: # along the last row
                        if border[i, j] != border[i - 1, j]:
                            optBorder[i, j] = True
                    elif j == border.shape[1] - 1:# along the last column
                        if border[i, j] != border[i, j - 1]:
                            optBorder[i, j] = True
                    else: # other cases
                        if (border[i, j] != border[i, j + 1]) or (border[i, j] != border[i + 1, j])\
                                or (border[i, j] != border[i, j - 1]) or (border[i, j] != border[i - 1, j]):
                            optBorder[i, j] = True

        border = optBorder

        xborder = np.zeros(tmap.shape, dtype=bool)
        yborder = np.zeros(tmap.shape, dtype=bool)
        for J in np.arange(ybin):
            xborder[J, find(border[J, :], 1, &#39;first&#39;)] = True # 1 added/subed to the next pixel of the traversed arena as the border
            xborder[J, find(border[J, :], 1, &#39;last&#39;)] = True
        for I in np.arange(xbin):
            yborder[find(border[:, I], 1, &#39;first&#39;), I] = True
            yborder[find(border[:, I], 1, &#39;last&#39;), I] = True

        #        self.border = border
        border = xborder | yborder
        self.tmap = tmap*self._timestamp

        distMat = np.zeros(border.shape)
        xx, yy = np.meshgrid(np.arange(xbin), np.arange(ybin))
        borderDist = np.zeros(self._time.size)

        xedges = np.arange(xbin)*pixel
        yedges = np.arange(ybin)*pixel
        xind = histogram(self._pos_x, xedges)[1]
        yind = histogram(self._pos_y, yedges)[1]

        for J in np.arange(ybin):
            for I in np.arange(xbin):
                dist_arr = (
                    np.abs(xx[border] - xx[J, I]) +
                    np.abs(yy[border] - yy[J, I]))
                if dist_arr.size == 0:
                    logging.error(&#34;could not calculate border&#34;)
                    return None, None, None, None
                tmp_dist = np.min(dist_arr)
                if find(np.logical_and(xind == I, yind == J)).size:
                    borderDist[np.logical_and(xind == I, yind == J)] = tmp_dist
                distMat[J, I] = tmp_dist
        
        dist_mat= distMat*pixel
        border_dist= borderDist*pixel
        
        return border_dist, xedges, yedges, dist_mat

    @staticmethod
    def skaggs_info(firing_rate, visit_time):
        &#34;&#34;&#34;
        Calculates the Skaggs information content of the spatial firing
        
        Parameters
        ----------
        firing_rate : ndarray
            Firing rate of the unit at each pixelated location or binned information,
            i.e., binned speed or head-direction            
        visit_time : ndarray
            Amount of time animal spent in each pixel or bin
        
        Returns
        -------
        float
            Skaggs information content
        
        &#34;&#34;&#34;
        
        firing_rate[np.isnan(firing_rate)] = 0
        Li = firing_rate # Lambda
        L = np.sum(firing_rate*visit_time)/ visit_time.sum()
        P = visit_time/visit_time.sum()
        
        return np.sum(P[Li &gt; 0]*(Li[Li &gt; 0]/L)*np.log2(Li[Li &gt; 0]/L))

    @staticmethod
    def spatial_sparsity(firing_rate, visit_time):
        &#34;&#34;&#34;
        Calculates the spatial sparsity of the spatial firing
        
        Parameters
        ----------
        firing_rate : ndarray
            Firing rate of the unit at each pixelated location  
        visit_time : ndarray
            Amount of time animal spent in each pixel
        
        Returns
        -------
        float
            Spatial sparsity
        
        &#34;&#34;&#34;
        
        firing_rate[np.isnan(firing_rate)] = 0
        Li = firing_rate # Lambda
        # L = np.sum(firing_rate*visit_time)/ visit_time.sum()
        P = visit_time/visit_time.sum()
        return np.sum(P*Li)**2/ np.sum(P*Li**2)

    def speed(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Calculates the firing rate of the unit at different binned speeds.
        
        The spike rate vs speed is fitted with a linear equation and goodness of fit
        is measured
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
        &#34;&#34;&#34;

        _results = oDict()
        graph_data = {}
        update = kwargs.get(&#39;update&#39;, True) # When update = True, it will use the
                                            #results for statistics, if False,
                                            #i.e. in Multiple Regression, it will ignore updating
        binsize = kwargs.get(&#39;binsize&#39;, 1)
        min_speed, max_speed = kwargs.get(&#39;range&#39;, [0, 40])

        speed = self.get_speed()
        max_speed = min(max_speed, np.ceil(speed.max()/binsize)*binsize)
        min_speed = max(min_speed, np.floor(speed.min()/binsize)*binsize)
        bins = np.arange(min_speed, max_speed, binsize)

        vid_count = histogram(ftimes, self.get_time())[0]
        visit_time, speedInd = histogram(speed, bins)[0:2]
        visit_time = visit_time/self.get_sampling_rate()

        rate = np.array([sum(vid_count[speedInd == i]) for i in range(len(bins))])/ visit_time
        rate[np.isnan(rate)] = 0

        _results[&#39;Speed Skaggs&#39;] = self.skaggs_info(rate, visit_time)

        rate = rate[visit_time &gt; 1]
        bins = bins[visit_time &gt; 1]

        fit_result = linfit(bins, rate)

        _results[&#39;Speed Pears R&#39;] = fit_result[&#39;Pearson R&#39;]
        _results[&#39;Speed Pears P&#39;] = fit_result[&#39;Pearson P&#39;]
        graph_data[&#39;bins&#39;] = bins
        graph_data[&#39;rate&#39;] = rate
        graph_data[&#39;fitRate&#39;] = fit_result[&#39;yfit&#39;]

        if update:
            self.update_result(_results)
        return graph_data

    def angular_velocity(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Calculates the firing rate of the unit at different binned angular head velocity.
        
        The spike rate vs speed is fitted with a linear equation individually
        for the negative and positive angular velocities, and goodness of fit
        is measured
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
        &#34;&#34;&#34;
        
        _results = oDict()
        graph_data = {}
        update = kwargs.get(&#39;update&#39;, True) # When update = True, it will use the
                                            #results for statistics, if False,
                                            #i.e. in Multiple Regression, it will ignore updating
        binsize = kwargs.get(&#39;binsize&#39;, 10)
        min_vel, max_vel = kwargs.get(&#39;range&#39;, [-100, 100])
        cutoff = kwargs.get(&#39;cutoff&#39;, 10)

        ang_vel = self.get_ang_vel()

        max_vel = min(max_vel, np.ceil(ang_vel.max()/binsize)*binsize)
        min_vel = max(min_vel, np.floor(ang_vel.min()/binsize)*binsize)
        bins = np.arange(min_vel, max_vel, binsize)

        vid_count = histogram(ftimes, self.get_time())[0]
        visit_time, velInd = histogram(ang_vel, bins)[0:2]
        visit_time = visit_time/self.get_sampling_rate()

        rate = np.array([sum(vid_count[velInd == i]) for i in range(len(bins))])/ visit_time
        rate[np.isnan(rate)] = 0

        _results[&#39;speedSkaggs&#39;] = self.skaggs_info(rate, visit_time)

        rate = rate[visit_time &gt; 1]
        bins = bins[visit_time &gt; 1]


        fit_result = linfit(bins[bins &lt;= -cutoff], rate[bins &lt;= -cutoff])

        _results[&#39;Ang Vel Left Pears R&#39;] = fit_result[&#39;Pearson R&#39;]
        _results[&#39;Ang Vel Left Pears P&#39;] = fit_result[&#39;Pearson P&#39;]
        graph_data[&#39;leftBins&#39;] = bins[bins &lt;= -cutoff]
        graph_data[&#39;leftRate&#39;] = rate[bins &lt;= -cutoff]
        graph_data[&#39;leftFitRate&#39;] = fit_result[&#39;yfit&#39;]

        fit_result = linfit(bins[bins &gt;= cutoff], rate[bins &gt;= cutoff])

        _results[&#39;Ang Vel Right Pears R&#39;] = fit_result[&#39;Pearson R&#39;]
        _results[&#39;Ang Vel Right Pears P&#39;] = fit_result[&#39;Pearson P&#39;]
        graph_data[&#39;rightBins&#39;] = bins[bins &gt;= cutoff]
        graph_data[&#39;rightRate&#39;] = rate[bins &gt;= cutoff]
        graph_data[&#39;rightFitRate&#39;] = fit_result[&#39;yfit&#39;]

        if update:
            self.update_result(_results)
        return graph_data

    def place(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Calculates the two-dimensional firing rate of the unit with respect to
        the location of the animal in the environment. This is called Firing map.
        
        Specificity indices are measured to assess the quality of location-specific firing of the unit.
        
        This method also plot the events of spike occurring superimposed on the
        trace of the animal in the arena, commonly known as Spike Plot.
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
        &#34;&#34;&#34;
        
        _results = oDict()
        graph_data = {}
        update = kwargs.get(&#39;update&#39;, True)
        pixel = kwargs.get(&#39;pixel&#39;, 3)
        chop_bound = kwargs.get(&#39;chop_bound&#39;, 5)
        filttype, filtsize = kwargs.get(&#39;filter&#39;, [&#39;b&#39;, 5])
        lim = kwargs.get(&#39;range&#39;, [0, self.get_duration()])
        brAdjust = kwargs.get(&#39;brAdjust&#39;, True)
        thresh = kwargs.get(&#39;fieldThresh&#39;, 0.2)
        required_neighbours = kwargs.get(&#39;minPlaceFieldNeighbours&#39;, 9)
        smooth_place = kwargs.get(&#39;smoothPlace&#39;, False)
        separate_border_data = kwargs.get(
            &#34;separateBorderData&#34;, False)

        # xedges = np.arange(0, np.ceil(np.max(self._pos_x)), pixel)
        # yedges = np.arange(0, np.ceil(np.max(self._pos_y)), pixel)

        # Update the border to match the requested pixel size
        if separate_border_data:
            self.set_border(
                separate_border_data.calc_border(**kwargs))
            times = self._time
            lower, upper = (times.min(), times.max())
            new_times = separate_border_data._time
            sample_spatial_idx = (
                (new_times &lt;= upper) &amp; (new_times &gt;= lower)).nonzero()
            self._border_dist = self._border_dist[sample_spatial_idx]
        else:  
            self.set_border(self.calc_border(**kwargs))

        xedges = self._xbound
        yedges = self._ybound

        spikeLoc = self.get_event_loc(ftimes, **kwargs)[1]
        posX = self._pos_x[np.logical_and(self.get_time() &gt;= lim[0], self.get_time() &lt;= lim[1])]
        posY = self._pos_y[np.logical_and(self.get_time() &gt;= lim[0], self.get_time() &lt;= lim[1])]

        tmap, yedges, xedges = histogram2d(posY, posX, yedges, xedges)

        if tmap.shape[0] != tmap.shape[1] &amp; np.abs(tmap.shape[0]- tmap.shape[1]) &lt;= chop_bound:
            tmap = chop_edges(tmap, min(tmap.shape), min(tmap.shape))[2]
        tmap /= self.get_sampling_rate()

        ybin, xbin = tmap.shape
        xedges = np.arange(xbin)*pixel
        yedges = np.arange(ybin)*pixel

        spike_count = histogram2d(spikeLoc[1], spikeLoc[0], yedges, xedges)[0]
        fmap = np.divide(spike_count, tmap, out=np.zeros_like(spike_count), where=tmap != 0)

        if brAdjust:
            nfmap = fmap/ fmap.max()
            if np.sum(np.logical_and(nfmap &gt;= 0.2, tmap != 0)) &gt;= 0.8*nfmap[tmap != 0].flatten().shape[0]:
                back_rate = np.mean(fmap[np.logical_and(nfmap &gt;= 0.2, nfmap &lt; 0.4)])
                fmap -= back_rate
                fmap[fmap &lt; 0] = 0

        if filttype is not None:
            smoothMap = smooth_2d(fmap, filttype, filtsize)
        else :
            smoothMap = fmap
        
        if smooth_place:
            pmap = smoothMap
        else:
            pmap = fmap
        
        pmap[tmap == 0] = None
        pfield, largest_group = NSpatial.place_field(
            pmap, thresh, required_neighbours)
        # if largest_group == 0:
        #     if smooth_place:
        #         info = &#34;where the place field was calculated from smoothed data&#34;
        #     else:
        #         info = &#34;where the place field was calculated from raw data&#34;
        #     logging.info(
        #         &#34;Lack of high firing neighbours to identify place field &#34; +
        #         info)
        centroid = NSpatial.place_field_centroid(pfield, pmap, largest_group)
        #centroid is currently in co-ordinates, convert to pixels
        centroid = centroid * pixel + (pixel * 0.5)
        #flip x and y
        centroid = centroid[::-1]
        
        p_shape = pfield.shape
        maxes = [xedges.max(), yedges.max()]
        scales = (
             maxes[0] / p_shape[1],
             maxes[1] / p_shape[0])
        co_ords = np.array(np.where(pfield == largest_group))
        boundary = [None, None]
        for i in range(2):
            j = (i + 1) % 2
            boundary[i] = (
                co_ords[j].min() * scales[i],
                np.clip((co_ords[j].max()+1) * scales[i], 0, maxes[i]))
        inside_x = (
            (boundary[0][0] &lt;= spikeLoc[0]) &amp;
            (spikeLoc[0] &lt;= boundary[0][1]))
        inside_y = (
            (boundary[1][0] &lt;= spikeLoc[1]) &amp;
            (spikeLoc[1] &lt;= boundary[1][1]))
        co_ords = np.nonzero(np.logical_and(inside_x, inside_y))

        if update:
            _results[&#39;Spatial Skaggs&#39;] = self.skaggs_info(fmap, tmap)
            _results[&#39;Spatial Sparsity&#39;] = self.spatial_sparsity(fmap, tmap)
            _results[&#39;Spatial Coherence&#39;] = np.corrcoef(fmap[tmap != 0].flatten(), smoothMap[tmap != 0].flatten())[0, 1]
            _results[&#39;Found strong place field&#39;] = (largest_group != 0)
            _results[&#39;Place field Centroid x&#39;] = centroid[0]
            _results[&#39;Place field Centroid y&#39;] = centroid[1]
            _results[&#39;Place field Boundary x&#39;] = boundary[0]
            _results[&#39;Place field Boundary y&#39;] = boundary[1]
            _results[&#39;Number of Spikes in Place Field&#39;] = co_ords[0].size
            _results[&#39;Percentage of Spikes in Place Field&#39;] = co_ords[0].size*100 / ftimes.size
            self.update_result(_results)

        smoothMap[tmap == 0] = None

        graph_data[&#39;posX&#39;] = posX
        graph_data[&#39;posY&#39;] = posY
        graph_data[&#39;fmap&#39;] = fmap
        graph_data[&#39;smoothMap&#39;] = smoothMap
        graph_data[&#39;firingMap&#39;] = fmap
        graph_data[&#39;tmap&#39;] = tmap
        graph_data[&#39;xedges&#39;] = xedges
        graph_data[&#39;yedges&#39;] = yedges
        graph_data[&#39;spikeLoc&#39;] = spikeLoc
        graph_data[&#39;placeField&#39;] = pfield
        graph_data[&#39;largestPlaceGroup&#39;] = largest_group
        graph_data[&#39;placeBoundary&#39;] = boundary
        graph_data[&#39;indicesInPlaceField&#39;] = co_ords
        graph_data[&#39;centroid&#39;] = centroid

        return graph_data
    
    # Created by Sean Martin: 13/02/2019
    def place_field_centroid_zscore(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        A naive method to find the centroid of the place field using the 
        z-score of the normal distribution to remove outliers, 
        and then averaging remaining locations&#39; co-ordinates.
                
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        **kwargs
            Keyword arguments
 
        Returns
        -------
        ndarray
            The centroid of the place field
        &#34;&#34;&#34;

        _results = oDict()
        update = kwargs.get(&#39;update&#39;, True)
        lim = kwargs.get(&#39;range&#39;, [0, self.get_duration()])
        remove_outliers = kwargs.get(&#39;remove_outliers&#39;, True)
        threshold = kwargs.get(&#39;z_threshold&#39;, 2)

        spikeLoc = self.get_event_loc(ftimes, **kwargs)[1]
        
        if remove_outliers:
            z_scores = sc.stats.zscore(spikeLoc, axis=1)
            # Filter out locations with x or y outside of 3 std devs.
            filter_array = np.logical_and((abs(z_scores[0]) &lt; threshold).astype(bool), (abs(z_scores[1]) &lt; threshold).astype(bool))
            spikeLoc[0] = spikeLoc[0][filter_array]
            spikeLoc[1] = spikeLoc[1][filter_array]

        centroid = np.average(spikeLoc, axis=1)
        if update:
            _results[&#39;Place field Centroid x&#39;] = centroid[0]
            _results[&#39;Place field Centroid y&#39;] = centroid[1]
            self.update_result(_results)
        return centroid

    def non_moving_periods(self, **kwargs):
        &#34;&#34;&#34;
        Returns a number of tuples indicating ranges where the subject is not moving

        kwargs
        ------
        should_smooth : bool
            flags if the speed data should be smoothed, default False
        min_range : float
            the minimum amount of time that the subject should not be moving for
        moving_thresh : float
            any speed above this thresh is considered to be movement
        &#34;&#34;&#34;
        should_smooth = kwargs.get(&#34;should_smooth&#34;, False)
        min_range = kwargs.get(&#34;min_range&#34;, 150)
        moving_thresh = kwargs.get(&#34;moving_thresh&#34;, 2.5)

        if should_smooth:
            self.smooth_speed()
        not_moving = self.get_speed() &lt; moving_thresh

        return find_true_ranges(
            self.get_time(), not_moving, min_range)

    def get_non_moving_times(self, **kwargs):
        &#34;&#34;&#34; Returns the times where the subject is not moving&#34;&#34;&#34;
        ranges = self.non_moving_periods(**kwargs)
        time_data = [
            val for val in self.get_time()
            if any(lower &lt;= val &lt;= upper for (lower, upper) in ranges)
        ]
        return time_data

    def loc_time_lapse(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Calculates the firing rate map and idnetifies the location of the spiking events
        at certain intervals. This method is useful in observing the evolution of
        unit-activity as the animal traverses the environment.
        
        Following intervals ar used:
            0-1min, 0-2min, 0-4min, 0-8min, 0-16min or 0-end depending on the recording duration
            0-1min, 1-2min, 2-4min, 4-8min, 8-16min or 16-end depending on the recording duration
                
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
        &#34;&#34;&#34;
        
        graph_data = oDict()
        pixel = kwargs.get(&#39;pixel&#39;, 3)
        chop_bound = kwargs.get(&#39;chop_bound&#39;, 5)
        filter = kwargs.get(&#39;filter&#39;, [&#39;b&#39;, 5])
        brAdjust = kwargs.get(&#39;brAdjust&#39;, True)

        lim = [0, 1*60]
        graph_data[&#39;0To1min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

        lim = [0, 2*60]
        graph_data[&#39;0To2min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

        lim = [0, 4*60]
        graph_data[&#39;0To4min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

        lim = [0, 8*60]
        graph_data[&#39;0To8min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

        # 0-1min, 1-2min,  2-4min, 4-8min
        lim = [1*60, 2*60]
        graph_data[&#39;1To2min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

        lim = [2*60, 4*60]
        graph_data[&#39;2To4min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

        lim = [4*60, 8*60]
        graph_data[&#39;4To8min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

        ## 0-16min, 8-16min 0-20min, 16-20min

        if self.get_duration() &gt; 8*60:
            if self.get_duration() &gt; 16*60:
                lim = [0, 16*60]
                graph_data[&#39;0To16min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

                lim = [8*60, 16*60]
                graph_data[&#39;8To16min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

                lim = [0, self.get_duration()]
                graph_data[&#39;0ToEnd&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

                lim = [16*60, self.get_duration()]
                graph_data[&#39;16ToEnd&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

            else:
                lim = [0, self.get_duration()]
                graph_data[&#39;0ToEnd&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

                lim = [8*60, self.get_duration()]
                graph_data[&#39;8ToEnd&#39;] = self.place(ftimes, range=lim, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

            return graph_data

    def hd_rate(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Calculates the firing rate of the unit with respect to the head-direciton
        of the animal in the environment. This is calle Tuning curve.
        
        Precited firing map from the locational firing is also calculated and
        distributive ratio is measured along with the Skaggs information.
        
        Spike-plot similar to locational firing is developed but in the circular bins
        which shows the direction of the animal&#39;s head at each spike&#39;s occurring time.
                                
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
        &#34;&#34;&#34;
        
        _results = oDict()
        graph_data = {}
        update = kwargs.get(&#39;update&#39;, True)
        binsize = kwargs.get(&#39;binsize&#39;, 5) # in degrees
        filttype, filtsize = kwargs.get(&#39;filter&#39;, [&#39;b&#39;, 5])
        lim = kwargs.get(&#39;range&#39;, [0, self.get_duration()])

        bins = np.arange(0, 360, binsize)

        spike_hd = self.get_event_loc(ftimes, **kwargs)[2]
        direction = self.get_direction()[np.logical_and(self.get_time() &gt;= lim[0], self.get_time() &lt;= lim[1])]

        tcount, ind, bins = histogram(direction, bins)

        tcount = tcount/ self.get_sampling_rate()

        spike_count = histogram(spike_hd, bins)[0].astype(tcount.dtype)

        hd_rate = np.divide(spike_count, tcount, out=np.zeros_like(spike_count), where=tcount != 0, casting=&#39;unsafe&#39;)

        smoothRate = smooth_1d(hd_rate, filttype, filtsize)

        if update:
            _results[&#39;HD Skaggs&#39;] = self.skaggs_info(hd_rate, tcount)
            cs = CircStat(rho=smoothRate, theta=bins)
            results = cs.calc_stat()
            _results[&#39;HD Rayl Z&#39;] = results[&#39;RaylZ&#39;]
            _results[&#39;HD Rayl P&#39;] = results[&#39;RaylP&#39;]
            _results[&#39;HD von Mises K&#39;] = results[&#39;vonMisesK&#39;]
            
            _results[&#39;HD Mean&#39;] = results[&#39;meanTheta&#39;]
            _results[&#39;HD Mean Rate&#39;] = results[&#39;meanRho&#39;]
            _results[&#39;HD Res Vect&#39;] = results[&#39;resultant&#39;]

            binInterp = np.arange(360)
            rateInterp = np.interp(binInterp, bins, hd_rate)

            _results[&#39;HD Peak Rate&#39;] = np.amax(rateInterp)
            _results[&#39;HD Peak&#39;] = binInterp[np.argmax(rateInterp)]

            half_max = np.amin(rateInterp)+ (np.amax(rateInterp)- np.amin(rateInterp))/2
            d = np.sign(half_max - rateInterp[0:-1]) - np.sign(half_max - rateInterp[1:])
            left_possible = find(d &gt; 0)
            if len(left_possible) &gt; 0:
                left_idx = left_possible[0]
            else:
                left_idx = None
            
            right_possible = find(d &lt; 0)
            if len(right_possible) &gt; 0:
                right_idx = right_possible[-1]
            else:
                right_idx = None
            _results[&#39;HD Half Width&#39;] = None if (not left_idx or not right_idx or left_idx &gt; right_idx) \
                                        else binInterp[right_idx]- binInterp[left_idx]

            pixel = kwargs.get(&#39;pixel&#39;, 3)
            placeData = self.place(ftimes, pixel=pixel)
            fmap = placeData[&#39;smoothMap&#39;]
            fmap[np.isnan(fmap)] = 0
            hdPred = np.zeros(bins.size)
            for i, b in enumerate(bins):
                hdInd = np.logical_and(direction &gt;= b, direction &lt; b+ binsize)
                tmap = histogram2d(self.get_pos_y()[hdInd], self.get_pos_x()[hdInd], placeData[&#39;yedges&#39;], placeData[&#39;xedges&#39;])[0]
                tmap /= self.get_sampling_rate()
                hdPred[i] = np.sum(fmap*tmap)/ tmap.sum()

            graph_data[&#39;hdPred&#39;] = smooth_1d(hdPred, &#39;b&#39;, 5)
            self.update_result(_results)

        graph_data[&#39;hd&#39;] = direction
        graph_data[&#39;hdRate&#39;] = hd_rate
        graph_data[&#39;smoothRate&#39;] = smoothRate
        graph_data[&#39;tcount&#39;] = tcount
        graph_data[&#39;bins&#39;] = bins
        graph_data[&#39;spike_hd&#39;] = spike_hd

        cs = CircStat()
        cs.set_theta(spike_hd)
        graph_data[&#39;scatter_radius&#39;], graph_data[&#39;scatter_bins&#39;] = cs.circ_scatter(bins=2, step=0.05)


        return graph_data

    def hd_rate_ccw(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Calculates the tuning curve but split into clock-wise vs counterclockwise
        head-directional movement.
                                
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
        &#34;&#34;&#34;
        
        _results = oDict()
        graph_data = {}
        update = kwargs.get(&#39;update&#39;, True)
        binsize = kwargs.get(&#39;binsize&#39;, 5) # in degrees
        filttype, filtsize = kwargs.get(&#39;filter&#39;, [&#39;b&#39;, 5])
        lim = kwargs.get(&#39;range&#39;, [0, self.get_duration()])
        thresh = kwargs.get(&#39;thresh&#39;, 30)

        edges = np.arange(0, 360, binsize)

        spikeInd, spikeLoc, spike_hd = self.get_event_loc(ftimes, **kwargs)
        vidInd = np.logical_and(self.get_time() &gt;= lim[0], self.get_time() &lt;= lim[1])
        direction = self.get_direction()[vidInd]

        ccwSpike_hd = spike_hd[self.get_ang_vel()[spikeInd] &lt; -thresh]
        cwSpike_hd = spike_hd[self.get_ang_vel()[spikeInd] &gt; thresh]

        ccw_dir = direction[self.get_ang_vel()[vidInd] &lt; -thresh]
        cw_dir = direction[self.get_ang_vel()[vidInd] &gt; thresh]


        binInterp = np.arange(360)

        tcount, ind, bins = histogram(cw_dir, edges)
        tcount = tcount/ self.get_sampling_rate()
        spike_count = histogram(cwSpike_hd, edges)[0].astype(tcount.dtype)
        cwRate = np.divide(spike_count, tcount, out=np.zeros_like(spike_count), where=tcount != 0, casting=&#39;unsafe&#39;)
        cwRate = np.interp(binInterp, bins, smooth_1d(cwRate, filttype, filtsize))

        tcount, ind, bins = histogram(ccw_dir, edges)
        tcount = tcount/ self.get_sampling_rate()
        spike_count = histogram(ccwSpike_hd, edges)[0].astype(tcount.dtype)
        ccwRate = np.divide(spike_count, tcount, out=np.zeros_like(spike_count), where=tcount != 0, casting=&#39;unsafe&#39;)
        ccwRate = np.interp(binInterp, bins, smooth_1d(ccwRate, filttype, filtsize))

        if update:
            _results[&#39;HD Delta&#39;] = binInterp[np.argmax(ccwRate)]- binInterp[np.argmax(cwRate)]
            _results[&#39;HD Peak CW&#39;] = np.argmax(cwRate)
            _results[&#39;HD Peak CCW&#39;] = np.argmax(ccwRate)
            _results[&#39;HD Peak Rate CW&#39;] = np.amax(cwRate)
            _results[&#39;HD Peak Rate CCW&#39;] = np.amax(ccwRate)
            self.update_result(_results)

        graph_data[&#39;bins&#39;] = binInterp
        graph_data[&#39;hdRateCW&#39;] = cwRate
        graph_data[&#39;hdRateCCW&#39;] = ccwRate

        return graph_data

    def hd_time_lapse(self, ftimes):
        &#34;&#34;&#34;
        Calculates the tuning curve and idnetifies the location of the spiking events
        at certain intervals. This method is useful in observing the evolution of
        unit-activity as the animal traverses the environment.
        
        Following intervals ar used:
            0-1min, 0-2min, 0-4min, 0-8min, 0-16min or 0-end depending on the recording duration
            0-1min, 1-2min, 2-4min, 4-8min, 8-16min or 16-end depending on the recording duration
                
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
        &#34;&#34;&#34;
        
        #### Breaking down the spike plot for firing evolution
        # 0-1min,  0-2min, 0-4min, 0-8min
        graph_data = oDict()
        lim = [0, 1*60]
        graph_data[&#39;0To1min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

        lim = [0, 2*60]
        graph_data[&#39;0To2min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

        lim = [0, 4*60]
        graph_data[&#39;0To4min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

        lim = [0, 8*60]
        graph_data[&#39;0To8min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

        # 0-1min, 1-2min,  2-4min, 4-8min
        lim = [1*60, 2*60]
        graph_data[&#39;1To2min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

        lim = [2*60, 4*60]
        graph_data[&#39;2To4min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

        lim = [4*60, 8*60]
        graph_data[&#39;4To8min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

        ## 0-16min, 8-16min 0-20min, 16-20min

        if self.get_duration() &gt; 8*60:
            if self.get_duration() &gt; 16*60:
                lim = [0, 16*60]
                graph_data[&#39;0To16min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

                lim = [8*60, 16*60]
                graph_data[&#39;8To16min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

                lim = [0, self.get_duration()]
                graph_data[&#39;0ToEnd&#39;] = self.hd_rate(ftimes, range=lim, update=False)

                lim = [16*60, self.get_duration()]
                graph_data[&#39;16ToEnd&#39;] = self.hd_rate(ftimes, range=lim, update=False)

            else:
                lim = [0, self.get_duration()]
                graph_data[&#39;0ToEnd&#39;] = self.hd_rate(ftimes, range=lim, update=False)

                lim = [8*60, self.get_duration()]
                graph_data[&#39;8ToEnd&#39;] = self.hd_rate(ftimes, range=lim, update=False)

            return graph_data

    def hd_shuffle(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Shuffling analysis of the unit to see if the head-directional firing specifity
        is by chance or actually correlated to the head-direction of the animal
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;
        
        _results = oDict()
        graph_data = {}
        nshuff = kwargs.get(&#39;nshuff&#39;, 500)
        limit = kwargs.get(&#39;limit&#39;, 0)
        bins = kwargs.get(&#39;bins&#39;, 100)
        # limit = 0 implies enirely random shuffle, limit = &#39;x&#39; implies nshuff number of shuffles in the range [-x x]
        dur = self.get_time()[-1]
        shift = nprand.uniform(low=limit- dur, high=dur- limit, size=nshuff)
        raylZ = np.zeros((nshuff,))
        vonMisesK = np.zeros((nshuff,))
        for i in np.arange(nshuff):
            shift_ftimes = ftimes+ shift[i]
            # Wrapping up the time
            shift_ftimes[shift_ftimes &gt; dur] -= dur
            shift_ftimes[shift_ftimes &lt; 0] += dur

            hdData = self.hd_rate(shift_ftimes, update=False)
            cs = CircStat(rho=hdData[&#39;smoothRate&#39;], theta=hdData[&#39;bins&#39;])
            results = cs.calc_stat()
            raylZ[i] = results[&#39;RaylZ&#39;]
            vonMisesK[i] = results[&#39;vonMisesK&#39;]

        graph_data[&#39;raylZ&#39;] = raylZ
        graph_data[&#39;vonMisesK&#39;] = vonMisesK
        hdData = self.hd_rate(ftimes, update=False)
        cs.set_rho(hdData[&#39;smoothRate&#39;])
        results = cs.calc_stat()
        graph_data[&#39;refRaylZ&#39;] = results[&#39;RaylZ&#39;]
        graph_data[&#39;refVonMisesK&#39;] = results[&#39;vonMisesK&#39;]

        graph_data[&#39;raylZCount&#39;], ind, graph_data[&#39;raylZEdges&#39;] = histogram(raylZ, bins=bins)
        graph_data[&#39;raylZPer95&#39;] = np.percentile(raylZ, 95)

        graph_data[&#39;vonMisesKCount&#39;], ind, graph_data[&#39;vonMisesKEdges&#39;] = histogram(vonMisesK, bins=bins)
        graph_data[&#39;vonMisesKPer95&#39;] = np.percentile(vonMisesK, 95)

        _results[&#39;HD Shuff Rayl Z Per 95&#39;] = np.percentile(raylZ, 95)
        _results[&#39;HD Shuff von Mises K Per 95&#39;] = np.percentile(vonMisesK, 95)
        self.update_result(_results)

        return graph_data

    def hd_shift(self, ftimes, shift_ind=np.arange(-10, 11)):
        &#34;&#34;&#34;
        Analysis of firing specificity of the unit with respect to animal&#39;s head
        direction to oberve whether it represents past direction or anicipates a
        future direction.
                
        Parameters
        ----------
        shift_ind : ndarray
            Index of spatial resolution shift for the spike event time. Shift -1
            implies shift to the past by 1 spatial time resolution, and +2 implies
            shift to the future by 2 spatial time resoultion.
 
        Returns
        -------
        dict
            Graphical data of the analysis
    
        &#34;&#34;&#34;
        
        _results = oDict()
        graph_data = {}
        shift = shift_ind/self.get_sampling_rate()
        shiftlen = shift.size
        dur = self.get_time()[-1]
        delta = np.zeros((shiftlen,))
        skaggs = np.zeros((shiftlen,))
        peakRate = np.zeros((shiftlen,))

        for i in np.arange(shiftlen):
            shift_ftimes = ftimes+ shift[i]
            # Wrapping up the time
            shift_ftimes[shift_ftimes &gt; dur] -= dur
            shift_ftimes[shift_ftimes &lt; 0] += dur

            hdData = self.hd_rate_ccw(shift_ftimes, update=False)
            delta[i] = hdData[&#39;bins&#39;][np.argmax(hdData[&#39;hdRateCCW&#39;])]- hdData[&#39;bins&#39;][np.argmax(hdData[&#39;hdRateCW&#39;])]
            hdData = self.hd_rate(shift_ftimes, update=False)
            peakRate[i] = np.amax(hdData[&#39;smoothRate&#39;])
            skaggs[i] = self.skaggs_info(hdData[&#39;hdRate&#39;], hdData[&#39;tcount&#39;])

        graph_data[&#39;delta&#39;] = delta
        graph_data[&#39;skaggs&#39;] = skaggs
        graph_data[&#39;peakRate&#39;] = peakRate
        graph_data[&#39;shiftTime&#39;] = shift*1000 # changing to milisecond

        # Find out the optimum skaggs location
        shiftUpsamp = np.arange(shift[0], shift[-1], np.mean(np.diff(shift))/10)
        skaggsUpsamp = np.interp(shiftUpsamp, shift, skaggs)
        peakRateUpsamp = np.interp(shiftUpsamp, shift, peakRate)

        dfit_result = linfit(shift, delta)
        deltaFit = dfit_result[&#39;yfit&#39;]
        sortInd = np.argsort(deltaFit)
        _results[&#39;HD ATI&#39;] = np.interp(0, deltaFit[sortInd], shift[sortInd])*1000 if dfit_result[&#39;Pearson R&#39;] &gt;= 0.85 else np.nan

        graph_data[&#39;deltaFit&#39;] = deltaFit
        imax = sg.argrelmax(skaggsUpsamp)[0]
        maxloc = find(skaggsUpsamp[imax] == skaggsUpsamp.max())
        _results[&#39;HD Opt Shift Skaggs&#39;] = np.nan if maxloc.size != 1 else \
                    (np.nan if imax[maxloc] == 0 or imax[maxloc] == skaggsUpsamp.size else shiftUpsamp[imax[maxloc]][0]*1000) # in milisecond

        imax = sg.argrelmax(peakRateUpsamp)[0]
        maxloc = find(peakRateUpsamp[imax] == peakRateUpsamp.max())
        _results[&#39;HD Opt Shift Peak Rate&#39;] = np.nan if maxloc.size != 1 else \
                    (np.nan if imax[maxloc] == 0 or imax[maxloc] == peakRateUpsamp.size else shiftUpsamp[imax[maxloc]][0]*1000) # in milisecond
        self.update_result(_results)

        return graph_data

    @staticmethod
    def place_field(pmap, thresh=0.2, required_neighbours=9):
        &#34;&#34;&#34;
        Calculates a mapping over the captured arena.
        For each bin in the place map, it is assigned to an integer group.
        These groups denote which neighbouring area the bin belongs to.

        Parameters
        ----------
        pmap : ndarray
            The firing map to calculate place fields from
        thresh : float
            The fraction of the peak firing that a bin must exceed
        required_neighbours : int
            The number of adjacent bins that must be together to form a field
        &#34;&#34;&#34;

        def alongColumn(pfield, ptag, J, I):
            &#34;&#34;&#34;
            Iterates along the columns of the ptags to find vertical neighbours

            Parameters
            ----------
            pfield : ndarray
                The place field map, consisting of 
                1 for groups satisying rules and 0 otherwise
            ptag : ndarray
                The place field map, grouped into tags of neighbouring areas
            J : int
                The vertical index to start searching at
            I : int
                The horizontal index to start searching at
            &#34;&#34;&#34;

            Ji = J
            Ii = I
            rows=[]
            while J+1 &lt; ptag.shape[0]:
                if not pfield[J+1, I] or ptag[J+1, I]:
                    break
                else:
                    ptag[J+1, I] = ptag[J, I]
                    rows.append(J+1)
                    J += 1
            J = Ji
            while J-1 &gt;0:
                if not pfield[J-1, I] or ptag[J-1, I]:
                    break
                else:
                    ptag[J-1, I] = ptag[J, I]
                    rows.append(J-1)
                    J-=1
            for J in rows:
                if J != Ji:
                    ptag = alongRows(pfield, ptag, J, Ii)
            return ptag

        def alongRows(pfield, ptag, J, I):
            &#34;&#34;&#34;
            Iterates along the columns of the ptags, finds horizontal neighbours

            Parameters
            ----------
            pfield : ndarray
                The place field map, consisting of 
                1 for groups satisying rules and 0 otherwise
            ptag : ndarray
                The place field map, grouped into tags of neighbouring areas
            J : int
                The vertical index to start searching at
            I : int
                The horizontal index to start searching at
            &#34;&#34;&#34;

            Ii = I
            columns=[]
            while I+1&lt;= ptag.shape[1]:
                if not pfield[J, I+1] or ptag[J, I+1]:
                    break
                else:
                    ptag[J, I+1] = ptag[J, I]
                    columns.append(I+1)
                    I += 1
            I = Ii
            while I-1 &gt;=0:
                if not pfield[J, I-1] or ptag[J, I-1]:
                    break
                else:
                    ptag[J, I-1] = ptag[J, I]
                    columns.append(I-1)
                I-=1
            for I in columns:
                if I!= Ii:
                    ptag = alongColumn(pfield, ptag, J, I)
            return ptag
        
        # Finding the place map firing field:
        # Rules to form a field: 
            # 1. There are sufficient spikes in bin 
            # 2. The bin shares a side with other bins which contain spikes
        
        # Apply Rule 1
        where_are_NaNs = np.isnan(pmap)
        pmap[where_are_NaNs] = 0
        pmap = pmap/pmap.max()
        weights = pmap
        pmap = pmap &gt; thresh

        # Pad the place field with a single layer of zeros to compare neighbours
        pfield = np.zeros(np.add(pmap.shape, 2))
        pfield[1:-1, 1:-1] = pmap

        # Apply rule 2
        has_neighbour_horizontal = np.logical_or(
            pfield[0:-2, 1:-1], pfield[2:, 1:-1])
        has_neighbour_vertical = np.logical_or(
            pfield[1:-1, 0:-2], pfield[1:-1, 2:])

        # Combine rules 1 and 2
        pfield[1:-1, 1:-1] = np.logical_and(
            pmap, 
            np.logical_or(has_neighbour_horizontal, has_neighbour_vertical))
        
        # Initialise all tags to 0
        ptag = np.zeros(pfield.shape, dtype = int)

        # Find the first non zero entry of the pfield
        J, I = find2d(pfield)
        
        #Group all the neighbouring pixels
        group = 1
        for (j, i) in zip(J, I): 
            if ptag[j, i] == 0:
                ptag[j, i] = group
                group = group + 1
                # Tag all neighbours as being of the same group
                ptag = alongColumn(pfield, ptag, j, i)
        
        # Remove the padding
        ptag = ptag[1:-1, 1:-1]

        # Find the largest field, and also remove fields that are too small
        # If there are no large enough fields, label all bins as 0
        uniques, counts = np.unique(ptag[ptag &gt; 0], return_counts=True)
        max_count, largest_group_num = 0, 0
        reduction = 0
        for unique, count in zip(uniques, counts):
            # Don&#39;t consider groups that are small
            unique = unique - reduction
            if count &lt; required_neighbours:
                ptag[ptag == unique] = 0
                ptag[ptag &gt; unique] = ptag[ptag &gt; unique] - 1
                reduction = reduction + 1
            # Define the largest group to be the one with largest weight
            # Could also be the one with the largest area
            else:
                interest_weights = weights[ptag == unique]
                weight = np.sum(interest_weights)
                if weight &gt; max_count:
                    max_count = weight
                    largest_group_num = unique

        return ptag, largest_group_num

    @staticmethod
    def place_field_centroid(pfield, fmap, group_num, **kwargs):
        &#34;&#34;&#34;
        Calculate the centroid of a place field
    
        Parameters
        ----------
        pfield : ndarray
            Input place field consisting of a map of groups
        fmap : ndarray
            Input firing map
        group_num : int
            The group to get the centroid for
        **kwargs :
            Keyword arguments
    
        Returns
        -------
        ndarray
            A list of co-ordinates for each place field group
        &#34;&#34;&#34;
        # For each group, get the list of co-ordinates from the pfield
        co_ords = np.array(np.where(pfield == group_num))
        weights = fmap[co_ords[0], co_ords[1]]
        return centre_of_mass(co_ords, weights, axis=1)
           
      
    def get_event_loc(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Calculates location of the event from its timestamps.
                                
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking or any other events
        **kwargs
            Keyword arguments
 
        Returns
        -------
        ndarray        
            Index of the events in spatial-timestamps
        [ Two item list containing
            ndarray
                x-coordinates of the event location
            ndarray
                y-ccordinates of the event location
        ]
        ndarray
            direction of the animal at the time of the event            
        &#34;&#34;&#34;
        
        
        time = self.get_time()
        lim = kwargs.get(&#39;range&#39;, [0, time.max()])

        # Sean - Why is zero idx is always thrown away?
        keep_zero_idx = kwargs.get(&#39;keep_zero_idx&#39;, False)
        
        hist = histogram(
            ftimes[np.logical_and(
                    ftimes &gt;= lim[0], ftimes &lt; lim[1])], 
            time)
        vidInd = hist[1]

        if keep_zero_idx:
            retInd = vidInd
        else:
            retInd = vidInd[vidInd != 0]
        
        return retInd, [self._pos_x[retInd], self._pos_y[retInd]], self._direction[retInd]

    def loc_shuffle(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Shuffling analysis of the unit to see if the locational firing specifity
        is by chance or actually correlated to the location of the animal
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;
        
        _results = oDict()
        graph_data = {}

        nshuff = kwargs.get(&#39;nshuff&#39;, 500)
        limit = kwargs.get(&#39;limit&#39;, 0)
        bins = kwargs.get(&#39;bins&#39;, 100)
        brAdjust = kwargs.get(&#39;brAdjust&#39;, False)
        pixel = kwargs.get(&#39;pixel&#39;, 3)
        chop_bound = kwargs.get(&#39;chop_bound&#39;, 5)
        filter = kwargs.get(&#39;filter&#39;, [&#39;b&#39;, 5])
        # limit = 0 implies enirely random shuffle, limit = &#39;x&#39; implies nshuff number of shuffles in the range [-x x]
        dur = self.get_time()[-1]
        shift = nprand.uniform(low=limit- dur, high=dur- limit, size=nshuff)
        skaggs = np.zeros((nshuff,))
        sparsity = np.zeros((nshuff,))
        coherence = np.zeros((nshuff,))
        for i in np.arange(nshuff):
            shift_ftimes = ftimes+ shift[i]
            # Wrapping up the time
            shift_ftimes[shift_ftimes &gt; dur] -= dur
            shift_ftimes[shift_ftimes &lt; 0] += dur

            placeData = self.place(shift_ftimes, filter=filter, \
                          chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)
            skaggs[i] = self.skaggs_info(placeData[&#39;fmap&#39;], placeData[&#39;tmap&#39;])
            sparsity[i] = self.spatial_sparsity(placeData[&#39;fmap&#39;], placeData[&#39;tmap&#39;])
            coherence[i] = np.corrcoef(placeData[&#39;fmap&#39;][placeData[&#39;tmap&#39;] != 0].flatten(), \
                            placeData[&#39;smoothMap&#39;][placeData[&#39;tmap&#39;] != 0].flatten())[0, 1]

        graph_data[&#39;skaggs&#39;] = skaggs
        graph_data[&#39;coherence&#39;] = coherence
        graph_data[&#39;sparsity&#39;] = sparsity

        placeData = self.place(ftimes, pixel=pixel, filter=filter, brAdjust=brAdjust,\
                              chop_bound=chop_bound, update=False)
        graph_data[&#39;refSkaggs&#39;] = self.skaggs_info(placeData[&#39;fmap&#39;], placeData[&#39;tmap&#39;])
        graph_data[&#39;refSparsity&#39;] = self.spatial_sparsity(placeData[&#39;fmap&#39;], placeData[&#39;tmap&#39;])
        graph_data[&#39;refCoherence&#39;] = np.corrcoef(placeData[&#39;fmap&#39;][placeData[&#39;tmap&#39;] != 0].flatten(), \
                            placeData[&#39;smoothMap&#39;][placeData[&#39;tmap&#39;] != 0].flatten())[0, 1]

        graph_data[&#39;skaggsCount&#39;], graph_data[&#39;skaggsEdges&#39;] = np.histogram(skaggs, bins=bins)
        graph_data[&#39;skaggs95&#39;] = np.percentile(skaggs, 95)

        graph_data[&#39;sparsityCount&#39;], graph_data[&#39;sparsityEdges&#39;] = np.histogram(sparsity, bins=bins)
        graph_data[&#39;sparsity05&#39;] = np.percentile(sparsity, 5)

        graph_data[&#39;coherenceCount&#39;], graph_data[&#39;coherenceEdges&#39;] = np.histogram(coherence, bins=bins)
        graph_data[&#39;coherence95&#39;] = np.percentile(coherence, 95)

        _results[&#39;Loc Skaggs 95&#39;] = np.percentile(skaggs, 95)
        _results[&#39;Loc Sparsity 05&#39;] = np.percentile(sparsity, 95)
        _results[&#39;Loc Coherence 95&#39;] = np.percentile(coherence, 95)

        self.update_result(_results)

        return graph_data

    def loc_shift(self, ftimes, shift_ind=np.arange(-10, 11), **kwargs):
        &#34;&#34;&#34;
        Analysis of firing specificity of the unit with respect to animal&#39;s location
        to oberve whether it represents past location of the animal or anicipates a
        future location.
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        shift_ind : ndarray
            Index of spatial resolution shift for the spike event time. Shift -1
            implies shift to the past by 1 spatial time resolution, and +2 implies
            shift to the future by 2 spatial time resoultion.
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;
        
        _results = oDict()
        graph_data = {}

        brAdjust = kwargs.get(&#39;brAdjust&#39;, False)
        pixel = kwargs.get(&#39;pixel&#39;, 3)
        chop_bound = kwargs.get(&#39;chop_bound&#39;, 5)
        _filter = kwargs.get(&#39;filter&#39;, [&#39;b&#39;, 5])

        # limit = 0 implies enirely random shuffle, limit = &#39;x&#39; implies nshuff number of shuffles in the range [-x x]
        shift = shift_ind/self.get_sampling_rate()
        shiftlen = shift.size
        dur = self.get_time()[-1]
        skaggs = np.zeros((shiftlen,))
        sparsity = np.zeros((shiftlen,))
        coherence = np.zeros((shiftlen,))

        for i in np.arange(shiftlen):
            shift_ftimes = ftimes+ shift[i]
            # Wrapping up the time
            shift_ftimes[shift_ftimes &gt; dur] -= dur
            shift_ftimes[shift_ftimes &lt; 0] += dur

            placeData = self.place(shift_ftimes, pixel=pixel, filter=_filter, \
                                  brAdjust=brAdjust, chop_bound=chop_bound, update=False)
            skaggs[i] = self.skaggs_info(placeData[&#39;fmap&#39;], placeData[&#39;tmap&#39;])
            sparsity[i] = self.spatial_sparsity(placeData[&#39;fmap&#39;], placeData[&#39;tmap&#39;])
            coherence[i] = np.corrcoef(placeData[&#39;fmap&#39;][placeData[&#39;tmap&#39;] != 0].flatten(), \
                            placeData[&#39;smoothMap&#39;][placeData[&#39;tmap&#39;] != 0].flatten())[0, 1]

        graph_data[&#39;skaggs&#39;] = skaggs
        graph_data[&#39;sparsity&#39;] = sparsity
        graph_data[&#39;coherence&#39;] = coherence

        graph_data[&#39;shiftTime&#39;] = shift

        # Find out the optimum skaggs location
        shiftUpsamp = np.arange(shift[0], shift[-1], np.mean(np.diff(shift))/4)
        skaggsUpsamp = np.interp(shiftUpsamp, shift, skaggs)
        sparsityUpsamp = np.interp(shiftUpsamp, shift, sparsity)
        coherenceUpsamp = np.interp(shiftUpsamp, shift, coherence)

        imax = sg.argrelmax(skaggsUpsamp)[0]
        maxloc = find(skaggsUpsamp[imax] == skaggsUpsamp.max())
        _results[&#39;Loc Opt Shift Skaggs&#39;] = np.nan if maxloc.size != 1 else (np.nan if imax[maxloc] == 0 or imax[maxloc] == skaggsUpsamp.size else shiftUpsamp[imax[maxloc]])

        imin = sg.argrelmin(sparsityUpsamp)[0]
        minloc = find(sparsityUpsamp[imin] == sparsityUpsamp.min())
        _results[&#39;Loc Opt Shift Sparsity&#39;] = np.nan if minloc.size != 1 else (np.nan if imin[minloc] == 0 or imin[minloc] == sparsityUpsamp.size else shiftUpsamp[imin[minloc]])

        imax = sg.argrelmax(coherenceUpsamp)[0]
        maxloc = find(coherenceUpsamp[imax] == coherenceUpsamp.max())
        _results[&#39;Loc Opt Shift Coherence&#39;] = np.nan if maxloc.size != 1 else (np.nan if imax[maxloc] == 0 or imax[maxloc] == coherenceUpsamp.size else shiftUpsamp[imax[maxloc]])

        self.update_result(_results)

        return graph_data

    def loc_auto_corr(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Calculates the two-dimensional correlation of firing map which is the
        map of the firing rate of the animal with respect to its location
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit        
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;
        graph_data = {}

        minPixel = kwargs.get(&#39;minPixel&#39;, 20)
        pixel = kwargs.get(&#39;pixel&#39;, 3)

        if &#39;update&#39; in kwargs.keys():
            del kwargs[&#39;update&#39;]
        placeData = self.place(ftimes, update=False, **kwargs)

        fmap = placeData[&#39;smoothMap&#39;]
        fmap[np.isnan(fmap)] = 0
        leny, lenx = fmap.shape

        xshift = np.arange(-(lenx-1), lenx)
        yshift = np.arange(-(leny-1), leny)

        corrMap = np.zeros((yshift.size, xshift.size))

        for J, ysh  in enumerate(yshift):
            for I, xsh in enumerate(xshift):
                if ysh &gt;= 0:
                    map1YInd = np.arange(ysh, leny)
                    map2YInd = np.arange(leny - ysh)
                elif ysh &lt; 0:
                    map1YInd = np.arange(leny + ysh)
                    map2YInd = np.arange(-ysh, leny)

                if xsh &gt;= 0:
                    map1XInd = np.arange(xsh, lenx)
                    map2XInd = np.arange(lenx - xsh)
                elif xsh &lt; 0:
                    map1XInd = np.arange(lenx + xsh)
                    map2XInd = np.arange(-xsh, lenx)
                map1 = fmap[tuple(np.meshgrid(map1YInd, map1XInd))]
                map2 = fmap[tuple(np.meshgrid(map2YInd, map2XInd))]
                if map1.size &lt; minPixel:
                    corrMap[J, I] = -1
                else:
                    corrMap[J, I] = corr_coeff(map1, map2)

        graph_data[&#39;corrMap&#39;] = corrMap
        graph_data[&#39;xshift&#39;] = xshift*pixel
        graph_data[&#39;yshift&#39;] = yshift*pixel

        return graph_data

    def loc_rot_corr(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Calculates the rotational correlation of the locational firing rate of the animal with
        respect to location, also called firing map    
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit        
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
    
        &#34;&#34;&#34;    
        
        graph_data = {}

        binsize = kwargs.get(&#39;binsize&#39;, 3) #degrees
#        filttype, filtsize = kwargs.get(&#39;filter&#39;, [&#39;b&#39;, 3])

        bins = np.arange(0, 360, binsize)
        placeData = self.place(ftimes, update=False, **kwargs)

        fmap = placeData[&#39;smoothMap&#39;]
        fmap[np.isnan(fmap)] = 0

        rotCorr = [corr_coeff(rot_2d(fmap, theta), fmap) for k, theta in enumerate(bins)]

        graph_data[&#39;rotAngle&#39;] = bins
        graph_data[&#39;rotCorr&#39;] = rotCorr

        return graph_data

    def border(self, ftimes, **kwargs):        
        &#34;&#34;&#34;
        Analysis of the firing characteristic of a unit with respect to the
        environmental border
                
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit        
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
    
        &#34;&#34;&#34;
                
        _results = oDict()
        graph_data = {}

        dist, xedges, yedges, distMat = self.get_border()
        pixel = np.diff(xedges).mean()

        update = kwargs.get(&#39;update&#39;, True)
        thresh = kwargs.get(&#39;thresh&#39;, 0.2)
        cbinsize = kwargs.get(&#39;cbinsize&#39;, 5) # Circular binsize in degrees
        lim = kwargs.get(&#39;range&#39;, [0, self.get_duration()])

        steps = kwargs.get(&#39;nstep&#39;, 5)

        distBins = np.arange(dist.min(), dist.max() + pixel, pixel)

        if &#39;update&#39; in kwargs.keys():
            del kwargs[&#39;update&#39;]

        placeData = self.place(ftimes, range=lim, update=False, **kwargs)
        fmap = placeData[&#39;smoothMap&#39;]

        xind = np.array([])
        yind = np.array([])
        if placeData[&#39;xedges&#39;].max() &lt; xedges.max():
            xind = xedges &lt;= placeData[&#39;xedges&#39;].max()
            xedges = xedges[xind]
        if placeData[&#39;yedges&#39;].max() &lt; yedges.max():
            yind = yedges &lt;= placeData[&#39;yedges&#39;].max()
            yedges = yedges[xind]

        if xind.any():
            distMat = distMat[:, xind]
        if yind.any():
            distMat = distMat[yind, :]

        nanInd = np.isnan(fmap)
        fmap[nanInd] = 0

        smoothRate = np.zeros(distBins.shape) # Calculated from smooth FR map not by smoothing from raw rate
        for i, edge in enumerate(distBins):
            edge_ind = distMat == edge
            if edge_ind.any() and np.logical_and(np.logical_not(nanInd), distMat == edge).any():
                smoothRate[i] = fmap[np.logical_and(np.logical_not(nanInd), distMat == edge)].mean()
#        smoothRate = smooth_1d(smoothRate, filttype, filtsize)

        fmap /= fmap.max()

        tcount = histogram(dist, distBins)[0]

        tcount = tcount/ self.get_sampling_rate()

        spikeDist = dist[self.get_event_loc(ftimes)[0]]
        spike_count = histogram(spikeDist, distBins)[0].astype(tcount.dtype)

        distRate = np.divide(spike_count, tcount, out=np.zeros_like(spike_count),\
                            where=tcount != 0, casting=&#39;unsafe&#39;) # for skaggs only
        pixelCount = histogram(distMat[np.logical_not(nanInd)], distBins)[0]
        distCount = np.divide(histogram(distMat[fmap &gt;= thresh], distBins)[0], pixelCount, \
                             out=np.zeros_like(distBins), where=pixelCount != 0, casting=&#39;unsafe&#39;)

        circBins = np.arange(0, 360, cbinsize)

        X, Y = np.meshgrid(xedges, np.flipud(yedges))
        X = X- xedges[-1]/2
        Y = Y- yedges[-1]/2
        angDist = np.arctan2(Y, X)* 180/np.pi
        angDist[angDist &lt; 0] += 360

        meanDist = distMat[fmap &gt;= thresh].mean()

        cs = CircStat()
        cs.set_theta(angDist[np.logical_and(distMat &lt;= meanDist, fmap &gt;= thresh)])
        angDistCount = cs.circ_histogram(circBins)[0]

        # Circular linear map
        circLinMap = np.zeros((distBins.size, circBins.size))

        for i, edge in enumerate(distBins):
            cs.set_theta(angDist[np.logical_and(distMat == edge, fmap &gt;= thresh)])
            circLinMap[i, :] = cs.circ_histogram(circBins)[0]

        perSteps = np.arange(0, 1, 1/steps)
        perDist = np.zeros(steps)

        for i in np.arange(steps):
            perDist[i] = distMat[np.logical_and(np.logical_not(nanInd), \
                        np.logical_and(fmap &gt;= perSteps[i], fmap &lt; perSteps[i]+ 1/steps))].mean()
        if update:
            _results[&#39;Border Skaggs&#39;] = self.skaggs_info(distRate, tcount)

            angDistExt = np.append(angDistCount, angDistCount)

            segsize = find_chunk(angDistExt &gt; 0)[0]
            _results[&#39;Border Ang Ext&#39;] = max(segsize)*cbinsize

            cBinsInterp = np.arange(0, 360, 0.1)
            dBinsInterp = np.arange(0, distBins[-1] + pixel, 0.1)
            graph_data[&#39;cBinsInterp&#39;] = cBinsInterp
            graph_data[&#39;dBinsInterp&#39;] = dBinsInterp
            graph_data[&#39;circLinMap&#39;] = sc.interpolate.interp2d(circBins, distBins, circLinMap, kind=&#39;cubic&#39;)(cBinsInterp, dBinsInterp)

            self.update_result(_results)

        graph_data[&#39;distBins&#39;] = distBins
        graph_data[&#39;distCount&#39;] = distCount
        graph_data[&#39;circBins&#39;] = circBins
        graph_data[&#39;angDistCount&#39;] = angDistCount
        graph_data[&#39;distRate&#39;] = distRate
        graph_data[&#39;smoothRate&#39;] = smoothRate
        graph_data[&#39;perSteps&#39;] = perSteps*100
        graph_data[&#39;perDist&#39;] = perDist

        return graph_data

    def gradient(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Analysis of gradient cell, a unit whose firing rate gradually increases 
        as the animal traverses from the border to the cneter of the environment
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit        
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
        &#34;&#34;&#34;
        
        _results = oDict()
        graph_data = {}

        alim = kwargs.get(&#39;alim&#39;, 0.25)
        blim = kwargs.get(&#39;blim&#39;, 0.25)
        clim = kwargs.get(&#39;clim&#39;, 0.5)

        graph_data = self.border(ftimes, **kwargs)

        x = graph_data[&#39;distBins&#39;]
        y = graph_data[&#39;smoothRate&#39;]
        x = x[np.isfinite(y)]
        y = y[np.isfinite(y)]
        y = np.log(y, out=np.zeros_like(y), where=y != 0, casting=&#39;unsafe&#39;)
        ai = y.max()
        y0 = y[x == 0]
        bi = ai- y0

        d_half = x.mean()
        for i, dist in enumerate(x):
            if i &lt; x.size-1 and (y0+ bi/2) &gt; y[i] and (y0+ bi/2) &lt;= y[i+1]:
                d_half = x[i:i+2].mean()

        ci = np.log(2)/d_half

        def fit_func(x, a, b, c):
            return a- b*np.exp(-c*x)

        popt, pcov = curve_fit(fit_func, x, y, \
                                p0=[ai, bi, ci], \
                                bounds=([(1- alim)*ai, (1- blim)*bi, (1- clim)*ci], \
                                [(1+ alim)*ai, (1+ blim)*bi, (1+ clim)*ci]), \
                                max_nfev=100000)
        a, b, c = popt

        y_fit = fit_func(x, *popt)


        gof = residual_stat(y, y_fit, 3)
        rateFit = np.exp(y_fit)

        graph_data[&#39;distBins&#39;] = x
#        graph_data[&#39;smoothRate&#39;] = y
        graph_data[&#39;rateFit&#39;] = rateFit
        graph_data[&#39;diffRate&#39;] = b*c*np.multiply(rateFit, np.exp(-c*x))

        _results[&#39;Grad Pearse R&#39;] = gof[&#39;Pearson R&#39;]
        _results[&#39;Grad Pearse P&#39;] = gof[&#39;Pearson P&#39;]
        _results[&#39;Grad adj Rsq&#39;] = gof[&#39;adj Rsq&#39;]
        _results[&#39;Grad Max Growth Rate&#39;] = c*np.exp(a-1)
        _results[&#39;Grad Inflect Dist&#39;] = np.log(b)/c

        self.update_result(_results)
        return graph_data

    def grid(self, ftimes, **kwargs):        
        &#34;&#34;&#34;
        Analysis of Grid cells characterised by formation of grid-like pattern
        of high activity in the firing-rate map        
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit   
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
    
        &#34;&#34;&#34;
        
        _results = oDict()
        tol = kwargs.get(&#39;angtol&#39;, 2)
        binsize = kwargs.get(&#39;binsize&#39;, 3)
        bins = np.arange(0, 360, binsize)

        graph_data = self.loc_auto_corr(ftimes, update=False, **kwargs)
        corrMap = graph_data[&#39;corrMap&#39;]
        corrMap[np.isnan(corrMap)] = 0
        xshift = graph_data[&#39;xshift&#39;]
        yshift = graph_data[&#39;yshift&#39;]

        pixel = np.int(np.diff(xshift).mean())

        ny, nx = corrMap.shape
        rpeaks = np.zeros(corrMap.shape, dtype=bool)
        cpeaks = np.zeros(corrMap.shape, dtype=bool)
        for j in np.arange(ny):
            rpeaks[j, extrema(corrMap[j, :])[1]] = True
        for i in np.arange(nx):
            cpeaks[extrema(corrMap[:, i])[1], i] = True
        ymax, xmax = find2d(np.logical_and(rpeaks, cpeaks))

        peakDist = np.sqrt((ymax- find(yshift == 0))**2+ (xmax- find(xshift == 0))**2)
        sortInd = np.argsort(peakDist)
        ymax, xmax, peakDist = ymax[sortInd], xmax[sortInd], peakDist[sortInd]

        ymax, xmax, peakDist = (ymax[1:7], xmax[1:7], peakDist[1:7]) if ymax.size &gt;= 7 else ([], [], [])
        theta = np.arctan2(yshift[ymax], xshift[xmax])*180/np.pi
        theta[theta &lt; 0] += 360
        sortInd = np.argsort(theta)
        ymax, xmax, peakDist, theta = (ymax[sortInd], xmax[sortInd], peakDist[sortInd], theta[sortInd])

        graph_data[&#39;ymax&#39;] = yshift[ymax]
        graph_data[&#39;xmax&#39;] = xshift[xmax]

        meanDist = peakDist.mean()
        X, Y = np.meshgrid(xshift, yshift)
        distMat = np.sqrt(X**2 + Y**2)/pixel

        if len(ymax) == np.logical_and(peakDist &gt; 0.75*meanDist, peakDist &lt; 1.25*meanDist).sum(): # if all of them are within tolerance(25%)
            maskInd = np.logical_and(distMat &gt; 0.5*meanDist, distMat &lt; 1.5*meanDist)
            rotCorr = np.array([corr_coeff(rot_2d(corrMap, theta)[maskInd], corrMap[maskInd]) for k, theta in enumerate(bins)])
            ramax, rimax, ramin, rimin = extrema(rotCorr)
            mThetaPk, mThetaTr = (np.diff(bins[rimax]).mean(), np.diff(bins[rimin]).mean()) if rimax.size and rimin.size else (None, None)
            graph_data[&#39;rimax&#39;] = rimax
            graph_data[&#39;rimin&#39;] = rimin
            graph_data[&#39;anglemax&#39;] = bins[rimax]
            graph_data[&#39;anglemin&#39;] = bins[rimin]
            graph_data[&#39;rotAngle&#39;] = bins
            graph_data[&#39;rotCorr&#39;] = rotCorr

            if mThetaPk is not None and mThetaTr is not None:
                isGrid = True if 60 - tol &lt; mThetaPk &lt; 60 + tol and 60 - tol &lt; mThetaTr &lt; 60 + tol else False
            else:
                isGrid = False

            meanAlpha = np.diff(theta).mean()
            psi = theta[np.array([2, 3, 4, 5, 0, 1])]- theta
            psi[psi &lt; 0] += 360
            meanPsi = psi.mean()

            _results[&#39;Is Grid&#39;] = isGrid and 120 - tol &lt; meanPsi &lt; 120 + tol and 60 - tol &lt; meanAlpha &lt; 60 + tol
            _results[&#39;Grid Mean Alpha&#39;] = meanAlpha
            _results[&#39;Grid Mean Psi&#39;] = meanPsi
            _results[&#39;Grid Spacing&#39;] = meanDist*pixel
            _results[&#39;Grid Score&#39;] = rotCorr[rimax].max()- rotCorr[rimin].min() # Difference between highest Pearson R at peaks and lowest at troughs
            _results[&#39;Grid Orientation&#39;] = theta[0]

        else:
            _results[&#39;Is Grid&#39;] = False

        self.update_result(_results)
        return graph_data

    def multiple_regression(self, ftimes, **kwargs):        
        &#34;&#34;&#34;
        Multiple-rgression analysis where firing rate for each variable, namely
        location, head-direction, speed, AHV, and distance from border, are used
        to regress the instantaneous firing rate of the unit.
                
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit   
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;
        
        _results = oDict()
        graph_data = oDict()
        subsampInterv = kwargs.get(&#39;subsampInterv&#39;, 0.1)
        episode = kwargs.get(&#39;episode&#39;, 120)
        nrep = kwargs.get(&#39;nrep&#39;, 1000)
        sampRate = 1/subsampInterv
        stamp = subsampInterv
 
        a_size = np.round(self.get_duration(), 4)
        time = np.linspace(
            0, a_size, endpoint=True,
            num=np.round(a_size/stamp)+1)
        time = np.round(time, 4)
        Y = histogram(ftimes, time)[0]* sampRate # Instant firing rate

        nt = time.size
        xloc, yloc, loc, hd, speed, ang_vel, distBorder = list(np.zeros((7, nt)))
        tmp = self.place(ftimes)
        placeRate, xedges, yedges = (tmp[&#39;smoothMap&#39;], tmp[&#39;xedges&#39;], tmp[&#39;yedges&#39;])
        placeRate[np.isnan(placeRate)] = 0
        for i in np.arange(nt):
            ind = find(np.logical_and(self.get_time() &gt;= time[i], self.get_time() &lt; time[i]+ stamp))
            if len(ind) == 0:
                continue
            xloc[i] = np.median(self.get_pos_x()[ind])
            yloc[i] = np.median(self.get_pos_y()[ind])
            if histogram(yloc[i], yedges)[1] &lt; yedges.size and histogram(xloc[i], xedges)[1] &lt; xedges.size:
                loc[i] = placeRate[histogram(yloc[i], yedges)[1], histogram(xloc[i], xedges)[1]]
            hd[i] = np.median(self.get_direction()[ind])
            speed[i] = np.median(self.get_speed()[ind])
            ang_vel[i] = np.median(self.get_ang_vel()[ind])
            distBorder[i] = np.median(self.get_border()[0][ind])

        tmp = self.hd_rate(ftimes, update=False)
        hd_rate, hdBins = (tmp[&#39;hdRate&#39;], tmp[&#39;bins&#39;])
        cs = CircStat()
        cs.set_theta(hd)
        hd = hd_rate[cs.circ_histogram(hdBins)[1]] # replaced by corresponding rate
        # Speed+ ang_vel will be linearly modelled, so no transformation required; ang_vel will be replaced by the non-linear rate
        tmp = self.border(ftimes, update=False)
        borderRate, borderBins = (tmp[&#39;distRate&#39;], tmp[&#39;distBins&#39;])
        distBorder = borderRate[histogram(distBorder, borderBins)[1]] # replaced by corresponding rate

        ns = int(episode/stamp) # row to select in random

        X = np.vstack((loc, hd, speed, ang_vel, distBorder)).transpose()
        lm = LinearRegression(fit_intercept=True, normalize=True)

        Rsq = np.zeros((nrep, 6))
        for i in np.arange(nrep):
            ind = np.random.permutation(time.size)[:ns]
            lm.fit(X[ind, :], Y[ind])
            Rsq[i, 0] = lm.score(X[ind, :], Y[ind])
            for j in np.arange(5):
                varind = np.array([k for k in range(5) if k != j])
                lm.fit(X[np.ix_(ind, varind)], Y[ind]) #np.ix_ is used for braodcasting the index arrays
                Rsq[i, j+1] = Rsq[i, 0]- lm.score(X[np.ix_(ind, varind)], Y[ind])

        meanRsq = Rsq.mean(axis=0)
        # Regresssion parameters are alays stored in following order
        varOrder = [&#39;Total&#39;, &#39;Loc&#39;, &#39;HD&#39;, &#39;Speed&#39;, &#39;Ang Vel&#39;, &#39;Dist Border&#39;]

#        graph_data[&#39;order&#39;] = varOrder
        graph_data[&#39;Rsq&#39;] = Rsq
        graph_data[&#39;meanRsq&#39;] = meanRsq
        graph_data[&#39;maxRsq&#39;] = Rsq.max(axis=0)
        graph_data[&#39;minRsq&#39;] = Rsq.min(axis=0)
        graph_data[&#39;stdRsq&#39;] = Rsq.std(axis=0)
        
        _results[&#39;Mult Rsq&#39;] = meanRsq[0]
        for i, key in enumerate(varOrder):
            if i &gt; 0:
                _results[&#39;Semi Rsq &#39;+key] = meanRsq[i]
        self.update_result(_results)

        return graph_data

    def interdependence(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Interdependence analysis where firing rate of each variable is predicted
        from another variable and the distributive ratio is measured between the
        predicted firing rate and the caclulated firing rate.
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit
        **kwargs
            Keyword arguments
 
        Returns
        -------
        None
        
        &#34;&#34;&#34;        

        _results = oDict()
        pixel = kwargs.get(&#39;pixel&#39;, 3)
        hdbinsize = kwargs.get(&#39;hdbinsize&#39;, 5)
        spbinsize = kwargs.get(&#39;spbinsize&#39;, 1)
        sprange = kwargs.get(&#39;sprange&#39;, [0, 40])
        abinsize = kwargs.get(&#39;abinsize&#39;, 10)
        ang_velrange = kwargs.get(&#39;ang_velrange&#39;, [-500, 500])

        placeData = self.place(ftimes, pixel=pixel, update=False)
        fmap = placeData[&#39;smoothMap&#39;]
        fmap[np.isnan(fmap)] = 0
        xloc = self.get_pos_x()
        yloc = self.get_pos_y()
        xedges = placeData[&#39;xedges&#39;]
        yedges = placeData[&#39;yedges&#39;]

        hdData = self.hd_rate(ftimes, binsize=hdbinsize, update=False)
        bins = hdData[&#39;bins&#39;]
        predRate = np.zeros(bins.size)
        for i, b in enumerate(bins):
            ind = np.logical_and(hdData[&#39;hd&#39;] &gt;= b, hdData[&#39;hd&#39;] &lt; b + hdbinsize)
            tmap = histogram2d(yloc[ind], xloc[ind], yedges, xedges)[0]
            tmap /= self.get_sampling_rate()
            predRate[i] = np.sum(fmap*tmap)/ tmap.sum()
        _results[&#39;DR HP&#39;] = np.abs(np.log((1 + hdData[&#39;smoothRate&#39;])/ (1 + predRate))).sum()/bins.size

        spData = self.speed(ftimes, binsize=spbinsize, range=sprange, update=False)
        bins = spData[&#39;bins&#39;]
        predRate = np.zeros(bins.size)
        speed = self.get_speed()
        for i, b in enumerate(bins):
            ind = np.logical_and(speed &gt;= b, speed &lt; b + spbinsize)
            tmap = histogram2d(yloc[ind], xloc[ind], yedges, xedges)[0]
            tmap /= self.get_sampling_rate()
            predRate[i] = np.sum(fmap*tmap)/ tmap.sum()
        _results[&#39;DR SP&#39;] = np.abs(np.log((1 + spData[&#39;rate&#39;])/ (1 + predRate))).sum()/bins.size

        ang_velData = self.angular_velocity(ftimes, binsize=abinsize, range=ang_velrange, update=False)
        bins = np.hstack((ang_velData[&#39;leftBins&#39;], ang_velData[&#39;rightBins&#39;]))
        predRate = np.zeros(bins.size)
        ang_vel = self.get_ang_vel()
        for i, b in enumerate(bins):
            ind = np.logical_and(ang_vel &gt;= b, ang_vel &lt; b + abinsize)
            tmap = histogram2d(yloc[ind], xloc[ind], yedges, xedges)[0]
            tmap /= self.get_sampling_rate()
            predRate[i] = np.sum(fmap*tmap)/ tmap.sum()
        ang_velObs = np.hstack((ang_velData[&#39;leftRate&#39;], ang_velData[&#39;rightRate&#39;]))
        _results[&#39;DR AP&#39;] = np.abs(np.log((1 + ang_velObs)/ (1 + predRate))).sum()/bins.size

        borderData = self.border(ftimes, update=False)
        bins = borderData[&#39;distBins&#39;]
        dbinsize = np.diff(bins).mean()
        predRate = np.zeros(bins.size)
        border = self.get_border()[0]
        for i, b in enumerate(bins):
            ind = np.logical_and(border &gt;= b, border &lt; b + dbinsize)
            tmap = histogram2d(yloc[ind], xloc[ind], yedges, xedges)[0]
            tmap /= self.get_sampling_rate()
            predRate[i] = np.sum(fmap*tmap)/ tmap.sum()
        _results[&#39;DR BP&#39;] = np.abs(np.log((1 + borderData[&#39;distRate&#39;]) / (1 + predRate))).sum()/bins.size

        self.update_result(_results)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="neurochat.nc_base.NAbstract" href="nc_base.html#neurochat.nc_base.NAbstract">NAbstract</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="neurochat.nc_spatial.NSpatial.place_field"><code class="name flex">
<span>def <span class="ident">place_field</span></span>(<span>pmap, thresh=0.2, required_neighbours=9)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates a mapping over the captured arena.
For each bin in the place map, it is assigned to an integer group.
These groups denote which neighbouring area the bin belongs to.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>pmap</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>The firing map to calculate place fields from</dd>
<dt><strong><code>thresh</code></strong> :&ensp;<code>float</code></dt>
<dd>The fraction of the peak firing that a bin must exceed</dd>
<dt><strong><code>required_neighbours</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of adjacent bins that must be together to form a field</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@staticmethod
def place_field(pmap, thresh=0.2, required_neighbours=9):
    &#34;&#34;&#34;
    Calculates a mapping over the captured arena.
    For each bin in the place map, it is assigned to an integer group.
    These groups denote which neighbouring area the bin belongs to.

    Parameters
    ----------
    pmap : ndarray
        The firing map to calculate place fields from
    thresh : float
        The fraction of the peak firing that a bin must exceed
    required_neighbours : int
        The number of adjacent bins that must be together to form a field
    &#34;&#34;&#34;

    def alongColumn(pfield, ptag, J, I):
        &#34;&#34;&#34;
        Iterates along the columns of the ptags to find vertical neighbours

        Parameters
        ----------
        pfield : ndarray
            The place field map, consisting of 
            1 for groups satisying rules and 0 otherwise
        ptag : ndarray
            The place field map, grouped into tags of neighbouring areas
        J : int
            The vertical index to start searching at
        I : int
            The horizontal index to start searching at
        &#34;&#34;&#34;

        Ji = J
        Ii = I
        rows=[]
        while J+1 &lt; ptag.shape[0]:
            if not pfield[J+1, I] or ptag[J+1, I]:
                break
            else:
                ptag[J+1, I] = ptag[J, I]
                rows.append(J+1)
                J += 1
        J = Ji
        while J-1 &gt;0:
            if not pfield[J-1, I] or ptag[J-1, I]:
                break
            else:
                ptag[J-1, I] = ptag[J, I]
                rows.append(J-1)
                J-=1
        for J in rows:
            if J != Ji:
                ptag = alongRows(pfield, ptag, J, Ii)
        return ptag

    def alongRows(pfield, ptag, J, I):
        &#34;&#34;&#34;
        Iterates along the columns of the ptags, finds horizontal neighbours

        Parameters
        ----------
        pfield : ndarray
            The place field map, consisting of 
            1 for groups satisying rules and 0 otherwise
        ptag : ndarray
            The place field map, grouped into tags of neighbouring areas
        J : int
            The vertical index to start searching at
        I : int
            The horizontal index to start searching at
        &#34;&#34;&#34;

        Ii = I
        columns=[]
        while I+1&lt;= ptag.shape[1]:
            if not pfield[J, I+1] or ptag[J, I+1]:
                break
            else:
                ptag[J, I+1] = ptag[J, I]
                columns.append(I+1)
                I += 1
        I = Ii
        while I-1 &gt;=0:
            if not pfield[J, I-1] or ptag[J, I-1]:
                break
            else:
                ptag[J, I-1] = ptag[J, I]
                columns.append(I-1)
            I-=1
        for I in columns:
            if I!= Ii:
                ptag = alongColumn(pfield, ptag, J, I)
        return ptag
    
    # Finding the place map firing field:
    # Rules to form a field: 
        # 1. There are sufficient spikes in bin 
        # 2. The bin shares a side with other bins which contain spikes
    
    # Apply Rule 1
    where_are_NaNs = np.isnan(pmap)
    pmap[where_are_NaNs] = 0
    pmap = pmap/pmap.max()
    weights = pmap
    pmap = pmap &gt; thresh

    # Pad the place field with a single layer of zeros to compare neighbours
    pfield = np.zeros(np.add(pmap.shape, 2))
    pfield[1:-1, 1:-1] = pmap

    # Apply rule 2
    has_neighbour_horizontal = np.logical_or(
        pfield[0:-2, 1:-1], pfield[2:, 1:-1])
    has_neighbour_vertical = np.logical_or(
        pfield[1:-1, 0:-2], pfield[1:-1, 2:])

    # Combine rules 1 and 2
    pfield[1:-1, 1:-1] = np.logical_and(
        pmap, 
        np.logical_or(has_neighbour_horizontal, has_neighbour_vertical))
    
    # Initialise all tags to 0
    ptag = np.zeros(pfield.shape, dtype = int)

    # Find the first non zero entry of the pfield
    J, I = find2d(pfield)
    
    #Group all the neighbouring pixels
    group = 1
    for (j, i) in zip(J, I): 
        if ptag[j, i] == 0:
            ptag[j, i] = group
            group = group + 1
            # Tag all neighbours as being of the same group
            ptag = alongColumn(pfield, ptag, j, i)
    
    # Remove the padding
    ptag = ptag[1:-1, 1:-1]

    # Find the largest field, and also remove fields that are too small
    # If there are no large enough fields, label all bins as 0
    uniques, counts = np.unique(ptag[ptag &gt; 0], return_counts=True)
    max_count, largest_group_num = 0, 0
    reduction = 0
    for unique, count in zip(uniques, counts):
        # Don&#39;t consider groups that are small
        unique = unique - reduction
        if count &lt; required_neighbours:
            ptag[ptag == unique] = 0
            ptag[ptag &gt; unique] = ptag[ptag &gt; unique] - 1
            reduction = reduction + 1
        # Define the largest group to be the one with largest weight
        # Could also be the one with the largest area
        else:
            interest_weights = weights[ptag == unique]
            weight = np.sum(interest_weights)
            if weight &gt; max_count:
                max_count = weight
                largest_group_num = unique

    return ptag, largest_group_num</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.place_field_centroid"><code class="name flex">
<span>def <span class="ident">place_field_centroid</span></span>(<span>pfield, fmap, group_num, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculate the centroid of a place field</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>pfield</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Input place field consisting of a map of groups</dd>
<dt><strong><code>fmap</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Input firing map</dd>
<dt><strong><code>group_num</code></strong> :&ensp;<code>int</code></dt>
<dd>The group to get the centroid for</dd>
</dl>
<p>**kwargs :
Keyword arguments</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>A list of co-ordinates for each place field group</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@staticmethod
def place_field_centroid(pfield, fmap, group_num, **kwargs):
    &#34;&#34;&#34;
    Calculate the centroid of a place field

    Parameters
    ----------
    pfield : ndarray
        Input place field consisting of a map of groups
    fmap : ndarray
        Input firing map
    group_num : int
        The group to get the centroid for
    **kwargs :
        Keyword arguments

    Returns
    -------
    ndarray
        A list of co-ordinates for each place field group
    &#34;&#34;&#34;
    # For each group, get the list of co-ordinates from the pfield
    co_ords = np.array(np.where(pfield == group_num))
    weights = fmap[co_ords[0], co_ords[1]]
    return centre_of_mass(co_ords, weights, axis=1)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.skaggs_info"><code class="name flex">
<span>def <span class="ident">skaggs_info</span></span>(<span>firing_rate, visit_time)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates the Skaggs information content of the spatial firing</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>firing_rate</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Firing rate of the unit at each pixelated location or binned information,
i.e., binned speed or head-direction</dd>
<dt><strong><code>visit_time</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Amount of time animal spent in each pixel or bin</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>Skaggs information content</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@staticmethod
def skaggs_info(firing_rate, visit_time):
    &#34;&#34;&#34;
    Calculates the Skaggs information content of the spatial firing
    
    Parameters
    ----------
    firing_rate : ndarray
        Firing rate of the unit at each pixelated location or binned information,
        i.e., binned speed or head-direction            
    visit_time : ndarray
        Amount of time animal spent in each pixel or bin
    
    Returns
    -------
    float
        Skaggs information content
    
    &#34;&#34;&#34;
    
    firing_rate[np.isnan(firing_rate)] = 0
    Li = firing_rate # Lambda
    L = np.sum(firing_rate*visit_time)/ visit_time.sum()
    P = visit_time/visit_time.sum()
    
    return np.sum(P[Li &gt; 0]*(Li[Li &gt; 0]/L)*np.log2(Li[Li &gt; 0]/L))</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.spatial_sparsity"><code class="name flex">
<span>def <span class="ident">spatial_sparsity</span></span>(<span>firing_rate, visit_time)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates the spatial sparsity of the spatial firing</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>firing_rate</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Firing rate of the unit at each pixelated location</dd>
<dt><strong><code>visit_time</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Amount of time animal spent in each pixel</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>Spatial sparsity</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@staticmethod
def spatial_sparsity(firing_rate, visit_time):
    &#34;&#34;&#34;
    Calculates the spatial sparsity of the spatial firing
    
    Parameters
    ----------
    firing_rate : ndarray
        Firing rate of the unit at each pixelated location  
    visit_time : ndarray
        Amount of time animal spent in each pixel
    
    Returns
    -------
    float
        Spatial sparsity
    
    &#34;&#34;&#34;
    
    firing_rate[np.isnan(firing_rate)] = 0
    Li = firing_rate # Lambda
    # L = np.sum(firing_rate*visit_time)/ visit_time.sum()
    P = visit_time/visit_time.sum()
    return np.sum(P*Li)**2/ np.sum(P*Li**2)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="neurochat.nc_spatial.NSpatial.angular_velocity"><code class="name flex">
<span>def <span class="ident">angular_velocity</span></span>(<span>self, ftimes, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates the firing rate of the unit at different binned angular head velocity.</p>
<p>The spike rate vs speed is fitted with a linear equation individually
for the negative and positive angular velocities, and goodness of fit
is measured</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ftimes</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Timestamps of the spiking activity of a unit</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def angular_velocity(self, ftimes, **kwargs):
    &#34;&#34;&#34;
    Calculates the firing rate of the unit at different binned angular head velocity.
    
    The spike rate vs speed is fitted with a linear equation individually
    for the negative and positive angular velocities, and goodness of fit
    is measured
    
    Parameters
    ----------
    ftimes : ndarray
        Timestamps of the spiking activity of a unit
    **kwargs
        Keyword arguments

    Returns
    -------
    dict
        Graphical data of the analysis
    &#34;&#34;&#34;
    
    _results = oDict()
    graph_data = {}
    update = kwargs.get(&#39;update&#39;, True) # When update = True, it will use the
                                        #results for statistics, if False,
                                        #i.e. in Multiple Regression, it will ignore updating
    binsize = kwargs.get(&#39;binsize&#39;, 10)
    min_vel, max_vel = kwargs.get(&#39;range&#39;, [-100, 100])
    cutoff = kwargs.get(&#39;cutoff&#39;, 10)

    ang_vel = self.get_ang_vel()

    max_vel = min(max_vel, np.ceil(ang_vel.max()/binsize)*binsize)
    min_vel = max(min_vel, np.floor(ang_vel.min()/binsize)*binsize)
    bins = np.arange(min_vel, max_vel, binsize)

    vid_count = histogram(ftimes, self.get_time())[0]
    visit_time, velInd = histogram(ang_vel, bins)[0:2]
    visit_time = visit_time/self.get_sampling_rate()

    rate = np.array([sum(vid_count[velInd == i]) for i in range(len(bins))])/ visit_time
    rate[np.isnan(rate)] = 0

    _results[&#39;speedSkaggs&#39;] = self.skaggs_info(rate, visit_time)

    rate = rate[visit_time &gt; 1]
    bins = bins[visit_time &gt; 1]


    fit_result = linfit(bins[bins &lt;= -cutoff], rate[bins &lt;= -cutoff])

    _results[&#39;Ang Vel Left Pears R&#39;] = fit_result[&#39;Pearson R&#39;]
    _results[&#39;Ang Vel Left Pears P&#39;] = fit_result[&#39;Pearson P&#39;]
    graph_data[&#39;leftBins&#39;] = bins[bins &lt;= -cutoff]
    graph_data[&#39;leftRate&#39;] = rate[bins &lt;= -cutoff]
    graph_data[&#39;leftFitRate&#39;] = fit_result[&#39;yfit&#39;]

    fit_result = linfit(bins[bins &gt;= cutoff], rate[bins &gt;= cutoff])

    _results[&#39;Ang Vel Right Pears R&#39;] = fit_result[&#39;Pearson R&#39;]
    _results[&#39;Ang Vel Right Pears P&#39;] = fit_result[&#39;Pearson P&#39;]
    graph_data[&#39;rightBins&#39;] = bins[bins &gt;= cutoff]
    graph_data[&#39;rightRate&#39;] = rate[bins &gt;= cutoff]
    graph_data[&#39;rightFitRate&#39;] = fit_result[&#39;yfit&#39;]

    if update:
        self.update_result(_results)
    return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.border"><code class="name flex">
<span>def <span class="ident">border</span></span>(<span>self, ftimes, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Analysis of the firing characteristic of a unit with respect to the
environmental border</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ftimes</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Timestamps of the spiking activity of a unit</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">    def border(self, ftimes, **kwargs):        
        &#34;&#34;&#34;
        Analysis of the firing characteristic of a unit with respect to the
        environmental border
                
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit        
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
    
        &#34;&#34;&#34;
                
        _results = oDict()
        graph_data = {}

        dist, xedges, yedges, distMat = self.get_border()
        pixel = np.diff(xedges).mean()

        update = kwargs.get(&#39;update&#39;, True)
        thresh = kwargs.get(&#39;thresh&#39;, 0.2)
        cbinsize = kwargs.get(&#39;cbinsize&#39;, 5) # Circular binsize in degrees
        lim = kwargs.get(&#39;range&#39;, [0, self.get_duration()])

        steps = kwargs.get(&#39;nstep&#39;, 5)

        distBins = np.arange(dist.min(), dist.max() + pixel, pixel)

        if &#39;update&#39; in kwargs.keys():
            del kwargs[&#39;update&#39;]

        placeData = self.place(ftimes, range=lim, update=False, **kwargs)
        fmap = placeData[&#39;smoothMap&#39;]

        xind = np.array([])
        yind = np.array([])
        if placeData[&#39;xedges&#39;].max() &lt; xedges.max():
            xind = xedges &lt;= placeData[&#39;xedges&#39;].max()
            xedges = xedges[xind]
        if placeData[&#39;yedges&#39;].max() &lt; yedges.max():
            yind = yedges &lt;= placeData[&#39;yedges&#39;].max()
            yedges = yedges[xind]

        if xind.any():
            distMat = distMat[:, xind]
        if yind.any():
            distMat = distMat[yind, :]

        nanInd = np.isnan(fmap)
        fmap[nanInd] = 0

        smoothRate = np.zeros(distBins.shape) # Calculated from smooth FR map not by smoothing from raw rate
        for i, edge in enumerate(distBins):
            edge_ind = distMat == edge
            if edge_ind.any() and np.logical_and(np.logical_not(nanInd), distMat == edge).any():
                smoothRate[i] = fmap[np.logical_and(np.logical_not(nanInd), distMat == edge)].mean()
#        smoothRate = smooth_1d(smoothRate, filttype, filtsize)

        fmap /= fmap.max()

        tcount = histogram(dist, distBins)[0]

        tcount = tcount/ self.get_sampling_rate()

        spikeDist = dist[self.get_event_loc(ftimes)[0]]
        spike_count = histogram(spikeDist, distBins)[0].astype(tcount.dtype)

        distRate = np.divide(spike_count, tcount, out=np.zeros_like(spike_count),\
                            where=tcount != 0, casting=&#39;unsafe&#39;) # for skaggs only
        pixelCount = histogram(distMat[np.logical_not(nanInd)], distBins)[0]
        distCount = np.divide(histogram(distMat[fmap &gt;= thresh], distBins)[0], pixelCount, \
                             out=np.zeros_like(distBins), where=pixelCount != 0, casting=&#39;unsafe&#39;)

        circBins = np.arange(0, 360, cbinsize)

        X, Y = np.meshgrid(xedges, np.flipud(yedges))
        X = X- xedges[-1]/2
        Y = Y- yedges[-1]/2
        angDist = np.arctan2(Y, X)* 180/np.pi
        angDist[angDist &lt; 0] += 360

        meanDist = distMat[fmap &gt;= thresh].mean()

        cs = CircStat()
        cs.set_theta(angDist[np.logical_and(distMat &lt;= meanDist, fmap &gt;= thresh)])
        angDistCount = cs.circ_histogram(circBins)[0]

        # Circular linear map
        circLinMap = np.zeros((distBins.size, circBins.size))

        for i, edge in enumerate(distBins):
            cs.set_theta(angDist[np.logical_and(distMat == edge, fmap &gt;= thresh)])
            circLinMap[i, :] = cs.circ_histogram(circBins)[0]

        perSteps = np.arange(0, 1, 1/steps)
        perDist = np.zeros(steps)

        for i in np.arange(steps):
            perDist[i] = distMat[np.logical_and(np.logical_not(nanInd), \
                        np.logical_and(fmap &gt;= perSteps[i], fmap &lt; perSteps[i]+ 1/steps))].mean()
        if update:
            _results[&#39;Border Skaggs&#39;] = self.skaggs_info(distRate, tcount)

            angDistExt = np.append(angDistCount, angDistCount)

            segsize = find_chunk(angDistExt &gt; 0)[0]
            _results[&#39;Border Ang Ext&#39;] = max(segsize)*cbinsize

            cBinsInterp = np.arange(0, 360, 0.1)
            dBinsInterp = np.arange(0, distBins[-1] + pixel, 0.1)
            graph_data[&#39;cBinsInterp&#39;] = cBinsInterp
            graph_data[&#39;dBinsInterp&#39;] = dBinsInterp
            graph_data[&#39;circLinMap&#39;] = sc.interpolate.interp2d(circBins, distBins, circLinMap, kind=&#39;cubic&#39;)(cBinsInterp, dBinsInterp)

            self.update_result(_results)

        graph_data[&#39;distBins&#39;] = distBins
        graph_data[&#39;distCount&#39;] = distCount
        graph_data[&#39;circBins&#39;] = circBins
        graph_data[&#39;angDistCount&#39;] = angDistCount
        graph_data[&#39;distRate&#39;] = distRate
        graph_data[&#39;smoothRate&#39;] = smoothRate
        graph_data[&#39;perSteps&#39;] = perSteps*100
        graph_data[&#39;perDist&#39;] = perDist

        return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.calc_ang_vel"><code class="name flex">
<span>def <span class="ident">calc_ang_vel</span></span>(<span>self, npoint=5)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates the angular head velocity of the animal from the direction data
Each sample is the slope of a fitted line of five directional data centred
around current sample.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def calc_ang_vel(self, npoint=5):
    &#34;&#34;&#34;
    Calculates the angular head velocity of the animal from the direction data
    Each sample is the slope of a fitted line of five directional data centred
    around current sample.
    
    Parameters
    ----------
    None
    
    Returns
    -------
    None        
    
    &#34;&#34;&#34;
    
    theta = self.get_direction()
    ang_vel = np.zeros(theta.shape)
    N = theta.size
    L = npoint
    l = int(np.floor(L/2))
    cs = CircStat()
    for i in np.arange(l):
        y = cs.circ_regroup(theta[:L-l+ i])
        ang_vel[i] = np.polyfit(np.arange(len(y)), y, 1)[0]

    for i in np.arange(l, N- l, 1):
        y = cs.circ_regroup(theta[i-l:i+l+ 1])
        ang_vel[i] = np.polyfit(np.arange(len(y)), y, 1)[0]

    for i in np.arange(N- l, N):
        y = cs.circ_regroup(theta[i- l:])
        ang_vel[i] = np.polyfit(np.arange(len(y)), y, 1)[0]

    return ang_vel*self.get_sampling_rate()</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.calc_border"><code class="name flex">
<span>def <span class="ident">calc_border</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Identifies the border of the recording arena from the trace of the foraging of the
animal in the arena</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>border_dist</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Distance of the animal from the border at each behavioural samples</dd>
<dt><strong><code>xedges</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Pixelated edge of the x-axis</dd>
<dt><strong><code>yedges</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Pixelated edge of the y-axis</dd>
<dt><strong><code>dist_mat</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>A matrix of distance of each pixel of the arena from the identified
border</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def calc_border(self, **kwargs):
    &#34;&#34;&#34;
    Identifies the border of the recording arena from the trace of the foraging of the
    animal in the arena
    
    Parameters
    ----------
    **kwargs
        Keyword arguments
    
    Returns
    -------
    border_dist : ndarray
        Distance of the animal from the border at each behavioural samples
    xedges : ndarray
        Pixelated edge of the x-axis
    yedges : ndarray
        Pixelated edge of the y-axis
    dist_mat : ndarray
        A matrix of distance of each pixel of the arena from the identified
        border
    
    &#34;&#34;&#34;
    
    # define edges
    pixel = kwargs.get(&#39;pixel&#39;, 3)
    chop_bound = kwargs.get(&#39;chop_bound&#39;, 5)

    xedges = np.arange(0, np.ceil(np.max(self._pos_x)), pixel)
    yedges = np.arange(0, np.ceil(np.max(self._pos_y)), pixel)

    tmap, yedges, xedges = histogram2d(self._pos_y, self._pos_x, yedges, xedges)
    if abs(xedges.size- yedges.size) &lt;= chop_bound:
        tmap = chop_edges(tmap, min(tmap.shape), min(tmap.shape))[2]
    else:
        tmap = chop_edges(tmap, tmap.shape[1], tmap.shape[0])[2]

    ybin, xbin = tmap.shape

    border = np.zeros(tmap.shape)
    border[tmap &gt; 0] = False
    border[tmap == 0] = True

    for J in np.arange(ybin):
        for I in np.arange(xbin):
            if not border[J, I] and (J == ybin-1 or J == 0 or I == xbin-1 or I == 0):
                border[J, I] = True


    # Optimize the border
    optBorder = np.zeros(border.shape)
    for i in np.arange(border.shape[0]):
        for j in np.arange(border.shape[1]):
            if border[i, j]:
                if i == 0: # along the 1st row
                    if border[i, j] != border[i + 1, j]:
                        optBorder[i, j] = True
                elif j == 0: # along the 1st column
                    if border[i, j] != border[i, j + 1]:
                        optBorder[i, j] = True
                elif i == border.shape[0] - 1: # along the last row
                    if border[i, j] != border[i - 1, j]:
                        optBorder[i, j] = True
                elif j == border.shape[1] - 1:# along the last column
                    if border[i, j] != border[i, j - 1]:
                        optBorder[i, j] = True
                else: # other cases
                    if (border[i, j] != border[i, j + 1]) or (border[i, j] != border[i + 1, j])\
                            or (border[i, j] != border[i, j - 1]) or (border[i, j] != border[i - 1, j]):
                        optBorder[i, j] = True

    border = optBorder

    xborder = np.zeros(tmap.shape, dtype=bool)
    yborder = np.zeros(tmap.shape, dtype=bool)
    for J in np.arange(ybin):
        xborder[J, find(border[J, :], 1, &#39;first&#39;)] = True # 1 added/subed to the next pixel of the traversed arena as the border
        xborder[J, find(border[J, :], 1, &#39;last&#39;)] = True
    for I in np.arange(xbin):
        yborder[find(border[:, I], 1, &#39;first&#39;), I] = True
        yborder[find(border[:, I], 1, &#39;last&#39;), I] = True

    #        self.border = border
    border = xborder | yborder
    self.tmap = tmap*self._timestamp

    distMat = np.zeros(border.shape)
    xx, yy = np.meshgrid(np.arange(xbin), np.arange(ybin))
    borderDist = np.zeros(self._time.size)

    xedges = np.arange(xbin)*pixel
    yedges = np.arange(ybin)*pixel
    xind = histogram(self._pos_x, xedges)[1]
    yind = histogram(self._pos_y, yedges)[1]

    for J in np.arange(ybin):
        for I in np.arange(xbin):
            dist_arr = (
                np.abs(xx[border] - xx[J, I]) +
                np.abs(yy[border] - yy[J, I]))
            if dist_arr.size == 0:
                logging.error(&#34;could not calculate border&#34;)
                return None, None, None, None
            tmp_dist = np.min(dist_arr)
            if find(np.logical_and(xind == I, yind == J)).size:
                borderDist[np.logical_and(xind == I, yind == J)] = tmp_dist
            distMat[J, I] = tmp_dist
    
    dist_mat= distMat*pixel
    border_dist= borderDist*pixel
    
    return border_dist, xedges, yedges, dist_mat</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.get_ang_vel"><code class="name flex">
<span>def <span class="ident">get_ang_vel</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns angular head velocity of the animal</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>Angular head velocity of the animal</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_ang_vel(self):
    &#34;&#34;&#34;
    Returns angular head velocity of the animal
            
    Parameters
    ----------
    None

    Returns
    -------
    ndarray
        Angular head velocity of the animal

    &#34;&#34;&#34;
    
    return self._ang_vel</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.get_border"><code class="name flex">
<span>def <span class="ident">get_border</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns animal's distance from the border</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>Animal's distance from the border</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_border(self):
    &#34;&#34;&#34;
    Returns animal&#39;s distance from the border
            
    Parameters
    ----------
    None

    Returns
    -------
    ndarray
        Animal&#39;s distance from the border

    &#34;&#34;&#34;
    
    return self._border_dist, self._xbound, self._ybound, self._dist_map</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.get_direction"><code class="name flex">
<span>def <span class="ident">get_direction</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns head direction of the animal</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>Head direction of the animal</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_direction(self):
    &#34;&#34;&#34;
    Returns head direction of the animal
            
    Parameters
    ----------
    None

    Returns
    -------
    ndarray
        Head direction of the animal

    &#34;&#34;&#34;
    
    return self._direction</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.get_duration"><code class="name flex">
<span>def <span class="ident">get_duration</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the duration of the experiment</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>Duration of the experiment</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_duration(self):
    &#34;&#34;&#34;
    Returns the duration of the experiment
            
    Parameters
    ----------
    None

    Returns
    -------
    float
        Duration of the experiment

    &#34;&#34;&#34;
    if len(self._time) == 0:
        return 0
    return self._time[-1]</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.get_event_loc"><code class="name flex">
<span>def <span class="ident">get_event_loc</span></span>(<span>self, ftimes, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates location of the event from its timestamps.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ftimes</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Timestamps of the spiking or any other events</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>Index of the events in spatial-timestamps</dd>
<dt>[ Two item list containing</dt>
<dt>ndarray</dt>
<dt>x-coordinates of the event location</dt>
<dt>ndarray</dt>
<dt>y-ccordinates of the event location</dt>
<dt>]</dt>
<dt><code>ndarray</code></dt>
<dd>direction of the animal at the time of the event</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_event_loc(self, ftimes, **kwargs):
    &#34;&#34;&#34;
    Calculates location of the event from its timestamps.
                            
    Parameters
    ----------
    ftimes : ndarray
        Timestamps of the spiking or any other events
    **kwargs
        Keyword arguments

    Returns
    -------
    ndarray        
        Index of the events in spatial-timestamps
    [ Two item list containing
        ndarray
            x-coordinates of the event location
        ndarray
            y-ccordinates of the event location
    ]
    ndarray
        direction of the animal at the time of the event            
    &#34;&#34;&#34;
    
    
    time = self.get_time()
    lim = kwargs.get(&#39;range&#39;, [0, time.max()])

    # Sean - Why is zero idx is always thrown away?
    keep_zero_idx = kwargs.get(&#39;keep_zero_idx&#39;, False)
    
    hist = histogram(
        ftimes[np.logical_and(
                ftimes &gt;= lim[0], ftimes &lt; lim[1])], 
        time)
    vidInd = hist[1]

    if keep_zero_idx:
        retInd = vidInd
    else:
        retInd = vidInd[vidInd != 0]
    
    return retInd, [self._pos_x[retInd], self._pos_y[retInd]], self._direction[retInd]</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.get_non_moving_times"><code class="name flex">
<span>def <span class="ident">get_non_moving_times</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the times where the subject is not moving</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_non_moving_times(self, **kwargs):
    &#34;&#34;&#34; Returns the times where the subject is not moving&#34;&#34;&#34;
    ranges = self.non_moving_periods(**kwargs)
    time_data = [
        val for val in self.get_time()
        if any(lower &lt;= val &lt;= upper for (lower, upper) in ranges)
    ]
    return time_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.get_pixel_size"><code class="name flex">
<span>def <span class="ident">get_pixel_size</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the pixel size of the recorded arena</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Pixel size</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_pixel_size(self):
    &#34;&#34;&#34;
    Returns the pixel size of the recorded arena
            
    Parameters
    ----------
    None

    Returns
    -------
    int
        Pixel size

    &#34;&#34;&#34;    
    return self._pixel_size</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.get_pos_x"><code class="name flex">
<span>def <span class="ident">get_pos_x</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the X-ccordinates of animal's location</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>X-coordinates of animal's location</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_pos_x(self):
    &#34;&#34;&#34;
    Returns the X-ccordinates of animal&#39;s location
            
    Parameters
    ----------
    None

    Returns
    -------
    ndarray
        X-coordinates of animal&#39;s location

    &#34;&#34;&#34;

    return self._pos_x</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.get_pos_y"><code class="name flex">
<span>def <span class="ident">get_pos_y</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the Y-ccordinates of animal's location</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>Y-coordinates of animal's location</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_pos_y(self):
    &#34;&#34;&#34;
    Returns the Y-ccordinates of animal&#39;s location
            
    Parameters
    ----------
    None

    Returns
    -------
    ndarray
        Y-coordinates of animal&#39;s location

    &#34;&#34;&#34;

    return self._pos_y</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.get_sampling_rate"><code class="name flex">
<span>def <span class="ident">get_sampling_rate</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the sampling rate of the spatial samples</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Spatial data sampling rate</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_sampling_rate(self):
    &#34;&#34;&#34;
    Returns the sampling rate of the spatial samples
            
    Parameters
    ----------
    None

    Returns
    -------
    int
        Spatial data sampling rate

    &#34;&#34;&#34;

    return self._fs</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.get_speed"><code class="name flex">
<span>def <span class="ident">get_speed</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns speed of the animal</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>Speed of the animal</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_speed(self):
    &#34;&#34;&#34;
    Returns speed of the animal
            
    Parameters
    ----------
    None

    Returns
    -------
    ndarray
        Speed of the animal

    &#34;&#34;&#34;       
    return self._speed</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.get_time"><code class="name flex">
<span>def <span class="ident">get_time</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the time of individual spatial samples</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Total spatial samples</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_time(self):
    &#34;&#34;&#34;
    Returns the time of individual spatial samples
            
    Parameters
    ----------
    None

    Returns
    -------
    int
        Total spatial samples

    &#34;&#34;&#34;

    return self._time</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.get_timestamp"><code class="name flex">
<span>def <span class="ident">get_timestamp</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the temporal resolution of spatial samples</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Temporal resolution of spatial samples</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_timestamp(self):
    &#34;&#34;&#34;
    Returns the temporal resolution of spatial samples
            
    Parameters
    ----------
    None

    Returns
    -------
    int
        Temporal resolution of spatial samples

    &#34;&#34;&#34;
    
    return self._timestamp</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.get_total_samples"><code class="name flex">
<span>def <span class="ident">get_total_samples</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the number of spatial samples</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Total spatial samples</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_total_samples(self):
    &#34;&#34;&#34;
    Returns the number of spatial samples
            
    Parameters
    ----------
    None

    Returns
    -------
    int
        Total spatial samples

    &#34;&#34;&#34;        
    return self._time.size</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.get_type"><code class="name flex">
<span>def <span class="ident">get_type</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the type of object. For NSpatial, this is always <code>spatial</code> type</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_type(self):
    &#34;&#34;&#34;
    Returns the type of object. For NSpatial, this is always `spatial` type
    
    Parameters
    ----------
    None
    
    Returns
    -------
    str

    &#34;&#34;&#34; 
    
    return self.__type        </code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.gradient"><code class="name flex">
<span>def <span class="ident">gradient</span></span>(<span>self, ftimes, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Analysis of gradient cell, a unit whose firing rate gradually increases
as the animal traverses from the border to the cneter of the environment</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ftimes</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Timestamps of the spiking activity of a unit</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">    def gradient(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Analysis of gradient cell, a unit whose firing rate gradually increases 
        as the animal traverses from the border to the cneter of the environment
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit        
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
        &#34;&#34;&#34;
        
        _results = oDict()
        graph_data = {}

        alim = kwargs.get(&#39;alim&#39;, 0.25)
        blim = kwargs.get(&#39;blim&#39;, 0.25)
        clim = kwargs.get(&#39;clim&#39;, 0.5)

        graph_data = self.border(ftimes, **kwargs)

        x = graph_data[&#39;distBins&#39;]
        y = graph_data[&#39;smoothRate&#39;]
        x = x[np.isfinite(y)]
        y = y[np.isfinite(y)]
        y = np.log(y, out=np.zeros_like(y), where=y != 0, casting=&#39;unsafe&#39;)
        ai = y.max()
        y0 = y[x == 0]
        bi = ai- y0

        d_half = x.mean()
        for i, dist in enumerate(x):
            if i &lt; x.size-1 and (y0+ bi/2) &gt; y[i] and (y0+ bi/2) &lt;= y[i+1]:
                d_half = x[i:i+2].mean()

        ci = np.log(2)/d_half

        def fit_func(x, a, b, c):
            return a- b*np.exp(-c*x)

        popt, pcov = curve_fit(fit_func, x, y, \
                                p0=[ai, bi, ci], \
                                bounds=([(1- alim)*ai, (1- blim)*bi, (1- clim)*ci], \
                                [(1+ alim)*ai, (1+ blim)*bi, (1+ clim)*ci]), \
                                max_nfev=100000)
        a, b, c = popt

        y_fit = fit_func(x, *popt)


        gof = residual_stat(y, y_fit, 3)
        rateFit = np.exp(y_fit)

        graph_data[&#39;distBins&#39;] = x
#        graph_data[&#39;smoothRate&#39;] = y
        graph_data[&#39;rateFit&#39;] = rateFit
        graph_data[&#39;diffRate&#39;] = b*c*np.multiply(rateFit, np.exp(-c*x))

        _results[&#39;Grad Pearse R&#39;] = gof[&#39;Pearson R&#39;]
        _results[&#39;Grad Pearse P&#39;] = gof[&#39;Pearson P&#39;]
        _results[&#39;Grad adj Rsq&#39;] = gof[&#39;adj Rsq&#39;]
        _results[&#39;Grad Max Growth Rate&#39;] = c*np.exp(a-1)
        _results[&#39;Grad Inflect Dist&#39;] = np.log(b)/c

        self.update_result(_results)
        return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.grid"><code class="name flex">
<span>def <span class="ident">grid</span></span>(<span>self, ftimes, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Analysis of Grid cells characterised by formation of grid-like pattern
of high activity in the firing-rate map
</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ftimes</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Timestamps of the spiking activity of a unit</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def grid(self, ftimes, **kwargs):        
    &#34;&#34;&#34;
    Analysis of Grid cells characterised by formation of grid-like pattern
    of high activity in the firing-rate map        
    
    Parameters
    ----------
    ftimes : ndarray
        Timestamps of the spiking activity of a unit   
    **kwargs
        Keyword arguments

    Returns
    -------
    dict
        Graphical data of the analysis

    &#34;&#34;&#34;
    
    _results = oDict()
    tol = kwargs.get(&#39;angtol&#39;, 2)
    binsize = kwargs.get(&#39;binsize&#39;, 3)
    bins = np.arange(0, 360, binsize)

    graph_data = self.loc_auto_corr(ftimes, update=False, **kwargs)
    corrMap = graph_data[&#39;corrMap&#39;]
    corrMap[np.isnan(corrMap)] = 0
    xshift = graph_data[&#39;xshift&#39;]
    yshift = graph_data[&#39;yshift&#39;]

    pixel = np.int(np.diff(xshift).mean())

    ny, nx = corrMap.shape
    rpeaks = np.zeros(corrMap.shape, dtype=bool)
    cpeaks = np.zeros(corrMap.shape, dtype=bool)
    for j in np.arange(ny):
        rpeaks[j, extrema(corrMap[j, :])[1]] = True
    for i in np.arange(nx):
        cpeaks[extrema(corrMap[:, i])[1], i] = True
    ymax, xmax = find2d(np.logical_and(rpeaks, cpeaks))

    peakDist = np.sqrt((ymax- find(yshift == 0))**2+ (xmax- find(xshift == 0))**2)
    sortInd = np.argsort(peakDist)
    ymax, xmax, peakDist = ymax[sortInd], xmax[sortInd], peakDist[sortInd]

    ymax, xmax, peakDist = (ymax[1:7], xmax[1:7], peakDist[1:7]) if ymax.size &gt;= 7 else ([], [], [])
    theta = np.arctan2(yshift[ymax], xshift[xmax])*180/np.pi
    theta[theta &lt; 0] += 360
    sortInd = np.argsort(theta)
    ymax, xmax, peakDist, theta = (ymax[sortInd], xmax[sortInd], peakDist[sortInd], theta[sortInd])

    graph_data[&#39;ymax&#39;] = yshift[ymax]
    graph_data[&#39;xmax&#39;] = xshift[xmax]

    meanDist = peakDist.mean()
    X, Y = np.meshgrid(xshift, yshift)
    distMat = np.sqrt(X**2 + Y**2)/pixel

    if len(ymax) == np.logical_and(peakDist &gt; 0.75*meanDist, peakDist &lt; 1.25*meanDist).sum(): # if all of them are within tolerance(25%)
        maskInd = np.logical_and(distMat &gt; 0.5*meanDist, distMat &lt; 1.5*meanDist)
        rotCorr = np.array([corr_coeff(rot_2d(corrMap, theta)[maskInd], corrMap[maskInd]) for k, theta in enumerate(bins)])
        ramax, rimax, ramin, rimin = extrema(rotCorr)
        mThetaPk, mThetaTr = (np.diff(bins[rimax]).mean(), np.diff(bins[rimin]).mean()) if rimax.size and rimin.size else (None, None)
        graph_data[&#39;rimax&#39;] = rimax
        graph_data[&#39;rimin&#39;] = rimin
        graph_data[&#39;anglemax&#39;] = bins[rimax]
        graph_data[&#39;anglemin&#39;] = bins[rimin]
        graph_data[&#39;rotAngle&#39;] = bins
        graph_data[&#39;rotCorr&#39;] = rotCorr

        if mThetaPk is not None and mThetaTr is not None:
            isGrid = True if 60 - tol &lt; mThetaPk &lt; 60 + tol and 60 - tol &lt; mThetaTr &lt; 60 + tol else False
        else:
            isGrid = False

        meanAlpha = np.diff(theta).mean()
        psi = theta[np.array([2, 3, 4, 5, 0, 1])]- theta
        psi[psi &lt; 0] += 360
        meanPsi = psi.mean()

        _results[&#39;Is Grid&#39;] = isGrid and 120 - tol &lt; meanPsi &lt; 120 + tol and 60 - tol &lt; meanAlpha &lt; 60 + tol
        _results[&#39;Grid Mean Alpha&#39;] = meanAlpha
        _results[&#39;Grid Mean Psi&#39;] = meanPsi
        _results[&#39;Grid Spacing&#39;] = meanDist*pixel
        _results[&#39;Grid Score&#39;] = rotCorr[rimax].max()- rotCorr[rimin].min() # Difference between highest Pearson R at peaks and lowest at troughs
        _results[&#39;Grid Orientation&#39;] = theta[0]

    else:
        _results[&#39;Is Grid&#39;] = False

    self.update_result(_results)
    return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.hd_rate"><code class="name flex">
<span>def <span class="ident">hd_rate</span></span>(<span>self, ftimes, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates the firing rate of the unit with respect to the head-direciton
of the animal in the environment. This is calle Tuning curve.</p>
<p>Precited firing map from the locational firing is also calculated and
distributive ratio is measured along with the Skaggs information.</p>
<p>Spike-plot similar to locational firing is developed but in the circular bins
which shows the direction of the animal's head at each spike's occurring time.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ftimes</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Timestamps of the spiking activity of a unit</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def hd_rate(self, ftimes, **kwargs):
    &#34;&#34;&#34;
    Calculates the firing rate of the unit with respect to the head-direciton
    of the animal in the environment. This is calle Tuning curve.
    
    Precited firing map from the locational firing is also calculated and
    distributive ratio is measured along with the Skaggs information.
    
    Spike-plot similar to locational firing is developed but in the circular bins
    which shows the direction of the animal&#39;s head at each spike&#39;s occurring time.
                            
    Parameters
    ----------
    ftimes : ndarray
        Timestamps of the spiking activity of a unit
    **kwargs
        Keyword arguments

    Returns
    -------
    dict
        Graphical data of the analysis
    &#34;&#34;&#34;
    
    _results = oDict()
    graph_data = {}
    update = kwargs.get(&#39;update&#39;, True)
    binsize = kwargs.get(&#39;binsize&#39;, 5) # in degrees
    filttype, filtsize = kwargs.get(&#39;filter&#39;, [&#39;b&#39;, 5])
    lim = kwargs.get(&#39;range&#39;, [0, self.get_duration()])

    bins = np.arange(0, 360, binsize)

    spike_hd = self.get_event_loc(ftimes, **kwargs)[2]
    direction = self.get_direction()[np.logical_and(self.get_time() &gt;= lim[0], self.get_time() &lt;= lim[1])]

    tcount, ind, bins = histogram(direction, bins)

    tcount = tcount/ self.get_sampling_rate()

    spike_count = histogram(spike_hd, bins)[0].astype(tcount.dtype)

    hd_rate = np.divide(spike_count, tcount, out=np.zeros_like(spike_count), where=tcount != 0, casting=&#39;unsafe&#39;)

    smoothRate = smooth_1d(hd_rate, filttype, filtsize)

    if update:
        _results[&#39;HD Skaggs&#39;] = self.skaggs_info(hd_rate, tcount)
        cs = CircStat(rho=smoothRate, theta=bins)
        results = cs.calc_stat()
        _results[&#39;HD Rayl Z&#39;] = results[&#39;RaylZ&#39;]
        _results[&#39;HD Rayl P&#39;] = results[&#39;RaylP&#39;]
        _results[&#39;HD von Mises K&#39;] = results[&#39;vonMisesK&#39;]
        
        _results[&#39;HD Mean&#39;] = results[&#39;meanTheta&#39;]
        _results[&#39;HD Mean Rate&#39;] = results[&#39;meanRho&#39;]
        _results[&#39;HD Res Vect&#39;] = results[&#39;resultant&#39;]

        binInterp = np.arange(360)
        rateInterp = np.interp(binInterp, bins, hd_rate)

        _results[&#39;HD Peak Rate&#39;] = np.amax(rateInterp)
        _results[&#39;HD Peak&#39;] = binInterp[np.argmax(rateInterp)]

        half_max = np.amin(rateInterp)+ (np.amax(rateInterp)- np.amin(rateInterp))/2
        d = np.sign(half_max - rateInterp[0:-1]) - np.sign(half_max - rateInterp[1:])
        left_possible = find(d &gt; 0)
        if len(left_possible) &gt; 0:
            left_idx = left_possible[0]
        else:
            left_idx = None
        
        right_possible = find(d &lt; 0)
        if len(right_possible) &gt; 0:
            right_idx = right_possible[-1]
        else:
            right_idx = None
        _results[&#39;HD Half Width&#39;] = None if (not left_idx or not right_idx or left_idx &gt; right_idx) \
                                    else binInterp[right_idx]- binInterp[left_idx]

        pixel = kwargs.get(&#39;pixel&#39;, 3)
        placeData = self.place(ftimes, pixel=pixel)
        fmap = placeData[&#39;smoothMap&#39;]
        fmap[np.isnan(fmap)] = 0
        hdPred = np.zeros(bins.size)
        for i, b in enumerate(bins):
            hdInd = np.logical_and(direction &gt;= b, direction &lt; b+ binsize)
            tmap = histogram2d(self.get_pos_y()[hdInd], self.get_pos_x()[hdInd], placeData[&#39;yedges&#39;], placeData[&#39;xedges&#39;])[0]
            tmap /= self.get_sampling_rate()
            hdPred[i] = np.sum(fmap*tmap)/ tmap.sum()

        graph_data[&#39;hdPred&#39;] = smooth_1d(hdPred, &#39;b&#39;, 5)
        self.update_result(_results)

    graph_data[&#39;hd&#39;] = direction
    graph_data[&#39;hdRate&#39;] = hd_rate
    graph_data[&#39;smoothRate&#39;] = smoothRate
    graph_data[&#39;tcount&#39;] = tcount
    graph_data[&#39;bins&#39;] = bins
    graph_data[&#39;spike_hd&#39;] = spike_hd

    cs = CircStat()
    cs.set_theta(spike_hd)
    graph_data[&#39;scatter_radius&#39;], graph_data[&#39;scatter_bins&#39;] = cs.circ_scatter(bins=2, step=0.05)


    return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.hd_rate_ccw"><code class="name flex">
<span>def <span class="ident">hd_rate_ccw</span></span>(<span>self, ftimes, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates the tuning curve but split into clock-wise vs counterclockwise
head-directional movement.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ftimes</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Timestamps of the spiking activity of a unit</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def hd_rate_ccw(self, ftimes, **kwargs):
    &#34;&#34;&#34;
    Calculates the tuning curve but split into clock-wise vs counterclockwise
    head-directional movement.
                            
    Parameters
    ----------
    ftimes : ndarray
        Timestamps of the spiking activity of a unit
    **kwargs
        Keyword arguments

    Returns
    -------
    dict
        Graphical data of the analysis
    &#34;&#34;&#34;
    
    _results = oDict()
    graph_data = {}
    update = kwargs.get(&#39;update&#39;, True)
    binsize = kwargs.get(&#39;binsize&#39;, 5) # in degrees
    filttype, filtsize = kwargs.get(&#39;filter&#39;, [&#39;b&#39;, 5])
    lim = kwargs.get(&#39;range&#39;, [0, self.get_duration()])
    thresh = kwargs.get(&#39;thresh&#39;, 30)

    edges = np.arange(0, 360, binsize)

    spikeInd, spikeLoc, spike_hd = self.get_event_loc(ftimes, **kwargs)
    vidInd = np.logical_and(self.get_time() &gt;= lim[0], self.get_time() &lt;= lim[1])
    direction = self.get_direction()[vidInd]

    ccwSpike_hd = spike_hd[self.get_ang_vel()[spikeInd] &lt; -thresh]
    cwSpike_hd = spike_hd[self.get_ang_vel()[spikeInd] &gt; thresh]

    ccw_dir = direction[self.get_ang_vel()[vidInd] &lt; -thresh]
    cw_dir = direction[self.get_ang_vel()[vidInd] &gt; thresh]


    binInterp = np.arange(360)

    tcount, ind, bins = histogram(cw_dir, edges)
    tcount = tcount/ self.get_sampling_rate()
    spike_count = histogram(cwSpike_hd, edges)[0].astype(tcount.dtype)
    cwRate = np.divide(spike_count, tcount, out=np.zeros_like(spike_count), where=tcount != 0, casting=&#39;unsafe&#39;)
    cwRate = np.interp(binInterp, bins, smooth_1d(cwRate, filttype, filtsize))

    tcount, ind, bins = histogram(ccw_dir, edges)
    tcount = tcount/ self.get_sampling_rate()
    spike_count = histogram(ccwSpike_hd, edges)[0].astype(tcount.dtype)
    ccwRate = np.divide(spike_count, tcount, out=np.zeros_like(spike_count), where=tcount != 0, casting=&#39;unsafe&#39;)
    ccwRate = np.interp(binInterp, bins, smooth_1d(ccwRate, filttype, filtsize))

    if update:
        _results[&#39;HD Delta&#39;] = binInterp[np.argmax(ccwRate)]- binInterp[np.argmax(cwRate)]
        _results[&#39;HD Peak CW&#39;] = np.argmax(cwRate)
        _results[&#39;HD Peak CCW&#39;] = np.argmax(ccwRate)
        _results[&#39;HD Peak Rate CW&#39;] = np.amax(cwRate)
        _results[&#39;HD Peak Rate CCW&#39;] = np.amax(ccwRate)
        self.update_result(_results)

    graph_data[&#39;bins&#39;] = binInterp
    graph_data[&#39;hdRateCW&#39;] = cwRate
    graph_data[&#39;hdRateCCW&#39;] = ccwRate

    return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.hd_shift"><code class="name flex">
<span>def <span class="ident">hd_shift</span></span>(<span>self, ftimes, shift_ind=array([-10,
-9,
-8,
-7,
-6,
-5,
-4,
-3,
-2,
-1,
0,
1,
2,
3,
4,
5,
6,
7,
8,
9,
10]))</span>
</code></dt>
<dd>
<section class="desc"><p>Analysis of firing specificity of the unit with respect to animal's head
direction to oberve whether it represents past direction or anicipates a
future direction.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>shift_ind</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Index of spatial resolution shift for the spike event time. Shift -1
implies shift to the past by 1 spatial time resolution, and +2 implies
shift to the future by 2 spatial time resoultion.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def hd_shift(self, ftimes, shift_ind=np.arange(-10, 11)):
    &#34;&#34;&#34;
    Analysis of firing specificity of the unit with respect to animal&#39;s head
    direction to oberve whether it represents past direction or anicipates a
    future direction.
            
    Parameters
    ----------
    shift_ind : ndarray
        Index of spatial resolution shift for the spike event time. Shift -1
        implies shift to the past by 1 spatial time resolution, and +2 implies
        shift to the future by 2 spatial time resoultion.

    Returns
    -------
    dict
        Graphical data of the analysis

    &#34;&#34;&#34;
    
    _results = oDict()
    graph_data = {}
    shift = shift_ind/self.get_sampling_rate()
    shiftlen = shift.size
    dur = self.get_time()[-1]
    delta = np.zeros((shiftlen,))
    skaggs = np.zeros((shiftlen,))
    peakRate = np.zeros((shiftlen,))

    for i in np.arange(shiftlen):
        shift_ftimes = ftimes+ shift[i]
        # Wrapping up the time
        shift_ftimes[shift_ftimes &gt; dur] -= dur
        shift_ftimes[shift_ftimes &lt; 0] += dur

        hdData = self.hd_rate_ccw(shift_ftimes, update=False)
        delta[i] = hdData[&#39;bins&#39;][np.argmax(hdData[&#39;hdRateCCW&#39;])]- hdData[&#39;bins&#39;][np.argmax(hdData[&#39;hdRateCW&#39;])]
        hdData = self.hd_rate(shift_ftimes, update=False)
        peakRate[i] = np.amax(hdData[&#39;smoothRate&#39;])
        skaggs[i] = self.skaggs_info(hdData[&#39;hdRate&#39;], hdData[&#39;tcount&#39;])

    graph_data[&#39;delta&#39;] = delta
    graph_data[&#39;skaggs&#39;] = skaggs
    graph_data[&#39;peakRate&#39;] = peakRate
    graph_data[&#39;shiftTime&#39;] = shift*1000 # changing to milisecond

    # Find out the optimum skaggs location
    shiftUpsamp = np.arange(shift[0], shift[-1], np.mean(np.diff(shift))/10)
    skaggsUpsamp = np.interp(shiftUpsamp, shift, skaggs)
    peakRateUpsamp = np.interp(shiftUpsamp, shift, peakRate)

    dfit_result = linfit(shift, delta)
    deltaFit = dfit_result[&#39;yfit&#39;]
    sortInd = np.argsort(deltaFit)
    _results[&#39;HD ATI&#39;] = np.interp(0, deltaFit[sortInd], shift[sortInd])*1000 if dfit_result[&#39;Pearson R&#39;] &gt;= 0.85 else np.nan

    graph_data[&#39;deltaFit&#39;] = deltaFit
    imax = sg.argrelmax(skaggsUpsamp)[0]
    maxloc = find(skaggsUpsamp[imax] == skaggsUpsamp.max())
    _results[&#39;HD Opt Shift Skaggs&#39;] = np.nan if maxloc.size != 1 else \
                (np.nan if imax[maxloc] == 0 or imax[maxloc] == skaggsUpsamp.size else shiftUpsamp[imax[maxloc]][0]*1000) # in milisecond

    imax = sg.argrelmax(peakRateUpsamp)[0]
    maxloc = find(peakRateUpsamp[imax] == peakRateUpsamp.max())
    _results[&#39;HD Opt Shift Peak Rate&#39;] = np.nan if maxloc.size != 1 else \
                (np.nan if imax[maxloc] == 0 or imax[maxloc] == peakRateUpsamp.size else shiftUpsamp[imax[maxloc]][0]*1000) # in milisecond
    self.update_result(_results)

    return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.hd_shuffle"><code class="name flex">
<span>def <span class="ident">hd_shuffle</span></span>(<span>self, ftimes, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Shuffling analysis of the unit to see if the head-directional firing specifity
is by chance or actually correlated to the head-direction of the animal</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ftimes</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Timestamps of the spiking activity of a unit</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def hd_shuffle(self, ftimes, **kwargs):
    &#34;&#34;&#34;
    Shuffling analysis of the unit to see if the head-directional firing specifity
    is by chance or actually correlated to the head-direction of the animal
    
    Parameters
    ----------
    ftimes : ndarray
        Timestamps of the spiking activity of a unit
    **kwargs
        Keyword arguments

    Returns
    -------
    dict
        Graphical data of the analysis

    &#34;&#34;&#34;
    
    _results = oDict()
    graph_data = {}
    nshuff = kwargs.get(&#39;nshuff&#39;, 500)
    limit = kwargs.get(&#39;limit&#39;, 0)
    bins = kwargs.get(&#39;bins&#39;, 100)
    # limit = 0 implies enirely random shuffle, limit = &#39;x&#39; implies nshuff number of shuffles in the range [-x x]
    dur = self.get_time()[-1]
    shift = nprand.uniform(low=limit- dur, high=dur- limit, size=nshuff)
    raylZ = np.zeros((nshuff,))
    vonMisesK = np.zeros((nshuff,))
    for i in np.arange(nshuff):
        shift_ftimes = ftimes+ shift[i]
        # Wrapping up the time
        shift_ftimes[shift_ftimes &gt; dur] -= dur
        shift_ftimes[shift_ftimes &lt; 0] += dur

        hdData = self.hd_rate(shift_ftimes, update=False)
        cs = CircStat(rho=hdData[&#39;smoothRate&#39;], theta=hdData[&#39;bins&#39;])
        results = cs.calc_stat()
        raylZ[i] = results[&#39;RaylZ&#39;]
        vonMisesK[i] = results[&#39;vonMisesK&#39;]

    graph_data[&#39;raylZ&#39;] = raylZ
    graph_data[&#39;vonMisesK&#39;] = vonMisesK
    hdData = self.hd_rate(ftimes, update=False)
    cs.set_rho(hdData[&#39;smoothRate&#39;])
    results = cs.calc_stat()
    graph_data[&#39;refRaylZ&#39;] = results[&#39;RaylZ&#39;]
    graph_data[&#39;refVonMisesK&#39;] = results[&#39;vonMisesK&#39;]

    graph_data[&#39;raylZCount&#39;], ind, graph_data[&#39;raylZEdges&#39;] = histogram(raylZ, bins=bins)
    graph_data[&#39;raylZPer95&#39;] = np.percentile(raylZ, 95)

    graph_data[&#39;vonMisesKCount&#39;], ind, graph_data[&#39;vonMisesKEdges&#39;] = histogram(vonMisesK, bins=bins)
    graph_data[&#39;vonMisesKPer95&#39;] = np.percentile(vonMisesK, 95)

    _results[&#39;HD Shuff Rayl Z Per 95&#39;] = np.percentile(raylZ, 95)
    _results[&#39;HD Shuff von Mises K Per 95&#39;] = np.percentile(vonMisesK, 95)
    self.update_result(_results)

    return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.hd_time_lapse"><code class="name flex">
<span>def <span class="ident">hd_time_lapse</span></span>(<span>self, ftimes)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates the tuning curve and idnetifies the location of the spiking events
at certain intervals. This method is useful in observing the evolution of
unit-activity as the animal traverses the environment.</p>
<p>Following intervals ar used:
0-1min, 0-2min, 0-4min, 0-8min, 0-16min or 0-end depending on the recording duration
0-1min, 1-2min, 2-4min, 4-8min, 8-16min or 16-end depending on the recording duration</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ftimes</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Timestamps of the spiking activity of a unit</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def hd_time_lapse(self, ftimes):
    &#34;&#34;&#34;
    Calculates the tuning curve and idnetifies the location of the spiking events
    at certain intervals. This method is useful in observing the evolution of
    unit-activity as the animal traverses the environment.
    
    Following intervals ar used:
        0-1min, 0-2min, 0-4min, 0-8min, 0-16min or 0-end depending on the recording duration
        0-1min, 1-2min, 2-4min, 4-8min, 8-16min or 16-end depending on the recording duration
            
    Parameters
    ----------
    ftimes : ndarray
        Timestamps of the spiking activity of a unit
    **kwargs
        Keyword arguments

    Returns
    -------
    dict
        Graphical data of the analysis
    &#34;&#34;&#34;
    
    #### Breaking down the spike plot for firing evolution
    # 0-1min,  0-2min, 0-4min, 0-8min
    graph_data = oDict()
    lim = [0, 1*60]
    graph_data[&#39;0To1min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

    lim = [0, 2*60]
    graph_data[&#39;0To2min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

    lim = [0, 4*60]
    graph_data[&#39;0To4min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

    lim = [0, 8*60]
    graph_data[&#39;0To8min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

    # 0-1min, 1-2min,  2-4min, 4-8min
    lim = [1*60, 2*60]
    graph_data[&#39;1To2min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

    lim = [2*60, 4*60]
    graph_data[&#39;2To4min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

    lim = [4*60, 8*60]
    graph_data[&#39;4To8min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

    ## 0-16min, 8-16min 0-20min, 16-20min

    if self.get_duration() &gt; 8*60:
        if self.get_duration() &gt; 16*60:
            lim = [0, 16*60]
            graph_data[&#39;0To16min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

            lim = [8*60, 16*60]
            graph_data[&#39;8To16min&#39;] = self.hd_rate(ftimes, range=lim, update=False)

            lim = [0, self.get_duration()]
            graph_data[&#39;0ToEnd&#39;] = self.hd_rate(ftimes, range=lim, update=False)

            lim = [16*60, self.get_duration()]
            graph_data[&#39;16ToEnd&#39;] = self.hd_rate(ftimes, range=lim, update=False)

        else:
            lim = [0, self.get_duration()]
            graph_data[&#39;0ToEnd&#39;] = self.hd_rate(ftimes, range=lim, update=False)

            lim = [8*60, self.get_duration()]
            graph_data[&#39;8ToEnd&#39;] = self.hd_rate(ftimes, range=lim, update=False)

        return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.interdependence"><code class="name flex">
<span>def <span class="ident">interdependence</span></span>(<span>self, ftimes, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Interdependence analysis where firing rate of each variable is predicted
from another variable and the distributive ratio is measured between the
predicted firing rate and the caclulated firing rate.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ftimes</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Timestamps of the spiking activity of a unit</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def interdependence(self, ftimes, **kwargs):
    &#34;&#34;&#34;
    Interdependence analysis where firing rate of each variable is predicted
    from another variable and the distributive ratio is measured between the
    predicted firing rate and the caclulated firing rate.
    
    Parameters
    ----------
    ftimes : ndarray
        Timestamps of the spiking activity of a unit
    **kwargs
        Keyword arguments

    Returns
    -------
    None
    
    &#34;&#34;&#34;        

    _results = oDict()
    pixel = kwargs.get(&#39;pixel&#39;, 3)
    hdbinsize = kwargs.get(&#39;hdbinsize&#39;, 5)
    spbinsize = kwargs.get(&#39;spbinsize&#39;, 1)
    sprange = kwargs.get(&#39;sprange&#39;, [0, 40])
    abinsize = kwargs.get(&#39;abinsize&#39;, 10)
    ang_velrange = kwargs.get(&#39;ang_velrange&#39;, [-500, 500])

    placeData = self.place(ftimes, pixel=pixel, update=False)
    fmap = placeData[&#39;smoothMap&#39;]
    fmap[np.isnan(fmap)] = 0
    xloc = self.get_pos_x()
    yloc = self.get_pos_y()
    xedges = placeData[&#39;xedges&#39;]
    yedges = placeData[&#39;yedges&#39;]

    hdData = self.hd_rate(ftimes, binsize=hdbinsize, update=False)
    bins = hdData[&#39;bins&#39;]
    predRate = np.zeros(bins.size)
    for i, b in enumerate(bins):
        ind = np.logical_and(hdData[&#39;hd&#39;] &gt;= b, hdData[&#39;hd&#39;] &lt; b + hdbinsize)
        tmap = histogram2d(yloc[ind], xloc[ind], yedges, xedges)[0]
        tmap /= self.get_sampling_rate()
        predRate[i] = np.sum(fmap*tmap)/ tmap.sum()
    _results[&#39;DR HP&#39;] = np.abs(np.log((1 + hdData[&#39;smoothRate&#39;])/ (1 + predRate))).sum()/bins.size

    spData = self.speed(ftimes, binsize=spbinsize, range=sprange, update=False)
    bins = spData[&#39;bins&#39;]
    predRate = np.zeros(bins.size)
    speed = self.get_speed()
    for i, b in enumerate(bins):
        ind = np.logical_and(speed &gt;= b, speed &lt; b + spbinsize)
        tmap = histogram2d(yloc[ind], xloc[ind], yedges, xedges)[0]
        tmap /= self.get_sampling_rate()
        predRate[i] = np.sum(fmap*tmap)/ tmap.sum()
    _results[&#39;DR SP&#39;] = np.abs(np.log((1 + spData[&#39;rate&#39;])/ (1 + predRate))).sum()/bins.size

    ang_velData = self.angular_velocity(ftimes, binsize=abinsize, range=ang_velrange, update=False)
    bins = np.hstack((ang_velData[&#39;leftBins&#39;], ang_velData[&#39;rightBins&#39;]))
    predRate = np.zeros(bins.size)
    ang_vel = self.get_ang_vel()
    for i, b in enumerate(bins):
        ind = np.logical_and(ang_vel &gt;= b, ang_vel &lt; b + abinsize)
        tmap = histogram2d(yloc[ind], xloc[ind], yedges, xedges)[0]
        tmap /= self.get_sampling_rate()
        predRate[i] = np.sum(fmap*tmap)/ tmap.sum()
    ang_velObs = np.hstack((ang_velData[&#39;leftRate&#39;], ang_velData[&#39;rightRate&#39;]))
    _results[&#39;DR AP&#39;] = np.abs(np.log((1 + ang_velObs)/ (1 + predRate))).sum()/bins.size

    borderData = self.border(ftimes, update=False)
    bins = borderData[&#39;distBins&#39;]
    dbinsize = np.diff(bins).mean()
    predRate = np.zeros(bins.size)
    border = self.get_border()[0]
    for i, b in enumerate(bins):
        ind = np.logical_and(border &gt;= b, border &lt; b + dbinsize)
        tmap = histogram2d(yloc[ind], xloc[ind], yedges, xedges)[0]
        tmap /= self.get_sampling_rate()
        predRate[i] = np.sum(fmap*tmap)/ tmap.sum()
    _results[&#39;DR BP&#39;] = np.abs(np.log((1 + borderData[&#39;distRate&#39;]) / (1 + predRate))).sum()/bins.size

    self.update_result(_results)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, filename=None, system=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Loads the spatial object
</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def load(self, filename=None, system=None):
    &#34;&#34;&#34;
    Loads the spatial object         
    
    Parameters
    ----------
    None
    
    Returns
    -------
    None        
    
    &#34;&#34;&#34;

    if system is None:
        system = self._system
    else:
        self._system = system
    if filename is None:
        filename = self._filename
    else:
        filename = self._filename
    loader = getattr(self, &#39;load_spatial_&#39;+ system)
    loader(filename)
    try:
        self.smooth_speed()
    except:
        logging.warning(self.get_system() + &#39; files may not have speed data!&#39;)
    if not np.array(self._ang_vel).any():
        self.set_ang_vel(self.calc_ang_vel())
    self.set_border(self.calc_border())</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.load_lfp"><code class="name flex">
<span>def <span class="ident">load_lfp</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Loads the composite lfp object
</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def load_lfp(self):
    &#34;&#34;&#34;
    Loads the composite lfp object         
    
    Parameters
    ----------
    None
    
    Returns
    -------
    None        
    
    &#34;&#34;&#34;

    
    self.lfp.load()</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.load_spatial_Axona"><code class="name flex">
<span>def <span class="ident">load_spatial_Axona</span></span>(<span>self, file_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Loads Axona format spatial data to the NSpatial() object</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def load_spatial_Axona(self, file_name):
    &#34;&#34;&#34;
    Loads Axona format spatial data to the NSpatial() object
    
    Parameters
    ----------
    None
    
    Returns
    -------
    None        
    
    &#34;&#34;&#34;

    try:
        f = open(file_name, &#39;rt&#39;)
        self._set_data_source(file_name)
        self._set_source_format(&#39;Axona&#39;)
        while True:
            line = f.readline()
            if line == &#39;&#39;:
                break
            elif line.startswith(&#39;time&#39;):
                spatial_data = np.loadtxt(f, dtype=&#39;float&#39;, usecols=range(5))
        self._set_time(spatial_data[:, 0])
        self._set_pos_x(spatial_data[:, 1]- np.min(spatial_data[:, 1]))
        self._set_pos_y(spatial_data[:, 2]- np.min(spatial_data[:, 2]))
        self._set_direction(spatial_data[:, 3])
        self._set_speed(spatial_data[:, 4])
        f.seek(0, 0)
        pixel_size = list(map(float, re.findall(r&#34;\d+.\d+|\d+&#34;, f.readline())))
        self.set_pixel_size(pixel_size)
        self.smooth_direction()
    except:
        logging.error(&#39;File does not exist or is open in another process!&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.load_spatial_NWB"><code class="name flex">
<span>def <span class="ident">load_spatial_NWB</span></span>(<span>self, file_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Loads HDF5 format spatial data to the NSpatial() object</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def load_spatial_NWB(self, file_name):
    &#34;&#34;&#34;
    Loads HDF5 format spatial data to the NSpatial() object
    
    Parameters
    ----------
    None
    
    Returns
    -------
    None        
    
    &#34;&#34;&#34;
    file_name, path = file_name.split(&#39;+&#39;)
    if os.path.exists(file_name):
        hdf = Nhdf()
        hdf.set_filename(file_name)

        _record_info = {}

        if path in hdf.f:
            g = hdf.f[path]
        elif &#39;/processing/Behavioural/Position&#39; in hdf.f:
            path = &#39;/processing/Behavioural/Position&#39;
            g = hdf.f[path]
            logging.info(&#39;Path for spatial data set to: &#39; + path)
        else:
            logging.error(&#39;Path for spatial data does not exist!&#39;)

        for key, value in g.attrs.items():
            _record_info[key] = value
        
        self.set_record_info(_record_info)

        if path+ &#39;/&#39;+ &#39;location&#39; in g:
            g_loc = g[path+ &#39;/&#39;+ &#39;location&#39;]
            data = hdf.get_dataset(group=g_loc, name=&#39;data&#39;)
            self._set_pos_x(data[:, 0])
            self._set_pos_y(data[:, 1])
            self._set_time(hdf.get_dataset(group=g_loc, name=&#39;timestamps&#39;))
        else:
            logging.error(&#39;Spatial location information not found!&#39;)

        if path+ &#39;/&#39;+ &#39;direction&#39; in g:
            g_dir = g[path+ &#39;/&#39;+ &#39;direction&#39;]
            data = hdf.get_dataset(group=g_dir, name=&#39;data&#39;)
            self._set_direction(data)
        else:
            logging.error(&#39;Spatial direction information not found!&#39;)

        if path+ &#39;/&#39;+ &#39;speed&#39; in g:
            g_speed = g[path+ &#39;/&#39;+ &#39;speed&#39;]
            data = hdf.get_dataset(group=g_speed, name=&#39;data&#39;)
            self._set_speed(data)
        else:
            logging.error(&#39;Spatial speed information not found!&#39;)

        if path+ &#39;/&#39;+ &#39;angular velocity&#39; in g:
            g_ang_vel = g[path+ &#39;/&#39;+ &#39;angular velocity&#39;]
            data = hdf.get_dataset(group=g_ang_vel, name=&#39;data&#39;)
            self.set_ang_vel(data)
        else:
            self.set_ang_vel(np.array([]))
            logging.warning(&#39;Spatial angular velocity information not found, will be calculated from direction!&#39;)

        hdf.close()</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.load_spatial_Neuralynx"><code class="name flex">
<span>def <span class="ident">load_spatial_Neuralynx</span></span>(<span>self, file_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Loads Neuralynx format spatial data to the NSpatial() object</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def load_spatial_Neuralynx(self, file_name):
    &#34;&#34;&#34;
    Loads Neuralynx format spatial data to the NSpatial() object
    
    Parameters
    ----------
    None
    
    Returns
    -------
    None        
    
    &#34;&#34;&#34;

    self._set_data_source(file_name)
    self._set_source_format(&#39;Neuralynx&#39;)

    # Format description for the NLX file:
    header_offset = 16*1024 # fixed for NLX files

    bytes_start_record = 2
    bytes_origin_id = 2
    bytes_videoRec_size = 2
    bytes_per_timestamp = 8
    bytes_per_bitfield = 4*400
    bytes_sncrc = 2
    bytes_per_xloc = 4
    bytes_per_yloc = 4
    bytes_per_angle = 4
    bytes_per_target = 4*50

    record_size = None
    with open(file_name, &#39;rb&#39;) as f:
        while True:
            line = f.readline()
            try:
                line = line.decode(&#39;UTF-8&#39;)
            except:
                break

            if line == &#39;&#39;:
                break
            if &#39;SamplingFrequency&#39; in line:
                self._set_sampling_rate(float(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line))))
            if &#39;RecordSize&#39; in line:
                record_size = int(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line)))
            if &#39;Time Opened&#39; in line:
                self._set_date(re.search(r&#39;\d+/\d+/\d+&#39;, line).group())
                self._set_time(re.search(r&#39;\d+:\d+:\d+&#39;, line).group())
            if &#39;FileVersion&#39; in line:
                self._set_file_version(line.split()[1])

        if not record_size:
            record_size = bytes_start_record+ \
                         bytes_origin_id+ \
                         bytes_videoRec_size+  \
                         bytes_per_timestamp+ \
                         bytes_per_bitfield+ \
                         bytes_sncrc+ \
                         bytes_per_xloc+ \
                         bytes_per_yloc+ \
                         bytes_per_angle+ \
                         bytes_per_target

        time_offset = bytes_start_record+ \
                         bytes_origin_id+ \
                         bytes_videoRec_size
        xloc_offset = time_offset+ \
                     bytes_per_timestamp+ \
                     bytes_per_bitfield+ \
                     bytes_sncrc
        yloc_offset = xloc_offset+bytes_per_xloc
        angle_offset = yloc_offset+bytes_per_xloc

        f.seek(0, 2)
        self._total_samples = int((f.tell()- header_offset)/ record_size)
        spatial_data = np.zeros([self._total_samples, 4])

        f.seek(header_offset)
        for i in np.arange(self._total_samples):
            sample_bytes = np.fromfile(f, dtype=&#39;uint8&#39;, count=record_size)
            spatial_data[i, 0] = int.from_bytes(sample_bytes[time_offset+ np.arange(bytes_per_timestamp)], byteorder=&#39;little&#39;, signed=False)
            spatial_data[i, 1] = int.from_bytes(sample_bytes[xloc_offset+ np.arange(bytes_per_xloc)], byteorder=&#39;little&#39;, signed=False)
            spatial_data[i, 2] = int.from_bytes(sample_bytes[yloc_offset+ np.arange(bytes_per_yloc)], byteorder=&#39;little&#39;, signed=False)
            spatial_data[i, 3] = int.from_bytes(sample_bytes[angle_offset+ np.arange(bytes_per_angle)], byteorder=&#39;little&#39;, signed=False)

        spatial_data[:, 0] /= 10**6
        spatial_data[:, 0] -= np.min(spatial_data[:, 0])
        self._timestamp = np.mean(np.diff(spatial_data[:, 0]))
        self._set_sampling_rate(1/self._timestamp)
        self._set_time(spatial_data[:, 0])
        self._set_pos_x(spatial_data[:, 1]- np.min(spatial_data[:, 1]))
        self._set_pos_y(spatial_data[:, 2]- np.min(spatial_data[:, 2]))
        self._set_direction(spatial_data[:, 3])
        # Neuralynx data does not have any speed information
        self.smooth_direction()</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.load_spike"><code class="name flex">
<span>def <span class="ident">load_spike</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Loads the composing spike object
</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def load_spike(self):
    &#34;&#34;&#34;
    Loads the composing spike object         
    
    Parameters
    ----------
    None
    
    Returns
    -------
    None        
    
    &#34;&#34;&#34;
    
    self.spike.load()</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.loc_auto_corr"><code class="name flex">
<span>def <span class="ident">loc_auto_corr</span></span>(<span>self, ftimes, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates the two-dimensional correlation of firing map which is the
map of the firing rate of the animal with respect to its location</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ftimes</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Timestamps of the spiking activity of a unit</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def loc_auto_corr(self, ftimes, **kwargs):
    &#34;&#34;&#34;
    Calculates the two-dimensional correlation of firing map which is the
    map of the firing rate of the animal with respect to its location
    
    Parameters
    ----------
    ftimes : ndarray
        Timestamps of the spiking activity of a unit        
    **kwargs
        Keyword arguments

    Returns
    -------
    dict
        Graphical data of the analysis

    &#34;&#34;&#34;
    graph_data = {}

    minPixel = kwargs.get(&#39;minPixel&#39;, 20)
    pixel = kwargs.get(&#39;pixel&#39;, 3)

    if &#39;update&#39; in kwargs.keys():
        del kwargs[&#39;update&#39;]
    placeData = self.place(ftimes, update=False, **kwargs)

    fmap = placeData[&#39;smoothMap&#39;]
    fmap[np.isnan(fmap)] = 0
    leny, lenx = fmap.shape

    xshift = np.arange(-(lenx-1), lenx)
    yshift = np.arange(-(leny-1), leny)

    corrMap = np.zeros((yshift.size, xshift.size))

    for J, ysh  in enumerate(yshift):
        for I, xsh in enumerate(xshift):
            if ysh &gt;= 0:
                map1YInd = np.arange(ysh, leny)
                map2YInd = np.arange(leny - ysh)
            elif ysh &lt; 0:
                map1YInd = np.arange(leny + ysh)
                map2YInd = np.arange(-ysh, leny)

            if xsh &gt;= 0:
                map1XInd = np.arange(xsh, lenx)
                map2XInd = np.arange(lenx - xsh)
            elif xsh &lt; 0:
                map1XInd = np.arange(lenx + xsh)
                map2XInd = np.arange(-xsh, lenx)
            map1 = fmap[tuple(np.meshgrid(map1YInd, map1XInd))]
            map2 = fmap[tuple(np.meshgrid(map2YInd, map2XInd))]
            if map1.size &lt; minPixel:
                corrMap[J, I] = -1
            else:
                corrMap[J, I] = corr_coeff(map1, map2)

    graph_data[&#39;corrMap&#39;] = corrMap
    graph_data[&#39;xshift&#39;] = xshift*pixel
    graph_data[&#39;yshift&#39;] = yshift*pixel

    return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.loc_rot_corr"><code class="name flex">
<span>def <span class="ident">loc_rot_corr</span></span>(<span>self, ftimes, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates the rotational correlation of the locational firing rate of the animal with
respect to location, also called firing map
</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ftimes</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Timestamps of the spiking activity of a unit</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">    def loc_rot_corr(self, ftimes, **kwargs):
        &#34;&#34;&#34;
        Calculates the rotational correlation of the locational firing rate of the animal with
        respect to location, also called firing map    
        
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit        
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis
    
        &#34;&#34;&#34;    
        
        graph_data = {}

        binsize = kwargs.get(&#39;binsize&#39;, 3) #degrees
#        filttype, filtsize = kwargs.get(&#39;filter&#39;, [&#39;b&#39;, 3])

        bins = np.arange(0, 360, binsize)
        placeData = self.place(ftimes, update=False, **kwargs)

        fmap = placeData[&#39;smoothMap&#39;]
        fmap[np.isnan(fmap)] = 0

        rotCorr = [corr_coeff(rot_2d(fmap, theta), fmap) for k, theta in enumerate(bins)]

        graph_data[&#39;rotAngle&#39;] = bins
        graph_data[&#39;rotCorr&#39;] = rotCorr

        return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.loc_shift"><code class="name flex">
<span>def <span class="ident">loc_shift</span></span>(<span>self, ftimes, shift_ind=array([-10,
-9,
-8,
-7,
-6,
-5,
-4,
-3,
-2,
-1,
0,
1,
2,
3,
4,
5,
6,
7,
8,
9,
10]), **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Analysis of firing specificity of the unit with respect to animal's location
to oberve whether it represents past location of the animal or anicipates a
future location.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ftimes</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Timestamps of the spiking activity of a unit</dd>
<dt><strong><code>shift_ind</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Index of spatial resolution shift for the spike event time. Shift -1
implies shift to the past by 1 spatial time resolution, and +2 implies
shift to the future by 2 spatial time resoultion.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def loc_shift(self, ftimes, shift_ind=np.arange(-10, 11), **kwargs):
    &#34;&#34;&#34;
    Analysis of firing specificity of the unit with respect to animal&#39;s location
    to oberve whether it represents past location of the animal or anicipates a
    future location.
    
    Parameters
    ----------
    ftimes : ndarray
        Timestamps of the spiking activity of a unit
    shift_ind : ndarray
        Index of spatial resolution shift for the spike event time. Shift -1
        implies shift to the past by 1 spatial time resolution, and +2 implies
        shift to the future by 2 spatial time resoultion.
    **kwargs
        Keyword arguments

    Returns
    -------
    dict
        Graphical data of the analysis

    &#34;&#34;&#34;
    
    _results = oDict()
    graph_data = {}

    brAdjust = kwargs.get(&#39;brAdjust&#39;, False)
    pixel = kwargs.get(&#39;pixel&#39;, 3)
    chop_bound = kwargs.get(&#39;chop_bound&#39;, 5)
    _filter = kwargs.get(&#39;filter&#39;, [&#39;b&#39;, 5])

    # limit = 0 implies enirely random shuffle, limit = &#39;x&#39; implies nshuff number of shuffles in the range [-x x]
    shift = shift_ind/self.get_sampling_rate()
    shiftlen = shift.size
    dur = self.get_time()[-1]
    skaggs = np.zeros((shiftlen,))
    sparsity = np.zeros((shiftlen,))
    coherence = np.zeros((shiftlen,))

    for i in np.arange(shiftlen):
        shift_ftimes = ftimes+ shift[i]
        # Wrapping up the time
        shift_ftimes[shift_ftimes &gt; dur] -= dur
        shift_ftimes[shift_ftimes &lt; 0] += dur

        placeData = self.place(shift_ftimes, pixel=pixel, filter=_filter, \
                              brAdjust=brAdjust, chop_bound=chop_bound, update=False)
        skaggs[i] = self.skaggs_info(placeData[&#39;fmap&#39;], placeData[&#39;tmap&#39;])
        sparsity[i] = self.spatial_sparsity(placeData[&#39;fmap&#39;], placeData[&#39;tmap&#39;])
        coherence[i] = np.corrcoef(placeData[&#39;fmap&#39;][placeData[&#39;tmap&#39;] != 0].flatten(), \
                        placeData[&#39;smoothMap&#39;][placeData[&#39;tmap&#39;] != 0].flatten())[0, 1]

    graph_data[&#39;skaggs&#39;] = skaggs
    graph_data[&#39;sparsity&#39;] = sparsity
    graph_data[&#39;coherence&#39;] = coherence

    graph_data[&#39;shiftTime&#39;] = shift

    # Find out the optimum skaggs location
    shiftUpsamp = np.arange(shift[0], shift[-1], np.mean(np.diff(shift))/4)
    skaggsUpsamp = np.interp(shiftUpsamp, shift, skaggs)
    sparsityUpsamp = np.interp(shiftUpsamp, shift, sparsity)
    coherenceUpsamp = np.interp(shiftUpsamp, shift, coherence)

    imax = sg.argrelmax(skaggsUpsamp)[0]
    maxloc = find(skaggsUpsamp[imax] == skaggsUpsamp.max())
    _results[&#39;Loc Opt Shift Skaggs&#39;] = np.nan if maxloc.size != 1 else (np.nan if imax[maxloc] == 0 or imax[maxloc] == skaggsUpsamp.size else shiftUpsamp[imax[maxloc]])

    imin = sg.argrelmin(sparsityUpsamp)[0]
    minloc = find(sparsityUpsamp[imin] == sparsityUpsamp.min())
    _results[&#39;Loc Opt Shift Sparsity&#39;] = np.nan if minloc.size != 1 else (np.nan if imin[minloc] == 0 or imin[minloc] == sparsityUpsamp.size else shiftUpsamp[imin[minloc]])

    imax = sg.argrelmax(coherenceUpsamp)[0]
    maxloc = find(coherenceUpsamp[imax] == coherenceUpsamp.max())
    _results[&#39;Loc Opt Shift Coherence&#39;] = np.nan if maxloc.size != 1 else (np.nan if imax[maxloc] == 0 or imax[maxloc] == coherenceUpsamp.size else shiftUpsamp[imax[maxloc]])

    self.update_result(_results)

    return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.loc_shuffle"><code class="name flex">
<span>def <span class="ident">loc_shuffle</span></span>(<span>self, ftimes, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Shuffling analysis of the unit to see if the locational firing specifity
is by chance or actually correlated to the location of the animal</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ftimes</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Timestamps of the spiking activity of a unit</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def loc_shuffle(self, ftimes, **kwargs):
    &#34;&#34;&#34;
    Shuffling analysis of the unit to see if the locational firing specifity
    is by chance or actually correlated to the location of the animal
    
    Parameters
    ----------
    ftimes : ndarray
        Timestamps of the spiking activity of a unit
    **kwargs
        Keyword arguments

    Returns
    -------
    dict
        Graphical data of the analysis

    &#34;&#34;&#34;
    
    _results = oDict()
    graph_data = {}

    nshuff = kwargs.get(&#39;nshuff&#39;, 500)
    limit = kwargs.get(&#39;limit&#39;, 0)
    bins = kwargs.get(&#39;bins&#39;, 100)
    brAdjust = kwargs.get(&#39;brAdjust&#39;, False)
    pixel = kwargs.get(&#39;pixel&#39;, 3)
    chop_bound = kwargs.get(&#39;chop_bound&#39;, 5)
    filter = kwargs.get(&#39;filter&#39;, [&#39;b&#39;, 5])
    # limit = 0 implies enirely random shuffle, limit = &#39;x&#39; implies nshuff number of shuffles in the range [-x x]
    dur = self.get_time()[-1]
    shift = nprand.uniform(low=limit- dur, high=dur- limit, size=nshuff)
    skaggs = np.zeros((nshuff,))
    sparsity = np.zeros((nshuff,))
    coherence = np.zeros((nshuff,))
    for i in np.arange(nshuff):
        shift_ftimes = ftimes+ shift[i]
        # Wrapping up the time
        shift_ftimes[shift_ftimes &gt; dur] -= dur
        shift_ftimes[shift_ftimes &lt; 0] += dur

        placeData = self.place(shift_ftimes, filter=filter, \
                      chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)
        skaggs[i] = self.skaggs_info(placeData[&#39;fmap&#39;], placeData[&#39;tmap&#39;])
        sparsity[i] = self.spatial_sparsity(placeData[&#39;fmap&#39;], placeData[&#39;tmap&#39;])
        coherence[i] = np.corrcoef(placeData[&#39;fmap&#39;][placeData[&#39;tmap&#39;] != 0].flatten(), \
                        placeData[&#39;smoothMap&#39;][placeData[&#39;tmap&#39;] != 0].flatten())[0, 1]

    graph_data[&#39;skaggs&#39;] = skaggs
    graph_data[&#39;coherence&#39;] = coherence
    graph_data[&#39;sparsity&#39;] = sparsity

    placeData = self.place(ftimes, pixel=pixel, filter=filter, brAdjust=brAdjust,\
                          chop_bound=chop_bound, update=False)
    graph_data[&#39;refSkaggs&#39;] = self.skaggs_info(placeData[&#39;fmap&#39;], placeData[&#39;tmap&#39;])
    graph_data[&#39;refSparsity&#39;] = self.spatial_sparsity(placeData[&#39;fmap&#39;], placeData[&#39;tmap&#39;])
    graph_data[&#39;refCoherence&#39;] = np.corrcoef(placeData[&#39;fmap&#39;][placeData[&#39;tmap&#39;] != 0].flatten(), \
                        placeData[&#39;smoothMap&#39;][placeData[&#39;tmap&#39;] != 0].flatten())[0, 1]

    graph_data[&#39;skaggsCount&#39;], graph_data[&#39;skaggsEdges&#39;] = np.histogram(skaggs, bins=bins)
    graph_data[&#39;skaggs95&#39;] = np.percentile(skaggs, 95)

    graph_data[&#39;sparsityCount&#39;], graph_data[&#39;sparsityEdges&#39;] = np.histogram(sparsity, bins=bins)
    graph_data[&#39;sparsity05&#39;] = np.percentile(sparsity, 5)

    graph_data[&#39;coherenceCount&#39;], graph_data[&#39;coherenceEdges&#39;] = np.histogram(coherence, bins=bins)
    graph_data[&#39;coherence95&#39;] = np.percentile(coherence, 95)

    _results[&#39;Loc Skaggs 95&#39;] = np.percentile(skaggs, 95)
    _results[&#39;Loc Sparsity 05&#39;] = np.percentile(sparsity, 95)
    _results[&#39;Loc Coherence 95&#39;] = np.percentile(coherence, 95)

    self.update_result(_results)

    return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.loc_time_lapse"><code class="name flex">
<span>def <span class="ident">loc_time_lapse</span></span>(<span>self, ftimes, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates the firing rate map and idnetifies the location of the spiking events
at certain intervals. This method is useful in observing the evolution of
unit-activity as the animal traverses the environment.</p>
<p>Following intervals ar used:
0-1min, 0-2min, 0-4min, 0-8min, 0-16min or 0-end depending on the recording duration
0-1min, 1-2min, 2-4min, 4-8min, 8-16min or 16-end depending on the recording duration</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ftimes</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Timestamps of the spiking activity of a unit</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def loc_time_lapse(self, ftimes, **kwargs):
    &#34;&#34;&#34;
    Calculates the firing rate map and idnetifies the location of the spiking events
    at certain intervals. This method is useful in observing the evolution of
    unit-activity as the animal traverses the environment.
    
    Following intervals ar used:
        0-1min, 0-2min, 0-4min, 0-8min, 0-16min or 0-end depending on the recording duration
        0-1min, 1-2min, 2-4min, 4-8min, 8-16min or 16-end depending on the recording duration
            
    Parameters
    ----------
    ftimes : ndarray
        Timestamps of the spiking activity of a unit
    **kwargs
        Keyword arguments

    Returns
    -------
    dict
        Graphical data of the analysis
    &#34;&#34;&#34;
    
    graph_data = oDict()
    pixel = kwargs.get(&#39;pixel&#39;, 3)
    chop_bound = kwargs.get(&#39;chop_bound&#39;, 5)
    filter = kwargs.get(&#39;filter&#39;, [&#39;b&#39;, 5])
    brAdjust = kwargs.get(&#39;brAdjust&#39;, True)

    lim = [0, 1*60]
    graph_data[&#39;0To1min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                      chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

    lim = [0, 2*60]
    graph_data[&#39;0To2min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                      chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

    lim = [0, 4*60]
    graph_data[&#39;0To4min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                      chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

    lim = [0, 8*60]
    graph_data[&#39;0To8min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                      chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

    # 0-1min, 1-2min,  2-4min, 4-8min
    lim = [1*60, 2*60]
    graph_data[&#39;1To2min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                      chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

    lim = [2*60, 4*60]
    graph_data[&#39;2To4min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                      chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

    lim = [4*60, 8*60]
    graph_data[&#39;4To8min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                      chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

    ## 0-16min, 8-16min 0-20min, 16-20min

    if self.get_duration() &gt; 8*60:
        if self.get_duration() &gt; 16*60:
            lim = [0, 16*60]
            graph_data[&#39;0To16min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                      chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

            lim = [8*60, 16*60]
            graph_data[&#39;8To16min&#39;] = self.place(ftimes, range=lim, filter=filter, \
                      chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

            lim = [0, self.get_duration()]
            graph_data[&#39;0ToEnd&#39;] = self.place(ftimes, range=lim, filter=filter, \
                      chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

            lim = [16*60, self.get_duration()]
            graph_data[&#39;16ToEnd&#39;] = self.place(ftimes, range=lim, filter=filter, \
                      chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

        else:
            lim = [0, self.get_duration()]
            graph_data[&#39;0ToEnd&#39;] = self.place(ftimes, range=lim, filter=filter, \
                      chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

            lim = [8*60, self.get_duration()]
            graph_data[&#39;8ToEnd&#39;] = self.place(ftimes, range=lim, filter=filter, \
                      chop_bound=chop_bound, pixel=pixel, brAdjust=brAdjust, update=False)

        return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.multiple_regression"><code class="name flex">
<span>def <span class="ident">multiple_regression</span></span>(<span>self, ftimes, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Multiple-rgression analysis where firing rate for each variable, namely
location, head-direction, speed, AHV, and distance from border, are used
to regress the instantaneous firing rate of the unit.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ftimes</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Timestamps of the spiking activity of a unit</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">    def multiple_regression(self, ftimes, **kwargs):        
        &#34;&#34;&#34;
        Multiple-rgression analysis where firing rate for each variable, namely
        location, head-direction, speed, AHV, and distance from border, are used
        to regress the instantaneous firing rate of the unit.
                
        Parameters
        ----------
        ftimes : ndarray
            Timestamps of the spiking activity of a unit   
        **kwargs
            Keyword arguments
 
        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;
        
        _results = oDict()
        graph_data = oDict()
        subsampInterv = kwargs.get(&#39;subsampInterv&#39;, 0.1)
        episode = kwargs.get(&#39;episode&#39;, 120)
        nrep = kwargs.get(&#39;nrep&#39;, 1000)
        sampRate = 1/subsampInterv
        stamp = subsampInterv
 
        a_size = np.round(self.get_duration(), 4)
        time = np.linspace(
            0, a_size, endpoint=True,
            num=np.round(a_size/stamp)+1)
        time = np.round(time, 4)
        Y = histogram(ftimes, time)[0]* sampRate # Instant firing rate

        nt = time.size
        xloc, yloc, loc, hd, speed, ang_vel, distBorder = list(np.zeros((7, nt)))
        tmp = self.place(ftimes)
        placeRate, xedges, yedges = (tmp[&#39;smoothMap&#39;], tmp[&#39;xedges&#39;], tmp[&#39;yedges&#39;])
        placeRate[np.isnan(placeRate)] = 0
        for i in np.arange(nt):
            ind = find(np.logical_and(self.get_time() &gt;= time[i], self.get_time() &lt; time[i]+ stamp))
            if len(ind) == 0:
                continue
            xloc[i] = np.median(self.get_pos_x()[ind])
            yloc[i] = np.median(self.get_pos_y()[ind])
            if histogram(yloc[i], yedges)[1] &lt; yedges.size and histogram(xloc[i], xedges)[1] &lt; xedges.size:
                loc[i] = placeRate[histogram(yloc[i], yedges)[1], histogram(xloc[i], xedges)[1]]
            hd[i] = np.median(self.get_direction()[ind])
            speed[i] = np.median(self.get_speed()[ind])
            ang_vel[i] = np.median(self.get_ang_vel()[ind])
            distBorder[i] = np.median(self.get_border()[0][ind])

        tmp = self.hd_rate(ftimes, update=False)
        hd_rate, hdBins = (tmp[&#39;hdRate&#39;], tmp[&#39;bins&#39;])
        cs = CircStat()
        cs.set_theta(hd)
        hd = hd_rate[cs.circ_histogram(hdBins)[1]] # replaced by corresponding rate
        # Speed+ ang_vel will be linearly modelled, so no transformation required; ang_vel will be replaced by the non-linear rate
        tmp = self.border(ftimes, update=False)
        borderRate, borderBins = (tmp[&#39;distRate&#39;], tmp[&#39;distBins&#39;])
        distBorder = borderRate[histogram(distBorder, borderBins)[1]] # replaced by corresponding rate

        ns = int(episode/stamp) # row to select in random

        X = np.vstack((loc, hd, speed, ang_vel, distBorder)).transpose()
        lm = LinearRegression(fit_intercept=True, normalize=True)

        Rsq = np.zeros((nrep, 6))
        for i in np.arange(nrep):
            ind = np.random.permutation(time.size)[:ns]
            lm.fit(X[ind, :], Y[ind])
            Rsq[i, 0] = lm.score(X[ind, :], Y[ind])
            for j in np.arange(5):
                varind = np.array([k for k in range(5) if k != j])
                lm.fit(X[np.ix_(ind, varind)], Y[ind]) #np.ix_ is used for braodcasting the index arrays
                Rsq[i, j+1] = Rsq[i, 0]- lm.score(X[np.ix_(ind, varind)], Y[ind])

        meanRsq = Rsq.mean(axis=0)
        # Regresssion parameters are alays stored in following order
        varOrder = [&#39;Total&#39;, &#39;Loc&#39;, &#39;HD&#39;, &#39;Speed&#39;, &#39;Ang Vel&#39;, &#39;Dist Border&#39;]

#        graph_data[&#39;order&#39;] = varOrder
        graph_data[&#39;Rsq&#39;] = Rsq
        graph_data[&#39;meanRsq&#39;] = meanRsq
        graph_data[&#39;maxRsq&#39;] = Rsq.max(axis=0)
        graph_data[&#39;minRsq&#39;] = Rsq.min(axis=0)
        graph_data[&#39;stdRsq&#39;] = Rsq.std(axis=0)
        
        _results[&#39;Mult Rsq&#39;] = meanRsq[0]
        for i, key in enumerate(varOrder):
            if i &gt; 0:
                _results[&#39;Semi Rsq &#39;+key] = meanRsq[i]
        self.update_result(_results)

        return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.non_moving_periods"><code class="name flex">
<span>def <span class="ident">non_moving_periods</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns a number of tuples indicating ranges where the subject is not moving</p>
<h2 id="kwargs">kwargs</h2>
<dl>
<dt><strong><code>should_smooth</code></strong> :&ensp;<code>bool</code></dt>
<dd>flags if the speed data should be smoothed, default False</dd>
<dt><strong><code>min_range</code></strong> :&ensp;<code>float</code></dt>
<dd>the minimum amount of time that the subject should not be moving for</dd>
<dt><strong><code>moving_thresh</code></strong> :&ensp;<code>float</code></dt>
<dd>any speed above this thresh is considered to be movement</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def non_moving_periods(self, **kwargs):
    &#34;&#34;&#34;
    Returns a number of tuples indicating ranges where the subject is not moving

    kwargs
    ------
    should_smooth : bool
        flags if the speed data should be smoothed, default False
    min_range : float
        the minimum amount of time that the subject should not be moving for
    moving_thresh : float
        any speed above this thresh is considered to be movement
    &#34;&#34;&#34;
    should_smooth = kwargs.get(&#34;should_smooth&#34;, False)
    min_range = kwargs.get(&#34;min_range&#34;, 150)
    moving_thresh = kwargs.get(&#34;moving_thresh&#34;, 2.5)

    if should_smooth:
        self.smooth_speed()
    not_moving = self.get_speed() &lt; moving_thresh

    return find_true_ranges(
        self.get_time(), not_moving, min_range)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.place"><code class="name flex">
<span>def <span class="ident">place</span></span>(<span>self, ftimes, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates the two-dimensional firing rate of the unit with respect to
the location of the animal in the environment. This is called Firing map.</p>
<p>Specificity indices are measured to assess the quality of location-specific firing of the unit.</p>
<p>This method also plot the events of spike occurring superimposed on the
trace of the animal in the arena, commonly known as Spike Plot.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ftimes</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Timestamps of the spiking activity of a unit</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def place(self, ftimes, **kwargs):
    &#34;&#34;&#34;
    Calculates the two-dimensional firing rate of the unit with respect to
    the location of the animal in the environment. This is called Firing map.
    
    Specificity indices are measured to assess the quality of location-specific firing of the unit.
    
    This method also plot the events of spike occurring superimposed on the
    trace of the animal in the arena, commonly known as Spike Plot.
    
    Parameters
    ----------
    ftimes : ndarray
        Timestamps of the spiking activity of a unit
    **kwargs
        Keyword arguments

    Returns
    -------
    dict
        Graphical data of the analysis
    &#34;&#34;&#34;
    
    _results = oDict()
    graph_data = {}
    update = kwargs.get(&#39;update&#39;, True)
    pixel = kwargs.get(&#39;pixel&#39;, 3)
    chop_bound = kwargs.get(&#39;chop_bound&#39;, 5)
    filttype, filtsize = kwargs.get(&#39;filter&#39;, [&#39;b&#39;, 5])
    lim = kwargs.get(&#39;range&#39;, [0, self.get_duration()])
    brAdjust = kwargs.get(&#39;brAdjust&#39;, True)
    thresh = kwargs.get(&#39;fieldThresh&#39;, 0.2)
    required_neighbours = kwargs.get(&#39;minPlaceFieldNeighbours&#39;, 9)
    smooth_place = kwargs.get(&#39;smoothPlace&#39;, False)
    separate_border_data = kwargs.get(
        &#34;separateBorderData&#34;, False)

    # xedges = np.arange(0, np.ceil(np.max(self._pos_x)), pixel)
    # yedges = np.arange(0, np.ceil(np.max(self._pos_y)), pixel)

    # Update the border to match the requested pixel size
    if separate_border_data:
        self.set_border(
            separate_border_data.calc_border(**kwargs))
        times = self._time
        lower, upper = (times.min(), times.max())
        new_times = separate_border_data._time
        sample_spatial_idx = (
            (new_times &lt;= upper) &amp; (new_times &gt;= lower)).nonzero()
        self._border_dist = self._border_dist[sample_spatial_idx]
    else:  
        self.set_border(self.calc_border(**kwargs))

    xedges = self._xbound
    yedges = self._ybound

    spikeLoc = self.get_event_loc(ftimes, **kwargs)[1]
    posX = self._pos_x[np.logical_and(self.get_time() &gt;= lim[0], self.get_time() &lt;= lim[1])]
    posY = self._pos_y[np.logical_and(self.get_time() &gt;= lim[0], self.get_time() &lt;= lim[1])]

    tmap, yedges, xedges = histogram2d(posY, posX, yedges, xedges)

    if tmap.shape[0] != tmap.shape[1] &amp; np.abs(tmap.shape[0]- tmap.shape[1]) &lt;= chop_bound:
        tmap = chop_edges(tmap, min(tmap.shape), min(tmap.shape))[2]
    tmap /= self.get_sampling_rate()

    ybin, xbin = tmap.shape
    xedges = np.arange(xbin)*pixel
    yedges = np.arange(ybin)*pixel

    spike_count = histogram2d(spikeLoc[1], spikeLoc[0], yedges, xedges)[0]
    fmap = np.divide(spike_count, tmap, out=np.zeros_like(spike_count), where=tmap != 0)

    if brAdjust:
        nfmap = fmap/ fmap.max()
        if np.sum(np.logical_and(nfmap &gt;= 0.2, tmap != 0)) &gt;= 0.8*nfmap[tmap != 0].flatten().shape[0]:
            back_rate = np.mean(fmap[np.logical_and(nfmap &gt;= 0.2, nfmap &lt; 0.4)])
            fmap -= back_rate
            fmap[fmap &lt; 0] = 0

    if filttype is not None:
        smoothMap = smooth_2d(fmap, filttype, filtsize)
    else :
        smoothMap = fmap
    
    if smooth_place:
        pmap = smoothMap
    else:
        pmap = fmap
    
    pmap[tmap == 0] = None
    pfield, largest_group = NSpatial.place_field(
        pmap, thresh, required_neighbours)
    # if largest_group == 0:
    #     if smooth_place:
    #         info = &#34;where the place field was calculated from smoothed data&#34;
    #     else:
    #         info = &#34;where the place field was calculated from raw data&#34;
    #     logging.info(
    #         &#34;Lack of high firing neighbours to identify place field &#34; +
    #         info)
    centroid = NSpatial.place_field_centroid(pfield, pmap, largest_group)
    #centroid is currently in co-ordinates, convert to pixels
    centroid = centroid * pixel + (pixel * 0.5)
    #flip x and y
    centroid = centroid[::-1]
    
    p_shape = pfield.shape
    maxes = [xedges.max(), yedges.max()]
    scales = (
         maxes[0] / p_shape[1],
         maxes[1] / p_shape[0])
    co_ords = np.array(np.where(pfield == largest_group))
    boundary = [None, None]
    for i in range(2):
        j = (i + 1) % 2
        boundary[i] = (
            co_ords[j].min() * scales[i],
            np.clip((co_ords[j].max()+1) * scales[i], 0, maxes[i]))
    inside_x = (
        (boundary[0][0] &lt;= spikeLoc[0]) &amp;
        (spikeLoc[0] &lt;= boundary[0][1]))
    inside_y = (
        (boundary[1][0] &lt;= spikeLoc[1]) &amp;
        (spikeLoc[1] &lt;= boundary[1][1]))
    co_ords = np.nonzero(np.logical_and(inside_x, inside_y))

    if update:
        _results[&#39;Spatial Skaggs&#39;] = self.skaggs_info(fmap, tmap)
        _results[&#39;Spatial Sparsity&#39;] = self.spatial_sparsity(fmap, tmap)
        _results[&#39;Spatial Coherence&#39;] = np.corrcoef(fmap[tmap != 0].flatten(), smoothMap[tmap != 0].flatten())[0, 1]
        _results[&#39;Found strong place field&#39;] = (largest_group != 0)
        _results[&#39;Place field Centroid x&#39;] = centroid[0]
        _results[&#39;Place field Centroid y&#39;] = centroid[1]
        _results[&#39;Place field Boundary x&#39;] = boundary[0]
        _results[&#39;Place field Boundary y&#39;] = boundary[1]
        _results[&#39;Number of Spikes in Place Field&#39;] = co_ords[0].size
        _results[&#39;Percentage of Spikes in Place Field&#39;] = co_ords[0].size*100 / ftimes.size
        self.update_result(_results)

    smoothMap[tmap == 0] = None

    graph_data[&#39;posX&#39;] = posX
    graph_data[&#39;posY&#39;] = posY
    graph_data[&#39;fmap&#39;] = fmap
    graph_data[&#39;smoothMap&#39;] = smoothMap
    graph_data[&#39;firingMap&#39;] = fmap
    graph_data[&#39;tmap&#39;] = tmap
    graph_data[&#39;xedges&#39;] = xedges
    graph_data[&#39;yedges&#39;] = yedges
    graph_data[&#39;spikeLoc&#39;] = spikeLoc
    graph_data[&#39;placeField&#39;] = pfield
    graph_data[&#39;largestPlaceGroup&#39;] = largest_group
    graph_data[&#39;placeBoundary&#39;] = boundary
    graph_data[&#39;indicesInPlaceField&#39;] = co_ords
    graph_data[&#39;centroid&#39;] = centroid

    return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.place_field_centroid_zscore"><code class="name flex">
<span>def <span class="ident">place_field_centroid_zscore</span></span>(<span>self, ftimes, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>A naive method to find the centroid of the place field using the
z-score of the normal distribution to remove outliers,
and then averaging remaining locations' co-ordinates.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ftimes</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Timestamps of the spiking activity of a unit</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>The centroid of the place field</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def place_field_centroid_zscore(self, ftimes, **kwargs):
    &#34;&#34;&#34;
    A naive method to find the centroid of the place field using the 
    z-score of the normal distribution to remove outliers, 
    and then averaging remaining locations&#39; co-ordinates.
            
    Parameters
    ----------
    ftimes : ndarray
        Timestamps of the spiking activity of a unit
    **kwargs
        Keyword arguments

    Returns
    -------
    ndarray
        The centroid of the place field
    &#34;&#34;&#34;

    _results = oDict()
    update = kwargs.get(&#39;update&#39;, True)
    lim = kwargs.get(&#39;range&#39;, [0, self.get_duration()])
    remove_outliers = kwargs.get(&#39;remove_outliers&#39;, True)
    threshold = kwargs.get(&#39;z_threshold&#39;, 2)

    spikeLoc = self.get_event_loc(ftimes, **kwargs)[1]
    
    if remove_outliers:
        z_scores = sc.stats.zscore(spikeLoc, axis=1)
        # Filter out locations with x or y outside of 3 std devs.
        filter_array = np.logical_and((abs(z_scores[0]) &lt; threshold).astype(bool), (abs(z_scores[1]) &lt; threshold).astype(bool))
        spikeLoc[0] = spikeLoc[0][filter_array]
        spikeLoc[1] = spikeLoc[1][filter_array]

    centroid = np.average(spikeLoc, axis=1)
    if update:
        _results[&#39;Place field Centroid x&#39;] = centroid[0]
        _results[&#39;Place field Centroid y&#39;] = centroid[1]
        self.update_result(_results)
    return centroid</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.save_to_hdf5"><code class="name flex">
<span>def <span class="ident">save_to_hdf5</span></span>(<span>self, file_name=None, system=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Save spatial dataset to HDF5 file
</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def save_to_hdf5(self, file_name=None, system=None):
    &#34;&#34;&#34;
    Save spatial dataset to HDF5 file         
    
    Parameters
    ----------
    None
    
    Returns
    -------
    None        
    
    &#34;&#34;&#34;

    hdf = Nhdf()
    if file_name and system:
        if os.path.exists(file_name):
            self.set_filename(file_name)
            self.set_system(system)
            self.load()
        else:
            logging.error(&#39;Specified file cannot be found!&#39;)

    hdf.save_spatial(spatial=self)
    hdf.close()</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.set_ang_vel"><code class="name flex">
<span>def <span class="ident">set_ang_vel</span></span>(<span>self, ang_vel)</span>
</code></dt>
<dd>
<section class="desc"><p>Sets the angular head velocity (AHV) of the animal</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ang_vel</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Angular head velocity (AHV) of the animal</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_ang_vel(self, ang_vel):
    &#34;&#34;&#34;
    Sets the angular head velocity (AHV) of the animal
            
    Parameters
    ----------
    ang_vel : ndarray
        Angular head velocity (AHV) of the animal

    Returns
    -------
    None

    &#34;&#34;&#34;
    
    self._ang_vel = ang_vel</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.set_border"><code class="name flex">
<span>def <span class="ident">set_border</span></span>(<span>self, border)</span>
</code></dt>
<dd>
<section class="desc"><p>Sets the distance of the animal from the arena border</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>border</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Distance of the animal from the arena border</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_border(self, border):
    &#34;&#34;&#34;
    Sets the distance of the animal from the arena border
            
    Parameters
    ----------
    border : ndarray
        Distance of the animal from the arena border

    Returns
    -------
    None

    &#34;&#34;&#34;
    
    self._border_dist = border[0]
    self._xbound = border[1]
    self._ybound = border[2]
    self._dist_map = border[3]</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.set_event"><code class="name flex">
<span>def <span class="ident">set_event</span></span>(<span>self, event, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Sets the NEvent() object to NSpatial().
</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>event</code></strong></dt>
<dd>NEvent or its childclass or NEvent() object</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>NEvent</code>()</dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_event(self, event, **kwargs):
    &#34;&#34;&#34;
    Sets the NEvent() object to NSpatial().         
    
    Parameters
    ----------
    event
        NEvent or its childclass or NEvent() object
    
    Returns
    -------
    NEvent()
    
    &#34;&#34;&#34;
    
    if event is isinstance(event, NEvent):
        self.event = event
    else:
        cls = NEvent if not event else event
        event = cls(**kwargs)
    self.event = event</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.set_event_filename"><code class="name flex">
<span>def <span class="ident">set_event_filename</span></span>(<span>self, filename=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Sets the filename for the event</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Full file of the event dataset</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_event_filename(self, filename=None):
    &#34;&#34;&#34;
    Sets the filename for the event
    
    Parameters
    ----------
    filename : str
        Full file of the event dataset
    
    Returns
    -------
    None
    
    &#34;&#34;&#34;
    
    self.event.set_filename(filename)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.set_event_name"><code class="name flex">
<span>def <span class="ident">set_event_name</span></span>(<span>self, name=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Sets the name of the event object.
</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the vent dataset</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_event_name(self, name=None):
    &#34;&#34;&#34;
    Sets the name of the event object.         
    
    Parameters
    ----------
    name : str
        Name of the vent dataset
    
    Returns
    -------
    None
    
    &#34;&#34;&#34;
    
    self.event.set_name(name)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.set_lfp"><code class="name flex">
<span>def <span class="ident">set_lfp</span></span>(<span>self, lfp, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Adds the NLfp object to NSpatial object </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>lfp</code></strong> :&ensp;<code>NLfp</code></dt>
<dd>NLfp object to be added to the NSpatial object. If no spike object
is provided, a new NLfp() object is created.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword argumemts for creating the new NLfp instance</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_lfp(self, lfp, **kwargs):
    &#34;&#34;&#34;
    Adds the NLfp object to NSpatial object 
            
    Parameters
    ----------
    lfp : NLfp
        NLfp object to be added to the NSpatial object. If no spike object
        is provided, a new NLfp() object is created.
    **kwargs
        Keyword argumemts for creating the new NLfp instance

    Returns
    -------
    None

    &#34;&#34;&#34;
    
    if lfp is isinstance(lfp, NLfp):
        self.lfp = lfp
    else:
        cls = NLfp if not lfp else lfp
        lfp = cls(**kwargs)
    self.lfp = lfp</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.set_lfp_filename"><code class="name flex">
<span>def <span class="ident">set_lfp_filename</span></span>(<span>self, filename=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Sets file name of the lfp dataset</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Full file directory of the lfp dataset</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_lfp_filename(self, filename=None):
    &#34;&#34;&#34;
    Sets file name of the lfp dataset
    
    Parameters
    ----------
    name : str
        Full file directory of the lfp dataset
    
    Returns
    -------
    None
    &#34;&#34;&#34;
    self.lfp.set_filename(filename)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.set_lfp_name"><code class="name flex">
<span>def <span class="ident">set_lfp_name</span></span>(<span>self, name=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Sets the name of the lfp dataset</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the lfp dataset</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_lfp_name(self, name=None):
    &#34;&#34;&#34;
    Sets the name of the lfp dataset
    
    Parameters
    ----------
    name : str
        Name of the lfp dataset
    
    Returns
    -------
    None

    &#34;&#34;&#34;
    
    self.lfp.set_name(name)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.set_pixel_size"><code class="name flex">
<span>def <span class="ident">set_pixel_size</span></span>(<span>self, pixel_size)</span>
</code></dt>
<dd>
<section class="desc"><p>Sets the size of pixel size by which the entire foraged arena is tessellated</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>pixel_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Pixel size of the foraged arena</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_pixel_size(self, pixel_size):
    &#34;&#34;&#34;
    Sets the size of pixel size by which the entire foraged arena is tessellated
    
    Parameters
    ----------
    pixel_size : int
        Pixel size of the foraged arena
    Returns
    -------
    None

    &#34;&#34;&#34;
    
    self._pixel_size = pixel_size</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.set_spike"><code class="name flex">
<span>def <span class="ident">set_spike</span></span>(<span>self, spike, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Adds the NSpike object to NSpatial object </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>spike</code></strong> :&ensp;<code>NSpike</code></dt>
<dd>NSpike object to be added to the NSpatial object. If no spike object
is provided, a new NSpike() object is created.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword argumemts for creating the new NSpike instance</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_spike(self, spike, **kwargs):
    &#34;&#34;&#34;
    Adds the NSpike object to NSpatial object 
            
    Parameters
    ----------
    spike : NSpike
        NSpike object to be added to the NSpatial object. If no spike object
        is provided, a new NSpike() object is created.
    **kwargs
        Keyword argumemts for creating the new NSpike instance

    Returns
    -------
    None

    &#34;&#34;&#34;

    
    if spike is isinstance(spike, NSpike):
        self.spike = spike
    else:
        cls = NSpike if not spike else spike
        spike = cls(**kwargs)
    self.spike = spike</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.set_spike_filename"><code class="name flex">
<span>def <span class="ident">set_spike_filename</span></span>(<span>self, filename=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Sets file name of the spike dataset</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Full file directory of the spike dataset</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_spike_filename(self, filename=None):
    &#34;&#34;&#34;
    Sets file name of the spike dataset
    
    Parameters
    ----------
    name : str
        Full file directory of the spike dataset
    
    Returns
    -------
    None
    &#34;&#34;&#34;
    
    if filename is not None:
        self.spike.set_filename()</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.set_spike_name"><code class="name flex">
<span>def <span class="ident">set_spike_name</span></span>(<span>self, name=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Sets the name of the spike dataset</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the spike dataset</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_spike_name(self, name=None):
    &#34;&#34;&#34;
    Sets the name of the spike dataset
    
    Parameters
    ----------
    name : str
        Name of the spike dataset
    
    Returns
    -------
    None

    &#34;&#34;&#34;
    
    if name is not None:
        self.spike.set_name(name)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.set_system"><code class="name flex">
<span>def <span class="ident">set_system</span></span>(<span>self, system=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Sets the data format or recording system.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>system</code></strong> :&ensp;<code>str</code></dt>
<dd>Data format or recording system</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_system(self, system=None):
    &#34;&#34;&#34;
    Sets the data format or recording system.
    
    Parameters
    ----------
    system : str
        Data format or recording system
    
    Returns
    -------
    None
    
    &#34;&#34;&#34;
    
    if system is not None:
        self._system = system

        if self.spike:
            self.spike.set_system(system)
        if self.lfp:
            self.lfp.set_system(system)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.smooth_direction"><code class="name flex">
<span>def <span class="ident">smooth_direction</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Smoothes the angular head direction data using a moving circular average</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="see-also">See also</h2>
<p><code>nc_circular.CircStat().circ_smooth()</code></p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def smooth_direction(self):
    &#34;&#34;&#34;
    Smoothes the angular head direction data using a moving circular average
    
    Parameters
    ----------
    None
    
    Returns
    -------
    None
    
    See also
    --------
    nc_circular.CircStat().circ_smooth()
    
    &#34;&#34;&#34;
    
    cs = CircStat()
    cs.set_theta(self.get_direction())
    self._set_direction(cs.circ_smooth(filttype=&#39;b&#39;, filtsize=5))</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.smooth_speed"><code class="name flex">
<span>def <span class="ident">smooth_speed</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Smoothes the speed data using a moving-average box filter</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def smooth_speed(self):
    &#34;&#34;&#34;
    Smoothes the speed data using a moving-average box filter
    
    Parameters
    ----------
    None
    
    Returns
    -------
    None        
    
    &#34;&#34;&#34;
    
    self._set_speed(smooth_1d(self.get_speed(), &#39;b&#39;, 5))</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.speed"><code class="name flex">
<span>def <span class="ident">speed</span></span>(<span>self, ftimes, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates the firing rate of the unit at different binned speeds.</p>
<p>The spike rate vs speed is fitted with a linear equation and goodness of fit
is measured</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ftimes</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Timestamps of the spiking activity of a unit</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def speed(self, ftimes, **kwargs):
    &#34;&#34;&#34;
    Calculates the firing rate of the unit at different binned speeds.
    
    The spike rate vs speed is fitted with a linear equation and goodness of fit
    is measured
    
    Parameters
    ----------
    ftimes : ndarray
        Timestamps of the spiking activity of a unit
    **kwargs
        Keyword arguments

    Returns
    -------
    dict
        Graphical data of the analysis
    &#34;&#34;&#34;

    _results = oDict()
    graph_data = {}
    update = kwargs.get(&#39;update&#39;, True) # When update = True, it will use the
                                        #results for statistics, if False,
                                        #i.e. in Multiple Regression, it will ignore updating
    binsize = kwargs.get(&#39;binsize&#39;, 1)
    min_speed, max_speed = kwargs.get(&#39;range&#39;, [0, 40])

    speed = self.get_speed()
    max_speed = min(max_speed, np.ceil(speed.max()/binsize)*binsize)
    min_speed = max(min_speed, np.floor(speed.min()/binsize)*binsize)
    bins = np.arange(min_speed, max_speed, binsize)

    vid_count = histogram(ftimes, self.get_time())[0]
    visit_time, speedInd = histogram(speed, bins)[0:2]
    visit_time = visit_time/self.get_sampling_rate()

    rate = np.array([sum(vid_count[speedInd == i]) for i in range(len(bins))])/ visit_time
    rate[np.isnan(rate)] = 0

    _results[&#39;Speed Skaggs&#39;] = self.skaggs_info(rate, visit_time)

    rate = rate[visit_time &gt; 1]
    bins = bins[visit_time &gt; 1]

    fit_result = linfit(bins, rate)

    _results[&#39;Speed Pears R&#39;] = fit_result[&#39;Pearson R&#39;]
    _results[&#39;Speed Pears P&#39;] = fit_result[&#39;Pearson P&#39;]
    graph_data[&#39;bins&#39;] = bins
    graph_data[&#39;rate&#39;] = rate
    graph_data[&#39;fitRate&#39;] = fit_result[&#39;yfit&#39;]

    if update:
        self.update_result(_results)
    return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_spatial.NSpatial.subsample"><code class="name flex">
<span>def <span class="ident">subsample</span></span>(<span>self, sample_range=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract a time range from the positions.</p>
<p>NOTE for now, the duration will be longer than sample time.
Duration is actually from 0 to max recording length.
This is to easier match ndata which assumes recordings start at 0.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sample_range</code></strong> :&ensp;<code>tuple</code></dt>
<dd>the time in seconds to extract from the positions.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>NSpike</code></dt>
<dd>subsampled version of initial spatial object</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def subsample(self, sample_range=None):
    &#34;&#34;&#34;
    Extract a time range from the positions.

    NOTE for now, the duration will be longer than sample time.
    Duration is actually from 0 to max recording length.
    This is to easier match ndata which assumes recordings start at 0.
    
    Parameters
    ----------
    sample_range : tuple
        the time in seconds to extract from the positions.
    
    Returns
    -------
    NSpike
        subsampled version of initial spatial object
    &#34;&#34;&#34;
    if sample_range is None:
        return self
    new_spatial = deepcopy(self)
    lower, upper = sample_range
    times = self._time
    sample_spatial_idx = (
        (times &lt;= upper) &amp; (times &gt;= lower)).nonzero()
    new_spatial._set_time(self._time[sample_spatial_idx])
    new_spatial._set_pos_x(self._pos_x[sample_spatial_idx])
    new_spatial._set_pos_y(self._pos_y[sample_spatial_idx])
    new_spatial._set_direction(self._direction[sample_spatial_idx])
    new_spatial._set_speed(self._speed[sample_spatial_idx])
    new_spatial.set_ang_vel(self._ang_vel[sample_spatial_idx])
    # NOTE can use to set proper duration
    #new_spatial._set_duration(upper-lower)
    return new_spatial</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="neurochat.nc_base.NAbstract" href="nc_base.html#neurochat.nc_base.NAbstract">NAbstract</a></b></code>:
<ul class="hlist">
<li><code><a title="neurochat.nc_base.NAbstract.get_comments" href="nc_base.html#neurochat.nc_base.NAbstract.get_comments">get_comments</a></code></li>
<li><code><a title="neurochat.nc_base.NAbstract.get_data_source" href="nc_base.html#neurochat.nc_base.NAbstract.get_data_source">get_data_source</a></code></li>
<li><code><a title="neurochat.nc_base.NAbstract.get_date" href="nc_base.html#neurochat.nc_base.NAbstract.get_date">get_date</a></code></li>
<li><code><a title="neurochat.nc_base.NAbstract.get_experimenter" href="nc_base.html#neurochat.nc_base.NAbstract.get_experimenter">get_experimenter</a></code></li>
<li><code><a title="neurochat.nc_base.NAbstract.get_file_version" href="nc_base.html#neurochat.nc_base.NAbstract.get_file_version">get_file_version</a></code></li>
<li><code><a title="neurochat.nc_base.NAbstract.get_filename" href="nc_base.html#neurochat.nc_base.NAbstract.get_filename">get_filename</a></code></li>
<li><code><a title="neurochat.nc_base.NAbstract.get_name" href="nc_base.html#neurochat.nc_base.NAbstract.get_name">get_name</a></code></li>
<li><code><a title="neurochat.nc_base.NAbstract.get_record_info" href="nc_base.html#neurochat.nc_base.NAbstract.get_record_info">get_record_info</a></code></li>
<li><code><a title="neurochat.nc_base.NAbstract.get_results" href="nc_base.html#neurochat.nc_base.NAbstract.get_results">get_results</a></code></li>
<li><code><a title="neurochat.nc_base.NAbstract.get_source_format" href="nc_base.html#neurochat.nc_base.NAbstract.get_source_format">get_source_format</a></code></li>
<li><code><a title="neurochat.nc_base.NAbstract.get_system" href="nc_base.html#neurochat.nc_base.NAbstract.get_system">get_system</a></code></li>
<li><code><a title="neurochat.nc_base.NAbstract.reset_results" href="nc_base.html#neurochat.nc_base.NAbstract.reset_results">reset_results</a></code></li>
<li><code><a title="neurochat.nc_base.NAbstract.set_description" href="nc_base.html#neurochat.nc_base.NAbstract.set_description">set_description</a></code></li>
<li><code><a title="neurochat.nc_base.NAbstract.set_filename" href="nc_base.html#neurochat.nc_base.NAbstract.set_filename">set_filename</a></code></li>
<li><code><a title="neurochat.nc_base.NAbstract.set_name" href="nc_base.html#neurochat.nc_base.NAbstract.set_name">set_name</a></code></li>
<li><code><a title="neurochat.nc_base.NAbstract.set_record_info" href="nc_base.html#neurochat.nc_base.NAbstract.set_record_info">set_record_info</a></code></li>
<li><code><a title="neurochat.nc_base.NAbstract.update_result" href="nc_base.html#neurochat.nc_base.NAbstract.update_result">update_result</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="neurochat" href="index.html">neurochat</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="neurochat.nc_spatial.NSpatial" href="#neurochat.nc_spatial.NSpatial">NSpatial</a></code></h4>
<ul class="">
<li><code><a title="neurochat.nc_spatial.NSpatial.angular_velocity" href="#neurochat.nc_spatial.NSpatial.angular_velocity">angular_velocity</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.border" href="#neurochat.nc_spatial.NSpatial.border">border</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.calc_ang_vel" href="#neurochat.nc_spatial.NSpatial.calc_ang_vel">calc_ang_vel</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.calc_border" href="#neurochat.nc_spatial.NSpatial.calc_border">calc_border</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.get_ang_vel" href="#neurochat.nc_spatial.NSpatial.get_ang_vel">get_ang_vel</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.get_border" href="#neurochat.nc_spatial.NSpatial.get_border">get_border</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.get_direction" href="#neurochat.nc_spatial.NSpatial.get_direction">get_direction</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.get_duration" href="#neurochat.nc_spatial.NSpatial.get_duration">get_duration</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.get_event_loc" href="#neurochat.nc_spatial.NSpatial.get_event_loc">get_event_loc</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.get_non_moving_times" href="#neurochat.nc_spatial.NSpatial.get_non_moving_times">get_non_moving_times</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.get_pixel_size" href="#neurochat.nc_spatial.NSpatial.get_pixel_size">get_pixel_size</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.get_pos_x" href="#neurochat.nc_spatial.NSpatial.get_pos_x">get_pos_x</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.get_pos_y" href="#neurochat.nc_spatial.NSpatial.get_pos_y">get_pos_y</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.get_sampling_rate" href="#neurochat.nc_spatial.NSpatial.get_sampling_rate">get_sampling_rate</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.get_speed" href="#neurochat.nc_spatial.NSpatial.get_speed">get_speed</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.get_time" href="#neurochat.nc_spatial.NSpatial.get_time">get_time</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.get_timestamp" href="#neurochat.nc_spatial.NSpatial.get_timestamp">get_timestamp</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.get_total_samples" href="#neurochat.nc_spatial.NSpatial.get_total_samples">get_total_samples</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.get_type" href="#neurochat.nc_spatial.NSpatial.get_type">get_type</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.gradient" href="#neurochat.nc_spatial.NSpatial.gradient">gradient</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.grid" href="#neurochat.nc_spatial.NSpatial.grid">grid</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.hd_rate" href="#neurochat.nc_spatial.NSpatial.hd_rate">hd_rate</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.hd_rate_ccw" href="#neurochat.nc_spatial.NSpatial.hd_rate_ccw">hd_rate_ccw</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.hd_shift" href="#neurochat.nc_spatial.NSpatial.hd_shift">hd_shift</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.hd_shuffle" href="#neurochat.nc_spatial.NSpatial.hd_shuffle">hd_shuffle</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.hd_time_lapse" href="#neurochat.nc_spatial.NSpatial.hd_time_lapse">hd_time_lapse</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.interdependence" href="#neurochat.nc_spatial.NSpatial.interdependence">interdependence</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.load" href="#neurochat.nc_spatial.NSpatial.load">load</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.load_lfp" href="#neurochat.nc_spatial.NSpatial.load_lfp">load_lfp</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.load_spatial_Axona" href="#neurochat.nc_spatial.NSpatial.load_spatial_Axona">load_spatial_Axona</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.load_spatial_NWB" href="#neurochat.nc_spatial.NSpatial.load_spatial_NWB">load_spatial_NWB</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.load_spatial_Neuralynx" href="#neurochat.nc_spatial.NSpatial.load_spatial_Neuralynx">load_spatial_Neuralynx</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.load_spike" href="#neurochat.nc_spatial.NSpatial.load_spike">load_spike</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.loc_auto_corr" href="#neurochat.nc_spatial.NSpatial.loc_auto_corr">loc_auto_corr</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.loc_rot_corr" href="#neurochat.nc_spatial.NSpatial.loc_rot_corr">loc_rot_corr</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.loc_shift" href="#neurochat.nc_spatial.NSpatial.loc_shift">loc_shift</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.loc_shuffle" href="#neurochat.nc_spatial.NSpatial.loc_shuffle">loc_shuffle</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.loc_time_lapse" href="#neurochat.nc_spatial.NSpatial.loc_time_lapse">loc_time_lapse</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.multiple_regression" href="#neurochat.nc_spatial.NSpatial.multiple_regression">multiple_regression</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.non_moving_periods" href="#neurochat.nc_spatial.NSpatial.non_moving_periods">non_moving_periods</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.place" href="#neurochat.nc_spatial.NSpatial.place">place</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.place_field" href="#neurochat.nc_spatial.NSpatial.place_field">place_field</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.place_field_centroid" href="#neurochat.nc_spatial.NSpatial.place_field_centroid">place_field_centroid</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.place_field_centroid_zscore" href="#neurochat.nc_spatial.NSpatial.place_field_centroid_zscore">place_field_centroid_zscore</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.save_to_hdf5" href="#neurochat.nc_spatial.NSpatial.save_to_hdf5">save_to_hdf5</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.set_ang_vel" href="#neurochat.nc_spatial.NSpatial.set_ang_vel">set_ang_vel</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.set_border" href="#neurochat.nc_spatial.NSpatial.set_border">set_border</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.set_event" href="#neurochat.nc_spatial.NSpatial.set_event">set_event</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.set_event_filename" href="#neurochat.nc_spatial.NSpatial.set_event_filename">set_event_filename</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.set_event_name" href="#neurochat.nc_spatial.NSpatial.set_event_name">set_event_name</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.set_lfp" href="#neurochat.nc_spatial.NSpatial.set_lfp">set_lfp</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.set_lfp_filename" href="#neurochat.nc_spatial.NSpatial.set_lfp_filename">set_lfp_filename</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.set_lfp_name" href="#neurochat.nc_spatial.NSpatial.set_lfp_name">set_lfp_name</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.set_pixel_size" href="#neurochat.nc_spatial.NSpatial.set_pixel_size">set_pixel_size</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.set_spike" href="#neurochat.nc_spatial.NSpatial.set_spike">set_spike</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.set_spike_filename" href="#neurochat.nc_spatial.NSpatial.set_spike_filename">set_spike_filename</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.set_spike_name" href="#neurochat.nc_spatial.NSpatial.set_spike_name">set_spike_name</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.set_system" href="#neurochat.nc_spatial.NSpatial.set_system">set_system</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.skaggs_info" href="#neurochat.nc_spatial.NSpatial.skaggs_info">skaggs_info</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.smooth_direction" href="#neurochat.nc_spatial.NSpatial.smooth_direction">smooth_direction</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.smooth_speed" href="#neurochat.nc_spatial.NSpatial.smooth_speed">smooth_speed</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.spatial_sparsity" href="#neurochat.nc_spatial.NSpatial.spatial_sparsity">spatial_sparsity</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.speed" href="#neurochat.nc_spatial.NSpatial.speed">speed</a></code></li>
<li><code><a title="neurochat.nc_spatial.NSpatial.subsample" href="#neurochat.nc_spatial.NSpatial.subsample">subsample</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.6.3</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>