<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.6.3" />
<title>neurochat.nc_containeranalysis API documentation</title>
<meta name="description" content="This module contains analysis functions for NDataContainer objects â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>neurochat.nc_containeranalysis</code></h1>
</header>
<section id="section-intro">
<p>This module contains analysis functions for NDataContainer objects.</p>
<p>@author: Sean Martin; martins7 at tcd dot ie</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">&#34;&#34;&#34;
This module contains analysis functions for NDataContainer objects.

@author: Sean Martin; martins7 at tcd dot ie
&#34;&#34;&#34;

import logging
from itertools import compress
from math import floor, ceil
import os
import gc
import re

from neurochat.nc_datacontainer import NDataContainer
from neurochat.nc_data import NData
from neurochat.nc_clust import NClust
from neurochat.nc_utils import smooth_1d, find_true_ranges
from neurochat.nc_utils import find_peaks
from neurochat.nc_utils import window_rms
from neurochat.nc_utils import distinct_window_rms
from neurochat.nc_utils import make_dir_if_not_exists
from neurochat.nc_utils import log_exception
from neurochat.nc_plot import print_place_cells

import numpy as np
from scipy.optimize import linear_sum_assignment
from matplotlib.pyplot import savefig, close


def place_cell_summary(
        collection, dpi=150, out_dirname=&#34;nc_plots&#34;,
        filter_place_cells=True, filter_low_freq=True, opt_end=&#34;&#34;,
        base_dir=None):
    &#34;&#34;&#34;
    Quick Png spatial information summary of each cell in collection.

    Parameters
    ----------
    collection : NDataCollection
        The collection to plot summaries of.
    dpi : int, default 150
        Dpi of the output figures.
    out_dirname : str, default &#34;nc_plots
        The relative name of the dir to save pngs to
    filter_place_cells: bool, default True
        Whether to filter out non spatial cells from the plots.
        Considered non spatial if shuffled Skaggs, Coherency or Sparsity
        is similar to actual values.
    filter_low_freq: bool, default True
        Filter out cells with spike freq less than 0.1Hz
    opt_end : str, default &#34;&#34;
        A string to append to the file output just before the extension
    base_dir : str, default None
        An optional directory to save the files to 

    Returns
    -------
    None
    &#34;&#34;&#34;

    placedata = []
    graphdata = []
    wavedata = []
    headdata = []
    thetadata = []
    isidata = []
    good_units = []

    for i, data in enumerate(collection):
        try:
            data_idx, unit_idx = collection._index_to_data_pos(i)
            filename = collection.get_file_dict()[&#34;Spike&#34;][data_idx][0]
            unit_number = collection.get_units(data_idx)[unit_idx]
            print(&#34;Working on {} unit {}&#34;.format(
                filename, unit_number))

            count = data.spike.get_unit_spikes_count()
            duration = data.spike.get_duration()
            good = True

            if filter_low_freq and (count / duration) &lt; 0.1:
                print(&#34;Reject spike frequency {}&#34;.format(count / duration))
                good = False

            elif filter_place_cells:
                skaggs = data.loc_shuffle(nshuff=300)
                bad_skaggs = skaggs[&#39;refSkaggs&#39;] &lt;= skaggs[&#39;skaggs95&#39;]
                bad_sparsity = skaggs[&#39;refSparsity&#39;] &gt;= skaggs[&#39;sparsity05&#39;]
                bad_cohere = skaggs[&#39;refCoherence&#39;] &lt;= skaggs[&#39;coherence95&#39;]

                if bad_skaggs or bad_sparsity or bad_cohere:
                    good = False
                    first_str_part = &#34;Reject &#34;

                else:
                    good_units.append(unit_idx)
                    first_str_part = &#34;Accept &#34;

                print((
                    first_str_part +
                    &#34;Skaggs {:2f} | {:2f}, &#34; +
                    &#34;Sparsity {:2f} | {:2f}, &#34; +
                    &#34;Coherence {:2f} | {:2f}&#34;).format(
                    skaggs[&#39;refSkaggs&#39;],
                    skaggs[&#39;skaggs95&#39;],
                    skaggs[&#39;refSparsity&#39;],
                    skaggs[&#39;sparsity05&#39;],
                    skaggs[&#39;refCoherence&#39;],
                    skaggs[&#39;coherence95&#39;]))
            if good:
                placedata.append(data.place())
                graphdata.append(data.isi_corr(bins=1, bound=[-10, 10]))
                wavedata.append(data.wave_property())
                headdata.append(data.hd_rate())
                thetadata.append(data.theta_index(bins=2, bound=[-350, 350]))
                isidata.append(data.isi(bins=int(350 / 2), bound=[0, 350]))

            # Save the accumulated information
            if unit_idx == len(collection.get_units(data_idx)) - 1:
                spike_name = os.path.basename(filename)
                parts = spike_name.split(&#34;.&#34;)
                f_dir = os.path.dirname(filename)
                png_basename = parts[0] + &#34;_&#34; + parts[1] + opt_end + &#34;.png&#34;
                if base_dir is not None:
                    main_dir = base_dir
                    png_base = f_dir[len(base_dir + os.sep):]
                    png_base = (&#34;--&#34;).join(png_base.split(os.sep))
                    # png_base = re.sub(os.sep, &#34;_&#34;, png_base)
                    png_basename = png_base + &#34;--&#34; + png_basename
                else:
                    main_dir = f_dir

                if filter_place_cells:
                    named_units = [
                        collection.get_units(data_idx)[j]
                        for j in good_units]
                else:
                    named_units = collection.get_units(data_idx)

                if len(named_units) &gt; 0:
                    if filter_place_cells:
                        print((
                            &#34;Plotting summary for {} &#34; +
                            &#34;spatial units {}&#34;).format(
                            spike_name, named_units))
                    else:
                        print((
                            &#34;Plotting summary for {} &#34; +
                            &#34;units {}&#34;).format(
                            spike_name, named_units))

                    fig = print_place_cells(
                        len(named_units),
                        placedata=placedata, graphdata=graphdata,
                        wavedata=wavedata, headdata=headdata,
                        thetadata=thetadata, isidata=isidata,
                        size_multiplier=4, point_size=dpi / 7.0,
                        units=named_units)
                    out_name = os.path.join(
                        main_dir, out_dirname, png_basename)
                    print(&#34;Saving place cell figure to {}&#34;.format(
                        out_name))
                    make_dir_if_not_exists(out_name)
                    fig.savefig(out_name, dpi=dpi)
                    close(&#34;all&#34;)
                    gc.collect()

                    placedata = []
                    graphdata = []
                    wavedata = []
                    headdata = []
                    thetadata = []
                    isidata = []
                    good_units = []

        except Exception as e:
            log_exception(
                e, &#34;Occured during place cell summary on data&#34; +
                   &#34; {} unit {} name {} in {}&#34;.format(
                       data_idx, unit_number, spike_name, main_dir))
    return


def evaluate_clusters(collection, idx1, idx2, set_units=False):
    &#34;&#34;&#34;
    Find which units are closest in terms of clustering.

    Uses the Hungarian (Munkres) cost optimisation based on Hellinger distance
    between the clusters.

    Parameters
    ----------
    collection : NDataCollection
        The collection to find similar cells in
    idx1 : int
        The first data point in the collection to consider
    idx2 : int
        The second data point in the collection to consider

    Returns
    -------
    dict
        For each unit in data[idx1] (key), a tuple consisting of the
        best matching unit from data[idx2] and the distance for this (val)

    &#34;&#34;&#34;
    nclust1 = NClust()
    nclust2 = NClust()

    sub_col1 = collection.subsample(idx1)
    info1 = sub_col1.get_file_dict()[&#34;Spike&#34;][0]
    nclust1.load(info1[0], info1[2])

    sub_col2 = collection.subsample(idx2)
    info2 = sub_col2.get_file_dict()[&#34;Spike&#34;][0]
    nclust2.load(info2[0], info2[2])

    distance_shape = (
        len(sub_col1.get_units()[0]), len(sub_col2.get_units()[0]))
    distances = np.zeros(shape=distance_shape)
    # Build a matrix of distances for each unit
    for idx1, unit1 in enumerate(sub_col1.get_units()[0]):
        for idx2, unit2 in enumerate(sub_col2.get_units()[0]):
            _, dh = nclust1.cluster_similarity(nclust2, unit1, unit2)
            distances[idx1, idx2] = dh
            # print(
            #     &#34;{} {}: Bhattacharyya {} Hellinger {}&#34;.format(
            #         unit1, unit2, bc, dh))

    # Solve the linear sum assignment problem based on the Hungarian method
    solution = linear_sum_assignment(distances)
    best_matches = {}
    for i, j in zip(solution[0], solution[1]):
        best_matches[sub_col1.get_units()[0][i]] = (
            sub_col2.get_units()[0][j], distances[i, j])

    print(&#34;Best assignment is {}&#34;.format(best_matches))

    if set_units:
        run_units = [key for key in best_matches.keys()]
        best_units = [val[0] for _, val in best_matches.items()]
        collection.set_units([run_units, best_units])
    return best_matches


def count_units_in_bins(
        collection, bin_length, in_range=None, multi_ranges=None):
    &#34;&#34;&#34;
    Count the amount of units that fire in certain bins.

    Parameters
    ----------
    collection : NDataCollection
        The collection of units to count over
    bin_length : float
        The length of the bins in seconds
    in_range : tuple
        The range of time to count units over

    Returns
    -------
    list of tuples:
        (unit counts in bins, bin_centres) for each data object in the collection

    &#34;&#34;&#34;
    result = []
    calc_range = (in_range is None) and (multi_ranges is None)

    for idx in range(collection.get_num_data()):
        if collection.get_num_data() &gt; 1:
            sub_collection = collection.subsample(idx)
        else:
            sub_collection = collection

        if calc_range:
            in_range = (0, sub_collection.get_data(0).get_duration())
        elif multi_ranges:
            in_range = multi_ranges[idx]
        num_bins = ceil(float(in_range[1] - in_range[0]) / bin_length)

        arr = np.empty(shape=(len(collection), num_bins))
        for data_idx, data in enumerate(sub_collection):
            spikes = data.get_unit_stamps_in_ranges([in_range])
            # Check if the unit spikes in the bin
            hist_val, bins = np.histogram(
                spikes, bins=num_bins, range=in_range)
            hist_val = np.clip(hist_val, 0, 1)
            arr[data_idx] = hist_val
        bin_centres = [(bins[j + 1] + bins[j]) / 2 for j in range(num_bins)]
        mua_tuple = (np.sum(arr, axis=0), bin_centres)
        result.append(mua_tuple)

    return result


def smooth_speeds(collection, allow_multiple=False):
    &#34;&#34;&#34;
    Smooth all the speed data in the collection.

    Parameters
    ----------
    collection : NDataContainer
        Container to get the information from
    allows_multiple : bool
        Allow smoothing multiple times, default False

    Returns
    -------
    None

    &#34;&#34;&#34;
    if collection._smoothed_speed and not allow_multiple:
        logging.warning(
            &#34;NDataContainer has already been speed smoothed, not smoothing&#34;)

    for i in range(collection.get_num_data()):
        data = collection.get_data(i)
        data.smooth_speed()
        collection._smoothed_speed = True


def spike_positions(collection, should_sort=True, mode=&#34;vertical&#34;):
    &#34;&#34;&#34;
    Get the spike positions for a number of units.

    Parameters
    ----------
    collection : NDataContainer or NData list or NData object


    Returns
    -------
    positions : list of positions of the rat when the cell spiked

    &#34;&#34;&#34;
    if isinstance(collection, NDataContainer) and should_sort:
        collection.sort_units_spatially(mode=mode)

    if isinstance(collection, NData):
        positions = collection.get_event_loc(collection.get_unit_stamp())[1]
        if mode == &#34;vertical&#34;:
            positions = positions[1]
        elif mode == &#34;horizontal&#34;:
            positions = positions[0]
        else:
            logging.error(&#34;nca: mode only supports vertical or horizontal&#34;)
    else:
        positions = []
        for data in collection:
            position = data.get_event_loc(data.get_unit_stamp())[1]
            if mode == &#34;vertical&#34;:
                position = position[1]
            elif mode == &#34;horizontal&#34;:
                position = position[0]
            else:
                logging.error(&#34;nca: mode only supports vertical or horizontal&#34;)
            positions.append(position)

    return positions


def spike_times(collection, filter_speed=False, **kwargs):
    &#34;&#34;&#34;
    Return a list of all spike times in the collection.

    Parameters
    ----------
    collection : NDataContainer or NData
        Either the container or data object to get spike times from
    filter_speed : bool
        If true, don&#39;t consider spike times when the rat is non moving
    kwargs
        should_smooth : bool
            Smooth the speed data if true
        ranges : list
            List of tuples indicating time ranges to get spikes in

    Returns
    -------
    list
        The list of spike times if collection is NData
        or a 2d list containing a list of times for each collection item

    &#34;&#34;&#34;
    should_smooth = kwargs.get(&#34;should_smooth&#34;, False)
    ranges = kwargs.get(&#34;ranges&#34;, None)

    if isinstance(collection, NData):
        if ranges is not None:
            time_data = collection.get_unit_stamps_in_ranges(ranges)
        elif filter_speed:
            ranges = collection.non_moving_periods(**kwargs)
            time_data = collection.get_unit_stamps_in_ranges(ranges)
        else:
            times = collection.get_unit_stamp()

    else:
        if should_smooth:
            smooth_speeds(collection)
            kwargs[&#34;should_smooth&#34;] = False

        times = []
        for data in collection:
            if ranges is not None:
                time_data = data.get_unit_stamps_in_ranges(ranges)
            elif filter_speed:
                ranges = data.non_moving_periods(**kwargs)
                time_data = data.get_unit_stamps_in_ranges(ranges)
            else:
                time_data = data.get_unit_stamp()
            times.append(time_data)
    return times

    # def multi_unit_activity(collection, time_range=None, strip=False, **kwargs):
    # &#34;&#34;&#34;
    # For each recording in the collection, detect periods of MUA.

    # WORK IN PROGRESS, NEEDS TO BE MODIFIED BEFORE REAL USE
    # Do not pass ranges and filter speed, only pass one.

    # Parameters
    # ----------
    # collection : NDataContainer
    #     The collection of units to detect Muti unit activity.
    # time_range : tuple, default None
    #     Optional time range to consider for the MUA.
    # strip: bool, default False
    #     If working with one data object in the collection,
    #     remove the surrounding array in output dict.

    # kwargs
    # ------
    # mua_bin_length : float
    #     The length of bins for mua histogram calculation in seconds
    # filter_length : float
    #     The std_dev of the gaussian used for filtering in seconds
    # mua_mode : str
    #     &#34;rms_peaks&#34; - calculate rms window and find peaks in this
    #     or &#34;raw&#34; - calculate mua histogram, extract bins with all cells active
    #     or &#34;high_activity&#34; - calculate rms window and
    #                          look for sustained high activity in this
    # mua_length : float
    #     The length of a mua event in seconds
    # filter_mua : bool
    #     Should the mua histogram be filtered by a guassian
    # mua_percentile : float
    #     The percentile threshold for a mua peak

    # Returns
    # -------
    # dict
    #     hists, mua

    # &#34;&#34;&#34;
    # mua_bin_length = kwargs.get(&#34;mua_bin_length&#34;, 0.001)
    # filter_length = kwargs.get(&#34;filter_length&#34;, 0.01)
    # mode = kwargs.get(&#34;mua_mode&#34;, &#34;rms_peaks&#34;)
    # mua_length = kwargs.get(&#34;mua_length&#34;, 0.6)
    # filter_mua = kwargs.get(&#34;filter_mua&#34;, True)
    # mua_percentile = kwargs.get(&#34;mua_percentile&#34;, 99)

    # result = {&#34;mua hists&#34;: [], &#34;mua&#34;: []}

    # # Get mua histogram for each data point
    # for data_idx in range(collection.get_num_data()):
    #     if collection.get_num_data() &gt; 1:
    #         sub_collection = collection.subsample(data_idx)
    #     else:
    #         sub_collection = collection
    #     sample_rate = sub_collection.get_data(0).lfp.get_sampling_rate()
    #     sigma = filter_length * sample_rate
    #     unit_hist = count_units_in_bins(
    #         collection, mua_bin_length, time_range)[0]
    #     if filter_mua:
    #         unit_hist = (
    #             smooth_1d(unit_hist[0], filttype=&#39;g&#39;, filtsize=sigma),
    #             unit_hist[1])
    #     result[&#34;mua hists&#34;].append(unit_hist)

    # for i, hist in enumerate(result[&#39;mua hists&#39;]):
    #     # Look for long periods of high activity
    #     if mode == &#34;high_activity&#34;:
    #         p95 = np.percentile(hist[0], 95)
    #         result[&#39;mua&#39;].append(find_true_ranges(
    #             hist[1], hist[0] &gt; p95, min_range=mua_length))

    #     # Look for peaks in the activity
    #     if mode == &#34;rms_peaks&#34;:
    #         p_val = np.percentile(hist[0], mua_percentile)
    #         _, peaks = find_peaks(hist[0], thresh=p_val)
    #         corresponding_ranges = [
    #             (hist[1][peak] - mua_length * 0.5,
    #              hist[1][peak] + mua_length * 0.5)
    #             for peak in peaks]
    #         result[&#39;mua&#39;].append(corresponding_ranges)

    #     # Get areas where the number of units active is maximal
    #     if mode == &#34;raw&#34;:
    #         if collection.get_num_data() &gt; 1:
    #             num_cells = len(collection.subsample(i))
    #         else:
    #             num_cells = len(collection)
    #         mua_indices = np.argwhere(hist[0] == num_cells)
    #         corresponding_ranges = [
    #             (hist[1][index] - mua_length * 0.5,
    #              hist[1][index] + mua_length * 0.5)
    #             for index in mua_indices.flatten()]
    #         result[&#34;mua&#34;].append(corresponding_ranges)

    # if strip and collection.get_num_data() == 1:
    #     result[&#34;mua hists&#34;] = result[&#34;mua hists&#34;][0]
    #     result[&#34;mua&#34;] = result[&#34;mua&#34;][0]

    # return result

    # def replay(collection, run_idx, sleep_idx, **kwargs):
    # &#34;&#34;&#34;
    # Run and sleep session comparison.

    # Set the units of interest in the collection before running.

    # Parameters
    # ----------
    # collection : NDataContainer
    #     The collection of run and sleep data
    # run_idx : int
    #     The index in the collection for the run data
    # sleep_idx : int
    #     The index in the collection for the sleep data

    # kwargs
    # ------
    # sorting_mode : str
    #     &#34;vertical&#34; or &#34;horizontal&#34; order for spatial sorting
    # swr_window : float
    #     Lenth of SWR event around peak in seconds
    # match_clusters : bool
    #     If true, set the units being used in sleep to those
    #     most similar from run

    # kwargs are also passed into
    # nc_lfp.sharp_wave_ripples and
    # multi_unit_activity and
    # nc_spatial.non_moving_periods

    # Returns
    # -------
    # dict
    #     Graphical and numerical analysis results

    # See also
    # --------
    # nc_lfp.sharp_wave_ripples
    # nc_spatial.non_moving_periods
    # multi_unit_activity

    # &#34;&#34;&#34;
    # results = {}

    # # Parse the kwargs
    # sorting_mode = kwargs.get(&#34;sorting_mode&#34;, &#34;vertical&#34;)
    # swr_window = kwargs.get(&#34;swr_window&#34;, 0.2)
    # match_clusters = kwargs.get(&#34;match_clusters&#34;, True)

    # # Sort the run data spatially
    # truth_arr = [False for i in range(collection.get_num_data())]
    # truth_arr[run_idx] = True
    # collection.sort_units_spatially(truth_arr, mode=sorting_mode)

    # # Match up cells between the recordings
    # if match_clusters:
    #     evaluate_clusters(collection, run_idx, sleep_idx, set_units=True)

    # # Find the longest period of continuous sleep
    # sleep = collection.get_data(sleep_idx)
    # sleep_subsample = collection.subsample(sleep_idx)
    # sample_rate = sleep.lfp.get_sampling_rate()
    # non_moving_periods = np.array(
    #     sleep.non_moving_periods(**kwargs)) * sample_rate

    # # Could take multiple periods instead of just the longest
    # sorted_periods = sorted(
    #     non_moving_periods, key=lambda x: x[1] - x[0], reverse=True)
    # longest_sleep_period = sorted_periods[0]
    # raw_spike_times = spike_times(
    #     sleep_subsample,
    #     ranges=[longest_sleep_period / sample_rate])

    # # Estimate SWR
    # result_swr = sleep.lfp.sharp_wave_ripples(
    #     in_range=longest_sleep_period / sample_rate, **kwargs)

    # # Estimate MUA bursts
    # result_mua = multi_unit_activity(
    #     sleep_subsample, longest_sleep_period / sample_rate,
    #     strip=True, **kwargs)

    # results.update(result_swr)
    # results.update(result_mua)
    # results[&#34;spike times&#34;] = raw_spike_times
    # results[&#34;num cells&#34;] = len(sleep_subsample)

    # # Get the overlapping ranges of SWR and MUA
    # def swr_interval(peak):
    #     return (peak - 0.5 * swr_window, peak + 0.5 * swr_window)

    # def overlapping_swr_mua(mua, swr_peak):
    #     swr_range = swr_interval(swr_peak)
    #     overlapping = (
    #         (swr_range[0] &lt; mua[0] &lt; swr_range[1]) or
    #         (swr_range[0] &lt; mua[1] &lt; swr_range[1])
    #     )
    #     return overlapping

    # overlap = [
    #     mua_range for mua_range in results[&#34;mua&#34;]
    #     if any(overlapping_swr_mua(mua_range, peak)
    #            for peak in results[&#34;swr times&#34;])
    # ]

    # results[&#34;overlap swr mua&#34;] = overlap

    # return results</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="neurochat.nc_containeranalysis.count_units_in_bins"><code class="name flex">
<span>def <span class="ident">count_units_in_bins</span></span>(<span>collection, bin_length, in_range=None, multi_ranges=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Count the amount of units that fire in certain bins.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>collection</code></strong> :&ensp;<code>NDataCollection</code></dt>
<dd>The collection of units to count over</dd>
<dt><strong><code>bin_length</code></strong> :&ensp;<code>float</code></dt>
<dd>The length of the bins in seconds</dd>
<dt><strong><code>in_range</code></strong> :&ensp;<code>tuple</code></dt>
<dd>The range of time to count units over</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code> of <code>tuples</code>:</dt>
<dd>(unit counts in bins, bin_centres) for each data object in the collection</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def count_units_in_bins(
        collection, bin_length, in_range=None, multi_ranges=None):
    &#34;&#34;&#34;
    Count the amount of units that fire in certain bins.

    Parameters
    ----------
    collection : NDataCollection
        The collection of units to count over
    bin_length : float
        The length of the bins in seconds
    in_range : tuple
        The range of time to count units over

    Returns
    -------
    list of tuples:
        (unit counts in bins, bin_centres) for each data object in the collection

    &#34;&#34;&#34;
    result = []
    calc_range = (in_range is None) and (multi_ranges is None)

    for idx in range(collection.get_num_data()):
        if collection.get_num_data() &gt; 1:
            sub_collection = collection.subsample(idx)
        else:
            sub_collection = collection

        if calc_range:
            in_range = (0, sub_collection.get_data(0).get_duration())
        elif multi_ranges:
            in_range = multi_ranges[idx]
        num_bins = ceil(float(in_range[1] - in_range[0]) / bin_length)

        arr = np.empty(shape=(len(collection), num_bins))
        for data_idx, data in enumerate(sub_collection):
            spikes = data.get_unit_stamps_in_ranges([in_range])
            # Check if the unit spikes in the bin
            hist_val, bins = np.histogram(
                spikes, bins=num_bins, range=in_range)
            hist_val = np.clip(hist_val, 0, 1)
            arr[data_idx] = hist_val
        bin_centres = [(bins[j + 1] + bins[j]) / 2 for j in range(num_bins)]
        mua_tuple = (np.sum(arr, axis=0), bin_centres)
        result.append(mua_tuple)

    return result</code></pre>
</details>
</dd>
<dt id="neurochat.nc_containeranalysis.evaluate_clusters"><code class="name flex">
<span>def <span class="ident">evaluate_clusters</span></span>(<span>collection, idx1, idx2, set_units=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Find which units are closest in terms of clustering.</p>
<p>Uses the Hungarian (Munkres) cost optimisation based on Hellinger distance
between the clusters.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>collection</code></strong> :&ensp;<code>NDataCollection</code></dt>
<dd>The collection to find similar cells in</dd>
<dt><strong><code>idx1</code></strong> :&ensp;<code>int</code></dt>
<dd>The first data point in the collection to consider</dd>
<dt><strong><code>idx2</code></strong> :&ensp;<code>int</code></dt>
<dd>The second data point in the collection to consider</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>For each unit in data[idx1] (key), a tuple consisting of the
best matching unit from data[idx2] and the distance for this (val)</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def evaluate_clusters(collection, idx1, idx2, set_units=False):
    &#34;&#34;&#34;
    Find which units are closest in terms of clustering.

    Uses the Hungarian (Munkres) cost optimisation based on Hellinger distance
    between the clusters.

    Parameters
    ----------
    collection : NDataCollection
        The collection to find similar cells in
    idx1 : int
        The first data point in the collection to consider
    idx2 : int
        The second data point in the collection to consider

    Returns
    -------
    dict
        For each unit in data[idx1] (key), a tuple consisting of the
        best matching unit from data[idx2] and the distance for this (val)

    &#34;&#34;&#34;
    nclust1 = NClust()
    nclust2 = NClust()

    sub_col1 = collection.subsample(idx1)
    info1 = sub_col1.get_file_dict()[&#34;Spike&#34;][0]
    nclust1.load(info1[0], info1[2])

    sub_col2 = collection.subsample(idx2)
    info2 = sub_col2.get_file_dict()[&#34;Spike&#34;][0]
    nclust2.load(info2[0], info2[2])

    distance_shape = (
        len(sub_col1.get_units()[0]), len(sub_col2.get_units()[0]))
    distances = np.zeros(shape=distance_shape)
    # Build a matrix of distances for each unit
    for idx1, unit1 in enumerate(sub_col1.get_units()[0]):
        for idx2, unit2 in enumerate(sub_col2.get_units()[0]):
            _, dh = nclust1.cluster_similarity(nclust2, unit1, unit2)
            distances[idx1, idx2] = dh
            # print(
            #     &#34;{} {}: Bhattacharyya {} Hellinger {}&#34;.format(
            #         unit1, unit2, bc, dh))

    # Solve the linear sum assignment problem based on the Hungarian method
    solution = linear_sum_assignment(distances)
    best_matches = {}
    for i, j in zip(solution[0], solution[1]):
        best_matches[sub_col1.get_units()[0][i]] = (
            sub_col2.get_units()[0][j], distances[i, j])

    print(&#34;Best assignment is {}&#34;.format(best_matches))

    if set_units:
        run_units = [key for key in best_matches.keys()]
        best_units = [val[0] for _, val in best_matches.items()]
        collection.set_units([run_units, best_units])
    return best_matches</code></pre>
</details>
</dd>
<dt id="neurochat.nc_containeranalysis.place_cell_summary"><code class="name flex">
<span>def <span class="ident">place_cell_summary</span></span>(<span>collection, dpi=150, out_dirname='nc_plots', filter_place_cells=True, filter_low_freq=True, opt_end='', base_dir=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Quick Png spatial information summary of each cell in collection.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>collection</code></strong> :&ensp;<code>NDataCollection</code></dt>
<dd>The collection to plot summaries of.</dd>
<dt><strong><code>dpi</code></strong> :&ensp;<code>int</code>, default <code>150</code></dt>
<dd>Dpi of the output figures.</dd>
<dt><strong><code>out_dirname</code></strong> :&ensp;<code>str</code>, default <code>"nc_plots</code></dt>
<dd>The relative name of the dir to save pngs to</dd>
<dt><strong><code>filter_place_cells</code></strong> :&ensp;<code>bool</code>, default <code>True</code></dt>
<dd>Whether to filter out non spatial cells from the plots.
Considered non spatial if shuffled Skaggs, Coherency or Sparsity
is similar to actual values.</dd>
<dt><strong><code>filter_low_freq</code></strong> :&ensp;<code>bool</code>, default <code>True</code></dt>
<dd>Filter out cells with spike freq less than 0.1Hz</dd>
<dt><strong><code>opt_end</code></strong> :&ensp;<code>str</code>, default <code>""</code></dt>
<dd>A string to append to the file output just before the extension</dd>
<dt><strong><code>base_dir</code></strong> :&ensp;<code>str</code>, default <code>None</code></dt>
<dd>An optional directory to save the files to</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def place_cell_summary(
        collection, dpi=150, out_dirname=&#34;nc_plots&#34;,
        filter_place_cells=True, filter_low_freq=True, opt_end=&#34;&#34;,
        base_dir=None):
    &#34;&#34;&#34;
    Quick Png spatial information summary of each cell in collection.

    Parameters
    ----------
    collection : NDataCollection
        The collection to plot summaries of.
    dpi : int, default 150
        Dpi of the output figures.
    out_dirname : str, default &#34;nc_plots
        The relative name of the dir to save pngs to
    filter_place_cells: bool, default True
        Whether to filter out non spatial cells from the plots.
        Considered non spatial if shuffled Skaggs, Coherency or Sparsity
        is similar to actual values.
    filter_low_freq: bool, default True
        Filter out cells with spike freq less than 0.1Hz
    opt_end : str, default &#34;&#34;
        A string to append to the file output just before the extension
    base_dir : str, default None
        An optional directory to save the files to 

    Returns
    -------
    None
    &#34;&#34;&#34;

    placedata = []
    graphdata = []
    wavedata = []
    headdata = []
    thetadata = []
    isidata = []
    good_units = []

    for i, data in enumerate(collection):
        try:
            data_idx, unit_idx = collection._index_to_data_pos(i)
            filename = collection.get_file_dict()[&#34;Spike&#34;][data_idx][0]
            unit_number = collection.get_units(data_idx)[unit_idx]
            print(&#34;Working on {} unit {}&#34;.format(
                filename, unit_number))

            count = data.spike.get_unit_spikes_count()
            duration = data.spike.get_duration()
            good = True

            if filter_low_freq and (count / duration) &lt; 0.1:
                print(&#34;Reject spike frequency {}&#34;.format(count / duration))
                good = False

            elif filter_place_cells:
                skaggs = data.loc_shuffle(nshuff=300)
                bad_skaggs = skaggs[&#39;refSkaggs&#39;] &lt;= skaggs[&#39;skaggs95&#39;]
                bad_sparsity = skaggs[&#39;refSparsity&#39;] &gt;= skaggs[&#39;sparsity05&#39;]
                bad_cohere = skaggs[&#39;refCoherence&#39;] &lt;= skaggs[&#39;coherence95&#39;]

                if bad_skaggs or bad_sparsity or bad_cohere:
                    good = False
                    first_str_part = &#34;Reject &#34;

                else:
                    good_units.append(unit_idx)
                    first_str_part = &#34;Accept &#34;

                print((
                    first_str_part +
                    &#34;Skaggs {:2f} | {:2f}, &#34; +
                    &#34;Sparsity {:2f} | {:2f}, &#34; +
                    &#34;Coherence {:2f} | {:2f}&#34;).format(
                    skaggs[&#39;refSkaggs&#39;],
                    skaggs[&#39;skaggs95&#39;],
                    skaggs[&#39;refSparsity&#39;],
                    skaggs[&#39;sparsity05&#39;],
                    skaggs[&#39;refCoherence&#39;],
                    skaggs[&#39;coherence95&#39;]))
            if good:
                placedata.append(data.place())
                graphdata.append(data.isi_corr(bins=1, bound=[-10, 10]))
                wavedata.append(data.wave_property())
                headdata.append(data.hd_rate())
                thetadata.append(data.theta_index(bins=2, bound=[-350, 350]))
                isidata.append(data.isi(bins=int(350 / 2), bound=[0, 350]))

            # Save the accumulated information
            if unit_idx == len(collection.get_units(data_idx)) - 1:
                spike_name = os.path.basename(filename)
                parts = spike_name.split(&#34;.&#34;)
                f_dir = os.path.dirname(filename)
                png_basename = parts[0] + &#34;_&#34; + parts[1] + opt_end + &#34;.png&#34;
                if base_dir is not None:
                    main_dir = base_dir
                    png_base = f_dir[len(base_dir + os.sep):]
                    png_base = (&#34;--&#34;).join(png_base.split(os.sep))
                    # png_base = re.sub(os.sep, &#34;_&#34;, png_base)
                    png_basename = png_base + &#34;--&#34; + png_basename
                else:
                    main_dir = f_dir

                if filter_place_cells:
                    named_units = [
                        collection.get_units(data_idx)[j]
                        for j in good_units]
                else:
                    named_units = collection.get_units(data_idx)

                if len(named_units) &gt; 0:
                    if filter_place_cells:
                        print((
                            &#34;Plotting summary for {} &#34; +
                            &#34;spatial units {}&#34;).format(
                            spike_name, named_units))
                    else:
                        print((
                            &#34;Plotting summary for {} &#34; +
                            &#34;units {}&#34;).format(
                            spike_name, named_units))

                    fig = print_place_cells(
                        len(named_units),
                        placedata=placedata, graphdata=graphdata,
                        wavedata=wavedata, headdata=headdata,
                        thetadata=thetadata, isidata=isidata,
                        size_multiplier=4, point_size=dpi / 7.0,
                        units=named_units)
                    out_name = os.path.join(
                        main_dir, out_dirname, png_basename)
                    print(&#34;Saving place cell figure to {}&#34;.format(
                        out_name))
                    make_dir_if_not_exists(out_name)
                    fig.savefig(out_name, dpi=dpi)
                    close(&#34;all&#34;)
                    gc.collect()

                    placedata = []
                    graphdata = []
                    wavedata = []
                    headdata = []
                    thetadata = []
                    isidata = []
                    good_units = []

        except Exception as e:
            log_exception(
                e, &#34;Occured during place cell summary on data&#34; +
                   &#34; {} unit {} name {} in {}&#34;.format(
                       data_idx, unit_number, spike_name, main_dir))
    return</code></pre>
</details>
</dd>
<dt id="neurochat.nc_containeranalysis.smooth_speeds"><code class="name flex">
<span>def <span class="ident">smooth_speeds</span></span>(<span>collection, allow_multiple=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Smooth all the speed data in the collection.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>collection</code></strong> :&ensp;<code>NDataContainer</code></dt>
<dd>Container to get the information from</dd>
<dt><strong><code>allows_multiple</code></strong> :&ensp;<code>bool</code></dt>
<dd>Allow smoothing multiple times, default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def smooth_speeds(collection, allow_multiple=False):
    &#34;&#34;&#34;
    Smooth all the speed data in the collection.

    Parameters
    ----------
    collection : NDataContainer
        Container to get the information from
    allows_multiple : bool
        Allow smoothing multiple times, default False

    Returns
    -------
    None

    &#34;&#34;&#34;
    if collection._smoothed_speed and not allow_multiple:
        logging.warning(
            &#34;NDataContainer has already been speed smoothed, not smoothing&#34;)

    for i in range(collection.get_num_data()):
        data = collection.get_data(i)
        data.smooth_speed()
        collection._smoothed_speed = True</code></pre>
</details>
</dd>
<dt id="neurochat.nc_containeranalysis.spike_positions"><code class="name flex">
<span>def <span class="ident">spike_positions</span></span>(<span>collection, should_sort=True, mode='vertical')</span>
</code></dt>
<dd>
<section class="desc"><p>Get the spike positions for a number of units.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>collection</code></strong> :&ensp;<code>NDataContainer</code> or <code>NData</code> <code>list</code> or <code>NData</code> <code>object</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>positions</code></strong> :&ensp;<code>list</code> of <code>positions</code> of <code>the</code> <code>rat</code> <code>when</code> <code>the</code> <code>cell</code> <code>spiked</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def spike_positions(collection, should_sort=True, mode=&#34;vertical&#34;):
    &#34;&#34;&#34;
    Get the spike positions for a number of units.

    Parameters
    ----------
    collection : NDataContainer or NData list or NData object


    Returns
    -------
    positions : list of positions of the rat when the cell spiked

    &#34;&#34;&#34;
    if isinstance(collection, NDataContainer) and should_sort:
        collection.sort_units_spatially(mode=mode)

    if isinstance(collection, NData):
        positions = collection.get_event_loc(collection.get_unit_stamp())[1]
        if mode == &#34;vertical&#34;:
            positions = positions[1]
        elif mode == &#34;horizontal&#34;:
            positions = positions[0]
        else:
            logging.error(&#34;nca: mode only supports vertical or horizontal&#34;)
    else:
        positions = []
        for data in collection:
            position = data.get_event_loc(data.get_unit_stamp())[1]
            if mode == &#34;vertical&#34;:
                position = position[1]
            elif mode == &#34;horizontal&#34;:
                position = position[0]
            else:
                logging.error(&#34;nca: mode only supports vertical or horizontal&#34;)
            positions.append(position)

    return positions</code></pre>
</details>
</dd>
<dt id="neurochat.nc_containeranalysis.spike_times"><code class="name flex">
<span>def <span class="ident">spike_times</span></span>(<span>collection, filter_speed=False, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Return a list of all spike times in the collection.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>collection</code></strong> :&ensp;<code>NDataContainer</code> or <code>NData</code></dt>
<dd>Either the container or data object to get spike times from</dd>
<dt><strong><code>filter_speed</code></strong> :&ensp;<code>bool</code></dt>
<dd>If true, don't consider spike times when the rat is non moving</dd>
<dt><strong><code>kwargs</code></strong></dt>
<dd>should_smooth : bool
Smooth the speed data if true
ranges : list
List of tuples indicating time ranges to get spikes in</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>The list of spike times if collection is NData
or a 2d list containing a list of times for each collection item</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def spike_times(collection, filter_speed=False, **kwargs):
    &#34;&#34;&#34;
    Return a list of all spike times in the collection.

    Parameters
    ----------
    collection : NDataContainer or NData
        Either the container or data object to get spike times from
    filter_speed : bool
        If true, don&#39;t consider spike times when the rat is non moving
    kwargs
        should_smooth : bool
            Smooth the speed data if true
        ranges : list
            List of tuples indicating time ranges to get spikes in

    Returns
    -------
    list
        The list of spike times if collection is NData
        or a 2d list containing a list of times for each collection item

    &#34;&#34;&#34;
    should_smooth = kwargs.get(&#34;should_smooth&#34;, False)
    ranges = kwargs.get(&#34;ranges&#34;, None)

    if isinstance(collection, NData):
        if ranges is not None:
            time_data = collection.get_unit_stamps_in_ranges(ranges)
        elif filter_speed:
            ranges = collection.non_moving_periods(**kwargs)
            time_data = collection.get_unit_stamps_in_ranges(ranges)
        else:
            times = collection.get_unit_stamp()

    else:
        if should_smooth:
            smooth_speeds(collection)
            kwargs[&#34;should_smooth&#34;] = False

        times = []
        for data in collection:
            if ranges is not None:
                time_data = data.get_unit_stamps_in_ranges(ranges)
            elif filter_speed:
                ranges = data.non_moving_periods(**kwargs)
                time_data = data.get_unit_stamps_in_ranges(ranges)
            else:
                time_data = data.get_unit_stamp()
            times.append(time_data)
    return times</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="neurochat" href="index.html">neurochat</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="neurochat.nc_containeranalysis.count_units_in_bins" href="#neurochat.nc_containeranalysis.count_units_in_bins">count_units_in_bins</a></code></li>
<li><code><a title="neurochat.nc_containeranalysis.evaluate_clusters" href="#neurochat.nc_containeranalysis.evaluate_clusters">evaluate_clusters</a></code></li>
<li><code><a title="neurochat.nc_containeranalysis.place_cell_summary" href="#neurochat.nc_containeranalysis.place_cell_summary">place_cell_summary</a></code></li>
<li><code><a title="neurochat.nc_containeranalysis.smooth_speeds" href="#neurochat.nc_containeranalysis.smooth_speeds">smooth_speeds</a></code></li>
<li><code><a title="neurochat.nc_containeranalysis.spike_positions" href="#neurochat.nc_containeranalysis.spike_positions">spike_positions</a></code></li>
<li><code><a title="neurochat.nc_containeranalysis.spike_times" href="#neurochat.nc_containeranalysis.spike_times">spike_times</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.6.3</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>